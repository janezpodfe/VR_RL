{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spodbujevalno u\u010denje","text":""},{"location":"#strojno-ucenje","title":"Strojno u\u010denje","text":""},{"location":"#spodbujevalno-ucenje_1","title":"Spodbujevalno u\u010denje","text":"<ul> <li>U\u010denje robotskih strategij v simuliranem okolju<ul> <li>OpenAI Gym + Stable Baselines3 </li> <li>Preprosta okolja</li> <li>AirHockey</li> </ul> </li> <li>Prenos na dejanskega robota</li> <li>Vodene + samostojne vaje</li> </ul> <ul> <li>Okolje<ul> <li>Stanje okolja</li> <li>Nagrada za agenta, za dolo\u010deno stanje</li> <li>Seznam mo\u017enih akcij</li> </ul> </li> <li>Agent<ul> <li>Strategija za izbiro najbolj\u0161ih akciji</li> <li>Izvaja in se odlo\u010da o najbolj\u0161i akciji</li> </ul> </li> </ul>"},{"location":"#okolje","title":"Okolje","text":"<ul> <li>Spodbujevalno u\u010denje zahteva veliko ponovitev<ul> <li>Zelo neprakti\u010dno za razvoj na robotu</li> <li>U\u010denje v simulaciji okolja -&gt; Env</li> <li>Zbirka okolij -&gt; Gym</li> </ul> </li> </ul> <p>OpenAI Gym okolja za izvajanje spodbujevalnega u\u010denja (https://gym.openai.com/)</p>"},{"location":"#programsko-okolje","title":"Programsko okolje","text":""},{"location":"#openai-gym","title":"OpenAI Gym","text":"<ul> <li>OpenAI Gym<ul> <li>Simulacij razli\u010dnih okolij</li> <li>Pred pripravljena okolja</li> <li>Struktura za razvoj lastnih okolij</li> </ul> </li> </ul> <p>https://gymnasium.farama.org/environments/toy_text/cliff_walking/</p> <p>https://gymnasium.farama.org/environments/mujoco/</p>"},{"location":"#stable-baselines3","title":"Stable Baselines3","text":"<ul> <li>Knji\u017enica za algoritme za spodbujevalno u\u010denje<ul> <li>Globoko spodbujevalno u\u010denje</li> </ul> </li> <li>Stable Baselines3 (SB3) algoritmi za spodbujevalno u\u010denje<ul> <li>https://stable-baselines3.readthedocs.io/en/master/</li> <li>vsebuje \u0161e druga okolja v skladu s strukturo OpenAI Gym</li> </ul> </li> </ul>"},{"location":"#q-ucenje","title":"Q u\u010denje","text":""},{"location":"car/","title":"MountainCar primer","text":"<ul> <li>Cilj je priti iz doline do zastavice s prenihavanjem na klan\u010dinah,</li> <li>github povezava na py skripto za okolje</li> <li>Mountain Car</li> <li>Diskretne akcije<ul> <li>Potisk levo</li> <li>Potisk desno</li> <li>Brez potiska</li> </ul> </li> </ul> <ul> <li>Zvezna opazovanja<ul> <li>pozicija</li> <li>hitrost</li> <li>Za Q tabelo potrebujemo diskretna opazovanja!</li> </ul> </li> <li>Diskretizacija opazovanj na obmo\u010dja</li> <li>Nagrada: <code>-1</code> za vsak korak</li> <li>Optimiziramo, da dose\u017ee cilj v najkraj\u0161em \u010dasu</li> </ul>"},{"location":"car/#python-skripta","title":"Python skripta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 15000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n</code></pre> </li> <li> <p>Preverimo prostor akcij in opazovanja</p> <pre><code>print( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Diskretizacija opazovanj</p> <pre><code>#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n</code></pre> </li> <li> <p>Inicializacija Q tabele</p> <pre><code>#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\n</code></pre> </li> <li> <p>Funkcija, ki vrne indeks diskretnega stanja glede na vrednost opazovanj</p> <pre><code>def get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\n</code></pre> </li> <li> <p>Za\u010dnemo izvajati epizode u\u010denja</p> <pre><code>for episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\n</code></pre> </li> <li> <p>Akcija, korak, stanje, Q tabela</p> <ul> <li>Izberemo akcijo</li> <li>Izvedemo korak</li> <li>Dolo\u010dimo novo diskretno stanje</li> </ul> <pre><code>\u00a0 \u00a0 while not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\n</code></pre> <ul> <li>Posodobimo Q tabelo</li> <li>Posodobimo stanje</li> </ul> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 if not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state\n</code></pre> <ul> <li>Posodobimo epsilon</li> </ul> <pre><code>\u00a0 \u00a0 if END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> <ul> <li>Shranimo trenutno vrednost Q tabele</li> </ul> <pre><code>\u00a0 \u00a0 if episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\n</code></pre> <ul> <li>Po kon\u010danih epizodah shranimo kon\u010dno Q tabelo </li> </ul> <pre><code>np.save(f\"cart_e{episode}-qtable.npy\", q_table) \n</code></pre> </li> <li> <p>Celotna koda</p> </li> </ol> car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 15000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nfor episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\nwhile not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\nif not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state     \nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nif episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table) \n</code></pre>"},{"location":"carSB3Contrib/","title":"Uporaba drugih algoritmov za u\u010denje","text":"<ul> <li>Stable-Baselines3 vsebuje veliko implementacij algoritmov<ul> <li>RL Algorithms</li> </ul> </li> <li>Dodatni algoritmi so v SB3 Contrib<ul> <li>SB3 Contrib</li> </ul> </li> <li>Hiperparametri za posamezna Gym okolja in RL algoritme so v RL Baselines3 Zoo<ul> <li>RL Baselines3 Zoo hiperparametri</li> </ul> </li> </ul>"},{"location":"carSB3Contrib/#python-skripta-za-ucenje-agenta-z-qrdqn-algoritmom","title":"Python skripta za u\u010denje agenta z QRDQN algoritmom","text":"carQRDQN.py<pre><code>import gym\nfrom sb3_contrib import QRDQN\nimport os\nmodels_dir = \"models/QRDQN\"\nif not os.path.exists(models_dir):\nos.makedirs(models_dir)\nlogdir = \"logs\"\nif not os.path.exists(logdir):\nos.makedirs(logdir) \nenv = gym.make(\"MountainCar-v0\")\npolicy_kwargs = dict(net_arch=[256, 256], n_quantiles=25)\nmodel = QRDQN('MlpPolicy', \nenv=env, \ntensorboard_log=logdir,\nverbose=1,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.98,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs)\nTIMESTEPS = 2000\niters = 0\nwhile True:\niters += 1\nmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"QRDQN\")\nmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n</code></pre>"},{"location":"carSB3Contrib/#ppo-algoritem","title":"PPO algoritem","text":"<p>Primer 1 Primer 2</p>"},{"location":"car_DQN/","title":"U\u010denje z nevronskimi mre\u017eami z metodo DQN","text":""},{"location":"car_DQN/#globoko-spodbujevalno-ucenje","title":"Globoko spodbujevalno u\u010denje","text":"<p>https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html</p>"},{"location":"car_DQN/#dqn","title":"DQN","text":"<p>Q-u\u010denje je algoritma za spodbujevalno u\u010denje, ki uporablja tabelo za shranjevanje vrednosti Q za vsak par stanje-akcija. Ker pa prostor stanj postaja vse ve\u010dji in kompleksnej\u0161i, postane shranjevanje vseh vrednosti Q v tabeli neprakti\u010dno. Tu pride na vrsto DQN.</p> <p>DQN je vrsta algoritma za Q-u\u010denje, ki za izra\u010dun (pribli\u017eka) vrednosti Q uporablja nevronsko mre\u017eo. Nevronska mre\u017ea sprejme stanje kot vhod in kot izhod izra\u010duna vrednosti Q za vsako mo\u017eno akcijo. Mre\u017ea je nau\u010dena s kombinacijo ponovitve izku\u0161enj in \u00bbtarget\u00ab nevronske mre\u017ee za izbolj\u0161anje stabilnosti in u\u010dinkovitosti.</p> <p>Tehnika ponovitve izku\u0161enj se uporablja za izbolj\u0161anje u\u010dinkovitosti u\u010denja. Namesto da bi nevronsko mre\u017eo u\u010dili takoj na vsaki izku\u0161nji, se izku\u0161nje shranijo v spomin in se iz njega naklju\u010dno izbere serija izku\u0161enj za u\u010denje mre\u017ee. To ima ve\u010d prednosti. Prvi\u010d, pomaga prekiniti korelacijo med zaporednimi izku\u0161njami, kar lahko zmanj\u0161a verjetnost, da bo mre\u017ea obti\u010dalo v lokalnem optimumu. Drugi\u010d, nevronski mre\u017ei omogo\u010da, da se u\u010di iz \u0161ir\u0161ega razpona izku\u0161enj, kar lahko privede do bolj\u0161e posplo\u0161itve in bolj\u0161ega delovanja.</p> <p>\u00bbTarget\u00ab nevronska mre\u017ea je lo\u010dena kopija glavnega nevronske mre\u017ee, ki se med u\u010denjem uporablja za izra\u010dun ciljnih (\u00bbtarget\u00ab) vrednosti Q. V DQN uporabljamo \u00bbtarget\u00ab nevronsko mre\u017eo za re\u0161evanje problema, znanega kot \"problem premikajo\u010de se tar\u010de\". Ta te\u017eava nastane, ko uporabimo isto omre\u017eje za izra\u010dunavanje napovedi vrednosti Q in ciljnih vrednosti Q ter po vsaki napovedi posodobimo ute\u017ei nevronske mre\u017ee. To lahko povzro\u010di hitro in nestanovitno spreminjanje ciljnih vrednosti Q, kar lahko privede do nestabilnosti v procesu u\u010denja. Za re\u0161itev te te\u017eave uporabimo \u00bbtarget\u00ab nevronsko mre\u017eo, ki je kopija glavne nevronske mre\u017ee, vendar z zamrznjenimi parametri. \u00bbTarget\u00ab nevronska mre\u017ea se uporablja za izra\u010dun ciljnih vrednosti Q, ute\u017ei glavnega omre\u017eja pa se redno posodabljajo le z uporabo ciljnih vrednosti Q, ki jih izra\u010duna ciljno omre\u017eje. To stabilizira postopek u\u010denja in omre\u017eju omogo\u010da u\u010dinkovitej\u0161e u\u010denje.</p> <p>Najprej za\u010dnemo z vhodnim stanjem kor vhodom v nevronsko mre\u017eo. Mre\u017ea nato vrne vrednosti Q za vsako mo\u017eno akcijo. Podobno kot pri Q-u\u010denju ne izberemo akcije z najvi\u0161jo vrednostjo Q. Namesto tega uporabimo tehniko, imenovano \"epsilon-greedy exploration\", kjer z verjetnostjo epsilon izberemo naklju\u010dno akcijo in z verjetnostjo 1-epsilon izberemo akcijo z najvi\u0161jo vrednostjo Q. To algoritmu omogo\u010da, da raziskuje nove akcije in potencialno najde bolj\u0161e re\u0161itve. Ko izberemo akcijo, jo izvedemo in opazujemo dobljeno nagrado in novo stanje. Ta izku\u0161nja se shrani v spomin za ponovitev, ki ga bomo pozneje uporabili za u\u010denje mre\u017ee.</p> <p>Nato naklju\u010dno izberemo serijo izku\u0161enj iz spomina in jih uporabimo za u\u010denje mre\u017ee. Vendar mre\u017ee ne \u017eelimo vedno znova u\u010diti na istih izku\u0161njah, saj to lahko privede do pojava \u00bboverfitting\u00ab. Za re\u0161itev te te\u017eave uporabimo \u00bbtarget\u00ab nevronsko mre\u017eo, ki je kopija glavnega nevronske mre\u017ee, vendar z zamrznjenimi parametri. To \u00bbtarget\u00ab nevronsko mre\u017eo uporabimo za izra\u010dun kon\u010dnih vrednosti Q za vsako izku\u0161njo, nato pa posodobimo parametre glavne mre\u017ee, da \u010dim bolj zmanj\u0161amo razliko med napovedanimi vrednostmi Q in kon\u010dnimi vrednostmi Q. Ta postopek se ponavlja iterativno, pri \u010demer nevronska mre\u017ea nenehno izbolj\u0161uje svoje ocene vrednosti Q, ko se u\u010di iz izku\u0161enj.</p> <p>\u010ce povzamemo, je DQN vrsta algoritma za Q-u\u010denje, ki za oceno vrednosti Q uporablja nevronsko mre\u017eo. Zdru\u017euje ponovitev izku\u0161enj in \u00bbtarget\u00ab nevronsko mre\u017eo za izbolj\u0161anje stabilnosti in u\u010dinkovitosti. Z uporabo te metode lahko obravnavamo ve\u010dje in bolj zapletene prostore stanj ter na koncu dose\u017eemo bolj\u0161e rezultate kot z Q-metodo.</p>"},{"location":"car_DQN/#ppo","title":"PPO","text":"<p>DQN (Deep Q-Network) in PPO (Proximal Policy Optimization) sta priljubljeni metodi za spodbujevalno u\u010denje, ki se uporabljata za u\u010denje agentov za izvajanje dolo\u010denih nalog. Vendar se razlikujeta glede na pristop in cilje. DQN je na Q vrednosti temelje\u010da metoda spodbujevalnega u\u010denja, ki u\u010di vrednosti Q (pri\u010dakovane prihodnje nagrade) za vsak par stanje-akcija. Cilj DQN je nau\u010diti se optimalno funkcijo Q, ki povezuje stanja z akcijami. To dose\u017eemo z u\u010denjem globoke nevronske mre\u017ee za oceno vrednosti Q z uporabo kombinacije ponovitve izku\u0161enj in \u00bbtarget\u00ab nevronske mre\u017ee.</p> <p>PPO pa je metoda spodbujevalnega u\u010denja na podlagi strategije, ki se neposredno u\u010di strategija za preslikavo iz stanj v akcije. Cilj PPO je maksimizirati pri\u010dakovano kumulativno nagrado agenta z iskanjem optimalne strategije. To se dose\u017ee z optimizacijo nadomestne objektivne funkcije, ki zagotavlja, da se nova strategija ne razlikuje preve\u010d od stare.</p> <p>\u010ceprav se lahko tako DQN kot PPO uporabljata za diskretne in zvezne prostore akcij, se razlikujeta po u\u010dinkovitosti vzor\u010denja, stabilnosti in enostavnosti izvajanja.</p> <p>DQN je na splo\u0161no manj vzor\u010dno u\u010dinkovit kot PPO, saj za u\u010denje natan\u010dne funkcije Q potrebuje veliko \u0161tevilo u\u010dnih vzorcev. Vendar je DQN znana po svoji stabilnosti in se uspe\u0161no uporablja v \u0161tevilnih aplikacijah, tudi v robotiki.</p> <p>Po drugi strani pa je PPO znana po svoji u\u010dinkovitosti vzor\u010denja in je dokazano dosegla vrhunske rezultate pri \u0161tevilnih nalogah zveznega vodenja. PPO je tudi razmeroma enostavna za izvajanje, saj ne potrebuje ponovitve izku\u0161enj ali lo\u010dene \u00bbtarget\u00ab nevronske mre\u017ee.</p> <p>\u010ceprav sta DQN in PPO zmogljivi metodi u\u010denja za spodbujevalno u\u010denje, se razlikujeta po pristopu in ciljih. DQN je metoda, ki temelji na vrednosti in se u\u010di optimalne funkcije Q, PPO pa je metoda, ki temelji na strategiji in se neposredno u\u010di optimalne strategije. </p>"},{"location":"car_DQN/#a2c","title":"A2C","text":"<p>A2C pa je metoda spodbujevalnega u\u010denja, ki temelji na strategiji in se neposredno u\u010di strategije (preslikava med stanji in akcijami). Cilj A2C je maksimizirati pri\u010dakovano kumulativno nagrado agenta z iskanjem optimalne strategije, podobno kot PPO. To se dose\u017ee s hkratnim u\u010denjem dveh nevronskih mre\u017e: \u00bbactor\u00ab nevronska mre\u017ea, ki aproksimira strategijo, in \u00bbcritic\u00ab nevronske mre\u017ee, ki ocenjuje funkcijo vrednosti.</p> <p>Klju\u010dna razlika med DQN in A2C je, da je DQN metoda \u00bboff-policy\u201d, kar pomeni, da se u\u010di optimalne vrednosti Q z uporabo lo\u010dene strategije, medtem ko je A2C metoda \u00bbon-policy\u201d, kar pomeni, da se u\u010di strategije in funkcije vrednosti neposredno iz trenutne strategije.</p> <p>Druga razlika je, da se DQN lahko uporablja tako za diskretne kot za zvezne prostore akcij, medtem ko se A2C obi\u010dajno uporablja za diskretne prostore akcij. Poleg tega je A2C na splo\u0161no bolj vzor\u010dno u\u010dinkovita kot DQN, saj se lahko u\u010di iz delnih trajektorij, medtem ko DQN za posodobitev svojih vrednosti Q potrebuje celotne trajektorije.</p> <p>Trajektorija $\\tau$ je sekvenca parov $stanje_0$-&gt;$akcija_0$-&gt;$stanje_1$-&gt;$akcija_1$-&gt;...</p> <p>Klju\u010dna razlika med PPO in A2C je v tem, da PPO uporablja \u00bbclipped\u00ab objektivno funkcijo, da prepre\u010di prevelike spremembe strategije ob vsaki posodobitvi, medtem ko A2C uporablja \u00bbadvantage\u00ab funkcijo za oceno kakovosti vsake akcije glede na trenutno strategijo.</p> <p>Druga razlika je, da je PPO na splo\u0161no bolj vzor\u010dno u\u010dinkovit kot A2C, saj uporablja tehniko, imenovano vzor\u010denje po pomembnosti, za popravke posodobitev gradienta. Vzor\u010denje po pomembnosti omogo\u010da, da se PPO u\u010di iz stare strategije, kar lahko izbolj\u0161a u\u010dinkovitost vzor\u010denja.</p> <p>\u010ceprav sta tako PPO kot A2C u\u010dinkoviti metodi za spodbujevalno u\u010denje, se razlikujeta po pristopu in ciljih. PPO je metoda, ki temelji na strategiji in se u\u010di optimalne strategije z uporabo optimizacije in \u00bbclipped\u00ab objektivne funkcije, A2C pa je prav tako metoda, ki temelji na strategiji in se u\u010di optimalne strategije in funkcije vrednosti z uporabo \u00bbadvantage\u00ab funkcije prednosti. </p> <p>\u201cAdvantage\u201d funkcija je temeljni koncept spodbujevalnega u\u010denja, ki se uporablja za oceno kakovosti vsake akcije agenta glede na trenutno strategijo. Meri, koliko bolj\u0161a je akcija v primerjavi s povpre\u010dno akcijo izvedeno na podlagi trenutne strategije, in se uporablja za posodabljanje strategije in funkcije vrednosti.</p> <p>Bolj formalno je funkcija prednosti A(s, a) opredeljena kot razlika med vrednostjo Q (pri\u010dakovana kumulativna nagrada) za akcijo a v stanju s in funkcijo vrednosti (pri\u010dakovana kumulativna nagrada po trenutni strategiji) stanja s:</p> <p>A(s, a) = Q(s, a) - V(s)</p> <p>\u201cAdvantage\u201d funkcija je pomemben gradnik \u0161tevilnih algoritmov spodbujevalnega u\u010denja, kot sta A2C in PPO. Agentu omogo\u010da razlikovanje med dobrimi in slabimi akcijami z izra\u010dunom pri\u010dakovanega izbolj\u0161anja dolgoro\u010dne nagrade za dolo\u010deno akcijo v primerjavi s povpre\u010dno akcijo izvedeno na podlagi trenutne strategije. Ta informacija se uporabi za posodobitev strategije na na\u010din, ki agenta spodbuja k sprejemanju akcij, ki so bolj\u0161a od povpre\u010dne akcije.</p> <p>Eden od na\u010dinov za oceno funkcije prednosti je uporaba napake TD (napake \u010dasovne razlike) med ocenjeno funkcijo vrednosti in opazovano nagrado. To lahko storimo z metodo, imenovano enostopenjsko u\u010denje TD (temporal difference), ki v vsakem \u010dasovnem koraku posodobi funkcijo vrednosti in \u201cadvantage\u201d funkcijo. Drug pristop je uporaba natan\u010dnej\u0161e ocene \u201cadvantage\u201d funkcije, kot je posplo\u0161ena ocena prednosti (GAE), ki zdru\u017euje ve\u010d \u010dasovnih korakov, da zagotovi stabilnej\u0161o oceno \u201cadvantage\u201d funkcije.</p> <p>\u201cAdvantage\u201d funkcija je klju\u010den gradnik algoritmov spodbujevalnega u\u010denja, ki se uporablja za oceno kakovosti vsake akcije agenta. Agentu omogo\u010da, da se nau\u010di, katere akcije so bolj\u0161e od drugih, ter ustrezno posodobi strategijo in funkcijo vrednosti.</p> <p>Primer u\u010denja z DQN in PPO </p>"},{"location":"car_DQN/#prakticna-uporaba","title":"Prakti\u010dna uporaba","text":"<ul> <li>U\u010denje z nevronskimi mre\u017eami z metodo DQN</li> <li>Uporabimo python paket Stable Baselines3 (SB3)</li> <li>Stable-Baselines3 Docs</li> <li>Stable Baselines3 omogo\u010da celo vrsto drugih algoritmov<ul> <li>A2C</li> <li>PPO</li> </ul> </li> <li>Raz\u0161iritev SB3 Contrib<ul> <li>dodatni sodobnej\u0161i algoritmi</li> <li>SB3 Contrib dokumentacije</li> <li>Github repozitorij</li> </ul> </li> </ul> <p>Q tabela </p> <p>Aproksimacija Q tabele s funkcijami</p> <p>Diskretne akcije </p> <p>Zvezne akcije </p> <p>Aproksimacija Q tabele z nevronsko mre\u017eo </p>"},{"location":"car_DQN/#python-skripta-za-ucenje-agenta","title":"Python skripta za u\u010denje agenta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <p><pre><code>import gym\nfrom stable_baselines3 import DQN \nenv = gym.make(\"MountainCar-v0\")\n</code></pre> 3. Preverimo prostor akcij in opazovanja</p> <pre><code>print( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Inicializiramo u\u010denje agenta</p> </li> <li> <p>Podatki za inicializacijo so na spletni strani ter na rl-baselines3-zoo</p> <pre><code>policy_kwargs = dict(net_arch=[256, 256])\nmodel = DQN('MlpPolicy', \nenv=env,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.99,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs,\nseed=2,\nverbose=1\n)\n</code></pre> </li> <li> <p>U\u010denje agenta</p> <pre><code>model.learn(total_timesteps=1.2e5)\n</code></pre> </li> <li> <p>Shranimo model</p> <pre><code>model.save(\"dqn_car\")\n</code></pre> </li> </ol>"},{"location":"car_DQN/#python-skripta-za-testiranje-agenta","title":"Python skripta za testiranje agenta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Nalo\u017eimo in incializiramo agenta</p> <pre><code>model = DQN.load(\"dqn_car\", env=env)\n</code></pre> </li> <li> <p>Za\u017eenemo in testiramo agenta</p> <pre><code>mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\nprint(f'Mean reward: {mean_reward}, Std reward: {std_reward}')\nobs = env.reset()\nwhile True:\naction, _state = model.predict(obs, deterministic=True)\nobs, reward, done, info = env.step(action)\nenv.render()\nif done:\nobs = env.reset()\n</code></pre> </li> </ol>"},{"location":"car_action/","title":"Prikaz akcij glede na Q tabelo","text":"load_car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nimport sys ###\nimport matplotlib.pyplot as plt ###\nimport matplotlib.patches as mpatches ###\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\ndef get_actions(dataset):\nstolpec = 0\n#print(type(data))\nactions = np.ndarray([GRID_SIZE, GRID_SIZE])\nfor stolpec in range(GRID_SIZE):\nvrstica = 0\nfor vrstica in range(GRID_SIZE):\nif dataset[stolpec, vrstica, 0] == dataset[stolpec, vrstica, 1] == dataset[stolpec, vrstica, 2] == 0:\nactions[stolpec, vrstica] = -1\nelse:\nactions[stolpec, vrstica] = np.argmax(dataset[stolpec, vrstica])\nreturn actions\ndef plot_graphs(ep_list):\nep = 0\nfig,axs = plt.subplots(2,2,figsize = (15,15))\nfig.suptitle(\"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\", fontsize = 16)\nprint(enumerate(axs.flat))\nfor i, ax in enumerate(axs.flat):\ndata = np.load('cart_e'+str(ep_list[ep])+'-qtable.npy')\nnp.set_printoptions(threshold=sys.maxsize)\npositions = np.arange(-1.2,0.6+discrete_os_win_size[0],discrete_os_win_size[0])\nvelocities = np.arange(-0.07,0.07+discrete_os_win_size[1],discrete_os_win_size[1])\n#ax = fig.add_subplot(2,2)\nlabels = [\"Neobiskana stanja\",\"Premik levo\", \"Ne naredimo ni\u010desar\", \"Premik desno\"]\ncmap = plt.colormaps.get_cmap('Blues') #matplotlib.colormaps\nax = plt.subplot(2,2,i+1)\nactions = get_actions(data)\nax.pcolor(velocities,positions, actions, cmap = cmap)\nax.set_ylabel(\"Pozicija\", fontsize = 14)\nax.set_xlabel(\"Hitrost\", fontsize = 14)        \n#ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r')\n#ax.plot(velocities_0[0],positions_0[0],'ro')\nax.set_title('Epizoda '+str(ep_list[ep]+1), fontsize = 13)\nep += 1\nbound = np.linspace(0, 1, 5)\nprint(bound)\nfig.legend([mpatches.Patch(color=cmap(b)) for b in bound[:-1]],\n[labels[i] for i in range(4)], loc = 'upper right')\nplt.subplots_adjust(wspace=0.4,hspace=0.4)\nfig.savefig('Qtable.jpg') ###\n#plt.show()\nplot_graphs([0,5000,10000,14999])\n</code></pre>"},{"location":"car_reward/","title":"Izpis nagrade agenta med u\u010denjem","text":"car.py<pre><code>import gym\nimport numpy as np\nimport matplotlib.pyplot as plt ###\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 25000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nreward_list = [] ###\nave_reward_list = [] ###\nfor episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\ntot_reward, reward = 0, 0 ###\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\nwhile not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\nif not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state\ntot_reward += reward ###\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nif episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\nreward_list.append(tot_reward) ###\nif episode % SHOW_EVERY == 0: ###\nave_reward = np.mean(reward_list) ###\nave_reward_list.append(ave_reward) ###\nreward_list = [] ###\nprint('Episode {} Average Reward: {}'.format(episode, ave_reward)) ###\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table) \n# Plot Rewards\nfig, ax = plt.subplots()\nax.plot(SHOW_EVERY * (np.arange(len(ave_reward_list)) + 1), ave_reward_list) ###\nax.set_xlabel('Episodes') ###\nax.set_ylabel('Average Reward') ###\nax.set_title('Average Reward vs Episodes') ###\nfig.savefig('rewards.jpg') ###\nplt.show() ###\n</code></pre>"},{"location":"car_reward/#izpis-nagrade-za-vec-ponovitev","title":"Izpis nagrade za ve\u010d ponovitev","text":"load_car.py<pre><code>import gym\nimport numpy as np\nimport matplotlib.pyplot as plt ###\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e24999-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nreward_list = [] ###\nfor episode in range(50): ###\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\ndone = False\nprint(\"EPISODE \", episode)\ntot_reward, reward = 0, 0 ###\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\ntot_reward += reward ###\nreward_list.append(tot_reward) ###\nenv.close()\nf = plt.figure()\nplt.plot(reward_list)\nplt.xticks(range(0,len(reward_list),2),range(1,len(reward_list)+1,2))\nplt.axhline(y = -200, color = 'k', linestyle = '--')\nplt.axhline(y = np.average(reward_list), color = 'blue', linestyle = '--', label = 'povpre\u010dna nagrada')\nplt.annotate(str(np.average(reward_list)), xy= (-2,np.average(reward_list)+0.7), color = 'blue',fontsize = 13, weight = 'bold')\nax=plt.gca()\nax.tick_params(axis=\"both\", labelsize=12)\nf.legend(loc = 'right', fontsize = 13)\nplt.xlabel('Epizoda', fontsize=14)\nplt.ylabel('Nagrada', fontsize=14)\nplt.title('Vrednost nagrade v posamezni epizodi', fontsize=16)\nplt.show()\n</code></pre>"},{"location":"cart/","title":"Cartpole primer","text":"<ul> <li>github povezava na py skripto za okolje</li> <li> <p>Cart Pole</p> </li> <li> <p>Diskretne akcije (2 akciji)</p> </li> <li> <p>Zvezna opazovanja (4 spremenljivke)</p> <ul> <li>Nujna diskretizacija za uporabo Q u\u010denja</li> <li>Nekatera opazovanje imajo meje od -\u221e do +\u221e -&gt; dolo\u010ditev mej na roke</li> </ul> </li> <li> <p>Nagrada: +1 za vsak korak     -Optimizacija, da vodenje \u010dim dlje obdr\u017ei nihalo in vozi\u010dek pokonci in znotraj okolja</p> </li> </ul>"},{"location":"cart/#q-ucenje","title":"Q u\u010denje","text":""},{"location":"cart/#namigi","title":"Namigi","text":"<pre><code>obs_high = np.array([2.4, 3, 0.21, 3])\nobs_low = -obs_high \n</code></pre>"},{"location":"cart/#predvideni-rezultati","title":"Predvideni rezultati","text":""},{"location":"cart/#ucenje","title":"U\u010denje","text":""},{"location":"cart/#qtabela","title":"Qtabela","text":"load_cartpole.py<pre><code>import gym\nimport numpy as np\nimport sys ###\nimport matplotlib.pyplot as plt ###\nimport matplotlib.patches as mpatches ###\nenv = gym.make(\"CartPole-v1\") ####\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = np.array([2.4, 3, 0.21, 3]) ####\nobs_low = -obs_high\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\ndef get_actions(dataset):\nstolpec = 0\n#print(type(data))\nactions = np.ndarray([GRID_SIZE, GRID_SIZE])\nfor stolpec in range(GRID_SIZE):\nvrstica = 0\nfor vrstica in range(GRID_SIZE):\nif dataset[stolpec, vrstica, GRID_SIZE//2, GRID_SIZE//2, 0] == dataset[stolpec, vrstica, GRID_SIZE//2, GRID_SIZE//2, 1] ==  0: ###\nactions[stolpec, vrstica] = -1\nelse:\nactions[stolpec, vrstica] = np.argmax(dataset[stolpec, vrstica, GRID_SIZE//2, GRID_SIZE//2]) ###\nreturn actions\ndef plot_graphs(ep_list):\nep = 0\nfig,axs = plt.subplots(2,2,figsize = (15,15))\nfig.suptitle(\"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\", fontsize = 16)\nprint(enumerate(axs.flat))\nfor i, ax in enumerate(axs.flat):\ndata = np.load('cartpole_e'+str(ep_list[ep])+'-qtable.npy')\nnp.set_printoptions(threshold=sys.maxsize)\npositions = np.arange(-2.4,2.4+discrete_os_win_size[0],discrete_os_win_size[0]) ###\nvelocities = np.arange(-3,3+discrete_os_win_size[1],discrete_os_win_size[1]) ###\n#ax = fig.add_subplot(2,2)\nlabels = [\"Neobiskana stanja\", \"Premik levo\", \"Premik desno\"]\ncmap = plt.colormaps.get_cmap('Blues') #matplotlib.colormaps\nax = plt.subplot(2,2,i+1)\nactions = get_actions(data)\nax.pcolor(velocities, positions, actions, cmap = cmap)\nax.set_ylabel(\"Pozicija\", fontsize = 14)\nax.set_xlabel(\"Hitrost\", fontsize = 14)        \n#ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r')\n#ax.plot(velocities_0[0],positions_0[0],'ro')\nax.set_title('Epizoda '+str(ep_list[ep]+1), fontsize = 13)\nep += 1\nbound = np.linspace(0, 1, 4)\nprint(bound)\nfig.legend([mpatches.Patch(color=cmap(b)) for b in bound[:-1]],\n[labels[i] for i in range(3)], loc = 'upper right')\nplt.subplots_adjust(wspace=0.4,hspace=0.4)\nfig.savefig('Qtable.jpg') ###\nplt.show()\nplot_graphs([0,10000,20000,50000])\n</code></pre>"},{"location":"cartpole1/","title":"Cartpole1","text":"cartpole.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\n#env = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\nenv = gym.make(\"CartPole-v1\") ##########\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 15000\nSHOW_EVERY = 1000\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = np.array([2.4, 3, 0.21, 3]) ####\nobs_low = -obs_high ####\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nfor episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\nwhile not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\nif not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state     \nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nif episode % SHOW_EVERY == 0:\nnp.save(f\"cartpole_e{episode}-qtable.npy\", q_table) ###\nnp.save(f\"cartpole_e{episode}-qtable.npy\", q_table) ###\n</code></pre> load_cartpole.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"CartPole-v1\") ####\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = np.array([2.4, 3, 0.21, 3]) ####\nobs_low = -obs_high\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cartpole_e14999-qtable.npy\") #### \nprint( \"Q table size = \", q_table.shape)\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\nenv.close()\n</code></pre>"},{"location":"circ_env/","title":"Okolje Circle Envrionment","text":"<p>This Python code defines a custom environment for the OpenAI Gym toolkit. The environment is a rectangular, and an agent in the form of a circle can move within it. The agent is required to avoid colliding with the boundary of the world. The agent's movement and collision with the environment are simulated using the Pybox2D and Pygame libraries.</p> <p>The environment is defined in a Python class named <code>CircleEnvironment</code> that inherits from the <code>gym.Env</code> class. The class defines several functions, including the <code>__init__()</code>, <code>reset()</code>, <code>step()</code>, <code>render()</code>, and <code>close()</code> functions, that are part of the OpenAI Gym environment API.</p>"},{"location":"circ_env/#libraries-used","title":"Libraries Used","text":"<p>The code imports the following libraries:</p> <ul> <li><code>gym</code>: an open-source toolkit for developing and comparing reinforcement learning algorithms. OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It provides a set of environments for testing and training reinforcement learning agents, as well as standardized interfaces for interacting with those environments.</li> <li><code>numpy</code>: a library for numerical computing with Python.</li> <li><code>math</code>: a library for mathematical functions.</li> <li><code>pygame</code>: a library for game development with Python. Pygame is a cross-platform set of Python modules designed for writing video games. It includes functionality to handle graphics, sound, input devices, and networking. Pygame can be used to create simple 2D games or more complex games with physics and AI.</li> <li><code>Box2D</code>: a 2D physics engine for game development. Box2D is a 2D physics engine for simulations of physical systems. It can be used for simulations in games, robotics, computer vision, and other fields. It provides realistic simulation of collisions, forces, friction, and other physical interactions between objects in a 2D space.</li> </ul>"},{"location":"circ_env/#variables-used","title":"Variables Used","text":"<p>The environment uses the following variables:</p> <ul> <li><code>PPM</code>: the number of pixels per meter used for rendering the environment.</li> <li><code>TARGET_FPS</code>: the target frame rate used for rendering the environment.</li> <li><code>TIME_STEP</code>: the length of each simulation time step.</li> <li><code>WORLD_WIDTH</code>: the width of the circular world in simulation units [m].</li> <li><code>WORLD_HEIGHT</code>: the height of the circular world in simulation units [m].</li> <li><code>metadata</code>: a dictionary that contains metadata about the environment, including the available render modes and the target render frame rate.</li> <li><code>contact_listener</code>: an instance of a custom contact listener class that is used to detect collisions between the agent and the environment.</li> </ul>"},{"location":"circ_env/#standard-gym-functions-used","title":"Standard <code>gym</code> functions Used","text":"<p>The environment uses the following functions:</p> <ul> <li><code>__init__()</code>: the initialization function for the environment. It sets up the environment parameters, initializes the Pybox2D world, and sets the observation and action spaces.</li> <li><code>_get_obs()</code>: a function that returns the current observation of the agent, which is its position within the world.</li> <li><code>reset()</code>: a function that resets the environment to its initial state and returns the initial observation.</li> <li><code>step()</code>: a function that takes an action as input, simulates the environment for one time step, and returns the new observation, reward, done flag, and optional information about the simulation.</li> <li><code>render()</code>: a function that renders the environment using the specified render mode.</li> <li><code>_render_frame()</code>: a function that renders a single frame of the environment.</li> <li><code>close()</code>: a function that destroys the Pybox2D world and the agent, and closes the Pygame window.</li> </ul>"},{"location":"circ_env/#custom-functions","title":"Custom functions","text":"<ul> <li> <p><code>_is_collision(object, goal)</code>: Checks if there is a collision between the body object and the body goal. It takes in two b2Body objects, object and goal, and returns a boolean indicating whether there is a collision. It can take any two <code>Box2D</code> b2Body objects to check collison between them and not just body object and the body goal.</p> </li> <li> <p><code>create_agent(radius_px, friction)</code>: Creates a new b2Body object representing the agent. It takes in the radius of the agent in pixels, radius_px, and a friction value, friction, for the agent's fixtures.</p> </li> <li> <p><code>create_puck(radius_px, friction, type)</code>: Creates a new b2Body object representing the puck. It takes in the radius of the puck in pixels, radius_px, a friction value, friction, for the puck's fixtures, and a string type that indicates whether the puck should be a dynamic object ('d') or a kinematic object ('k').</p> </li> <li> <p><code>create_goal(dim_px, position)</code>: Creates a new b2Body object representing the goal. It takes in a tuple dim_px representing the dimensions of the goal in pixels, and a tuple position representing the position of the goal in the simulation.</p> </li> <li> <p><code>create_circ_target(radius_px)</code>: Creates a new circular target. It takes in the radius of the target in pixels, radius_px.</p> </li> <li> <p><code>create_random_object(radius_px)</code>: Creates a new random object with a random shape. It takes in the radius of the object in pixels, radius_px..</p> </li> <li> <p><code>create_border()</code>: Creates the boundary of the simulation.</p> </li> <li> <p><code>get_agent_position()</code>: Returns the current position of the agent as a numpy array.</p> </li> <li> <p><code>get_puck_position()</code>: Returns the current position of the puck as a numpy array.</p> </li> <li> <p><code>get_puck_velocity()</code>: Returns the current velocity of the puck as a numpy array.</p> </li> <li> <p><code>get_target_position()</code>: Returns the current position of the target as a numpy array.</p> </li> <li> <p><code>reset_agent(position)</code>: Resets the position and velocity of the agent to a specified position.</p> </li> <li> <p><code>reset_puck(position)</code>: Resets the position and velocity of the puck to a specified position.</p> </li> <li> <p><code>reset_target(position)</code>: Resets the position of the target to a specified position.</p> </li> <li> <p><code>reset_random_object(position)</code>: Resets the position and velocity of the random object to a specified position.</p> </li> <li> <p><code>set_agent_velocity(vel)</code>: This function sets the linear and angular velocity of the agent in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the agent.</p> </li> <li> <p><code>set_puck_velocity(vel)</code>: This function sets the linear and angular velocity of the puck in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the puck.</p> </li> <li> <p><code>draw_agent(color)</code>: This function draws the agent as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the agent is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_puck(color)</code>: This function draws the puck as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the puck is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_border(color)</code>: This function draws the border of the game as a rectangle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color.</p> </li> <li> <p><code>draw_target(color)</code>: This function draws the target as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the target is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_goal(color)</code>: This function draws the goal as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color.</p> </li> <li> <p><code>draw_random_object(color)</code>: This function draws a random object as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position, angle and vertices of the object are converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>is_agent_outside_screen()</code>: This function checks if the agent is outside the screen or not. It returns True if the agent is outside the screen, otherwise it returns False.</p> </li> <li> <p><code>calc_distance(pos1, pos2)</code>: This function calculates the Euclidean distance between two points. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points.</p> </li> <li> <p><code>unit_vector(pos1, pos2)</code>: This function calculates the unit vector pointing from pos1 to pos2. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. The function returns the unit vector as a numpy array.</p> </li> <li> <p><code>calculate_scaled_component(pos_agent, pos_target, vel)</code>: This function calculates the scaled component of the velocity vector in the direction of the unit vector pointing from pos_agent to pos_target. The input parameters pos_agent and pos_target are tuples containing the x and y coordinates of the points. The input parameter vel is a tuple containing the x and y components of the velocity vector. The function returns the scaled component as a float value.</p> </li> </ul>"},{"location":"circ_env/#koda","title":"Koda","text":""},{"location":"circ_env/#premikanje-agenta-s-tipkovnico","title":"Premikanje agenta s tipkovnico","text":""},{"location":"circ_env/#ustvarimo-agenta-dolocimo-observation_space-in-action_space","title":"Ustvarimo agenta, dolo\u010dimo <code>observation_space</code> in <code>action_space</code>.","text":"<ul> <li><code>__init__()</code> <pre><code>        self.observation_space = spaces.Box(low=np.array([0.0, 0.0]), high=np.array([self.width, self.height]), dtype=np.float32)\nself.action_space = spaces.Box(low=-2.0, high=2.0, shape=(2,), dtype=np.float32)\nself.create_agent(agent_radius_px, 0.1)        \n</code></pre></li> </ul>"},{"location":"circ_env/#potrebujemo-vsaj-en-podatek-za-observacijo","title":"Potrebujemo vsaj en podatek za observacijo.","text":"<ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        agent_pos = self.get_agent_position()\nreturn agent_pos\n</code></pre>"},{"location":"circ_env/#ponastavimo-agenta-v-reset-funkciji","title":"Ponastavimo agenta v <code>reset()</code> funkciji.","text":"<ul> <li><code>reset()</code></li> </ul> <pre><code>        self.reset_agent((np.random.uniform(self.agent_radius*1.5, self.width - self.agent_radius*1.5), np.random.uniform(self.agent_radius*1.5, self.height - self.agent_radius*1.5)))\n</code></pre>"},{"location":"circ_env/#naredimo-korak-v-step-funkciji","title":"Naredimo korak v <code>step()</code> funkciji.","text":"<ul> <li><code>step()</code></li> </ul> <p>Premik agenta za <code>action</code></p> <pre><code>        self.set_agent_velocity(action)\n</code></pre> <p>Dolo\u010dimo <code>obs</code> spremenljivko.</p> <pre><code>        obs = self._get_obs()\n</code></pre> <p>Dolo\u010dimo, <code>reward</code> in <code>done</code> spremenljivki. Za za\u010detek inicializacija:</p> <pre><code>        reward = 0.0\ndone = False \n</code></pre> <p>Nato lahko dolo\u010dimo osnovne pogoje:</p> <pre><code>        if self.current_step &gt;= self.time_steps:\n#reward = -1.0\ndone = True\n</code></pre>"},{"location":"circ_env/#izrisemo-agenta","title":"izri\u0161emo agenta","text":"<ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        self.draw_agent(green)\n</code></pre>"},{"location":"circ_env/#na-koncu-izbrisemo-agenta-v-close-funkciji","title":"Na koncu izbri\u0161emo agenta v <code>close()</code> funkciji","text":"<ul> <li><code>close()</code></li> </ul> <pre><code>        self.world.DestroyBody(self.agent)\n</code></pre>"},{"location":"circ_env/#zazenemo-okolje","title":"Za\u017eenemo okolje","text":"<p>Za\u017eeni skripto <code>playCirc.py</code></p> <pre><code>python3 playCirc.py\n</code></pre> <p>V skripti pove\u010dajte hitrost na <code>5</code>.</p> playCirc.py<pre><code>while True:\nif any([event.type == pygame.QUIT for event in pygame.event.get()]): break\n#player controls\nkeys = pygame.key.get_pressed() \nif keys[pygame.K_LEFT]: x = -1\nelif keys[pygame.K_RIGHT]: x = 1\nelse: x = 0\nif keys[pygame.K_UP]: y = -1\nelif keys[pygame.K_DOWN]: y = 1\nelse: y = 0   \naction = np.array([x,y],dtype=np.float32)\n#action = env.action_space.sample()\n#print(action)\nobs, reward, done, _ = env.step(action)\n# Render the environment\nenv.render()\n# Check if the episode is finished\nif done:\nobs = env.reset()\n# Close the environment\nenv.close()\n</code></pre>"},{"location":"circ_env/#dodajmo-osnovno-mejo","title":"Dodajmo osnovno mejo","text":"<ul> <li><code>__init__()</code></li> </ul> <p><pre><code>        self.create_border()    \n</code></pre> - <code>_render_frame()</code></p> <pre><code>        self.draw_border(black)\n</code></pre> <p>Zaklju\u010dimo episodo, ko se agent dotakne meje.</p> <ul> <li><code>step()</code></li> </ul> <pre><code>        if self._is_collision(self.agent, self.border):\nreward = -1.0\ndone = True  \n</code></pre>"},{"location":"circ_env_obj_gol/","title":"Pomik naklju\u010dnega objekta v gol","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\nagent_radius_px = 30\nobject_radius_px = 30\nself.max_puck_vel = 5.0\nself.max_agent_vel = 2.0\nself.time_steps = 500\nself.observation_space = spaces.Box(low=np.array([0.0, 0.0, 0.0, 0.0]), high=np.array([self.width, self.height, self.width, self.height]), dtype=np.float32)\nself.action_space = spaces.Box(low=-self.max_agent_vel, high=self.max_agent_vel, shape=(2,), dtype=np.float32)\nself.create_agent(agent_radius_px, 0.1) \nself.create_border()\n#self.create_circ_target(25)\n#self.create_puck(object_radius_px, 0.5, 'k')\nself.create_random_object(object_radius_px)\nself.create_goal((400, 10), (self.width/2, 10/self.PPM))\n############ DO TUKAJ SPREMINJATE\n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        agent_pos = self.get_agent_position()\n#target_pos = self.get_target_position()\npak_pos = self.get_puck_position()\n#pak_vel = self.get_puck_velocity()\nreturn np.concatenate((agent_pos, pak_pos))\n</code></pre> <ul> <li><code>reset()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\nself.reset_agent((np.random.uniform(self.agent_radius*1.5, self.width - self.agent_radius*1.5), np.random.uniform(self.agent_radius*1.5, self.height - self.agent_radius*1.5)))\n#self.reset_target((np.random.uniform(self.agent_radius*2, self.width - self.agent_radius*2), np.random.uniform(self.agent_radius*2, self.height - self.agent_radius*2)))\n#self.reset_puck((np.random.uniform(self.object_radius+self.agent_radius*3, self.width - self.object_radius-self.agent_radius*3), np.random.uniform(self.object_radius+self.agent_radius*3, self.height - self.object_radius-self.agent_radius*3)))\nself.reset_random_object((np.random.uniform(self.object_radius+self.agent_radius*3, self.width - self.object_radius-self.agent_radius*3), np.random.uniform(self.object_radius+self.agent_radius*3, self.height - self.object_radius-self.agent_radius*3)))\n############ DO TUKAJ SPREMINJATE\n</code></pre> <ul> <li><code>step()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\nself.set_agent_velocity(action)  \nobs = self._get_obs() \nself.object.ApplyAngularImpulse(-0.25*self.object.inertia*self.object.angularVelocity, True)\nself.object.ApplyForce(-1*self.object.linearVelocity, self.object.worldCenter, True)        \nreward = 0.0\ndone = False \n#if self.calc_distance(self.get_puck_position(), self.get_target_position()) &lt; self.target_radius:\n#    reward = 1.0\n#    done = True\nif self._is_collision(self.object, self.goal):\nreward = 1.0\ndone = True        \nif self._is_collision(self.agent, self.object):\nreward = 0.01        \nif self._is_collision(self.object, self.border):\nreward = -0.01               \nif self.current_step &gt;= self.time_steps:\nreward = -1.0\ndone = True\nif self._is_collision(self.agent, self.border):\nreward = -1.0\ndone = True  \n############ DO TUKAJ SPREMINJATE\n</code></pre> <ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\nself.draw_agent(green)\nself.draw_border(black)\n#self.draw_target(blue)\nself.draw_goal(blue)\n#self.draw_puck(yellow)\nself.draw_random_object(yellow)\n############ DO TUKAJ SPREMINJATE  \n</code></pre>"},{"location":"circ_env_pak/","title":"Dotik agenta s pakom","text":""},{"location":"circ_env_pak/#koda-posameznih-funkcij-v-circle_worldpy","title":"Koda posameznih funkcij v <code>circle_world.py</code>","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        self.create_puck(object_radius_px, 'k')\n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        pak_pos = self.get_puck_position()\nreturn np.concatenate((agent_pos, pak_pos))\n</code></pre> <ul> <li><code>reset()</code></li> </ul> <pre><code>        self.reset_puck((np.random.uniform(self.object_radius+self.agent_radius*3, self.width - self.object_radius-self.agent_radius*3), np.random.uniform(self.object_radius+self.agent_radius*3, self.height - self.object_radius-self.agent_radius*3)))\n</code></pre> <ul> <li><code>step()</code></li> </ul> <pre><code>        if self._is_collision(self.agent, self.object):\nreward = 1.0\ndone = True \n</code></pre> <ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        self.draw_puck(yellow)\n</code></pre> <p>Potek u\u010denja </p> <p></p>"},{"location":"circ_env_pak_gol/","title":"Pomik paka v gol","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        self.max_puck_vel = 5.0\nself.observation_space = spaces.Box(low=np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -self.max_puck_vel, -self.max_puck_vel]), high=np.array([self.width, self.height, self.width, self.height, self.width, self.height, self.max_puck_vel, self.max_puck_vel]), dtype=np.float32)\n#self.create_circ_target(25)\nself.create_goal((400, 10), (self.width/2, 10/self.PPM))\n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        pak_vel = self.get_puck_velocity()\nreturn np.concatenate((agent_pos, pak_pos, pak_vel))\n</code></pre> <ul> <li><code>reset()</code></li> </ul> <pre><code>        #self.reset_target((np.random.uniform(self.agent_radius*2, self.width - self.agent_radius*2), np.random.uniform(self.agent_radius*2, self.height - self.agent_radius*2)))\n</code></pre> <ul> <li><code>step()</code></li> </ul> <pre><code>        self.limit_puck_velocity(self.max_puck_vel)\n#if self.calc_distance(self.get_puck_position(), self.get_target_position()) &lt; self.target_radius:\n#    reward = 1.0\n#    done = True\nif self._is_collision(self.object, self.goal):\nreward = 1.0\ndone = True    \n</code></pre> <ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        #self.draw_target(blue)\nself.draw_goal(blue)  \n</code></pre>"},{"location":"circ_env_pak_tocka/","title":"Dinamika agenta s pakom","text":""},{"location":"circ_env_pak_tocka/#popravki-funkcij","title":"Popravki funkcij","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        self.observation_space = spaces.Box(low=np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), high=np.array([self.width, self.height, self.width, self.height, self.width, self.height]), dtype=np.float32)\n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        return np.concatenate((agent_pos, target_pos, pak_pos))\n</code></pre> <ul> <li><code>step()</code></li> </ul> <p>Po spodnjem algoritmu napi\u0161ite kodo za izra\u010dun hitrosti paka.</p> <p></p> <p>Razlaga algoritma:</p> <ol> <li>Nastavite K na 2000 in B na 0.5</li> <li>Nastavite m na 1</li> <li>Pridobite polo\u017eaj agenta in ga shranite v pos_a</li> <li>Pridobite polo\u017eaj paka in ga shranite v pos_p</li> <li>Izra\u010dunajte razdaljo med agentom in pakom kot (agent_radius+object_radius) - calc_distance(pos_a, pos_p) in jo shranite v dist</li> <li>Ustvari spremenljivko FK z ni\u010dlami z enakimi dimenzijami kot pos_p</li> <li>\u010ce je dist &gt; 0, potem nastavite </li> <li>smer kot enotski vektor razlike med pos_a in pos_p in </li> <li>nastavite FK kot K krat dist krat smer</li> <li>Pridobite hitrost plo\u0161\u010dka in jo shranite v vel_p</li> <li>Nastavite FB kot -B krat vel_p</li> <li>Ustvari spremenljivko F z ni\u010del z enakimi dimenzijami kot pos_p</li> <li>Ustvari spremenljivko acc z ni\u010dlami, ki ima enake dimenzije kot pos_p</li> <li>Nastavite F kot vsoto FK in FB</li> <li>Nastavite acc kot F, deljeno z m</li> <li>Posodobite hitrost plo\u0161\u010dka tako, da jo nastavite na vel_p plus acc krat TIME_STEP</li> </ol> <p></p> <p>Izra\u010dun trka med krogoma </p> <p>Koda za nagrado in kon\u010danje epizode:</p> <pre><code>        if self.calc_distance(self.get_puck_position(), self.get_target_position()) &lt; self.target_radius:\nreward = 1.0\ndone = True\n#if self._is_collision(self.agent, self.object):\n#    reward = 1.0\n#    done = True\n</code></pre>"},{"location":"circ_env_pak_tocka/#dotik-s-pakom","title":"Dotik s pakom","text":"<ul> <li><code>step()</code></li> </ul> <p>V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za pozitivno nagrado <code>reward = 1.0</code> in kon\u010danje epizode.</p> <pre><code>        if is_contact_between_agent_and_pack\n...\nreward = 1.0\ndone = True \n</code></pre>"},{"location":"circ_env_pak_tocka/#odbijanje-paka-izven-okolja","title":"Odbijanje paka izven okolja","text":"<ul> <li><code>step()</code></li> </ul> <p>V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za majhno pozitivno nagrado <code>reward = 0.1</code>.</p> <pre><code>        if is_contact_between_agent_and_pack\n...\nreward = 0.1\n#done = True\n</code></pre> <pre><code>        if self.is_puck_outside_screen():\nreward = 1.0\ndone = True  \n</code></pre>"},{"location":"circ_env_slo/","title":"Okolje Circle Envrionment","text":"<p>Slede\u010da Python koda definira novo okolje za orodje OpenAI Gym. Okolje je pravokotno, agent v obliki kroga pa se lahko giblje v njem. Agent se mora izogibati trkom s steno oziroma mejo okolja. Gibanje agenta in trki v okolju so simulirani z uporabo knji\u017enic Pybox2D in prikazani s knji\u017enjico Pygame.</p> <p>Okolje je definirano kot Python razred <code>CircleEnvironment</code>, ki deduje iz razreda <code>gym.Env</code>. Razred definira ve\u010d funkcij, vklju\u010dno s funkcijami <code>__init__()</code>, <code>reset()</code>, <code>step()</code>, <code>render()</code> in <code>close()</code>, ki so del API okolja OpenAI Gym.</p>"},{"location":"circ_env_slo/#uporabljene-funkcije","title":"Uporabljene funkcije","text":"<p>The code imports the following libraries:</p> <ul> <li><code>gym</code>: odprtokodno orodje za razvoj in primerjavo algoritmov za spodbujevalno u\u010denje. OpenAI Gym je zbirka orodij za razvoj in testiranje algoritmov za spodbujevalno u\u010denje. Zagotavlja nabor okolij za testiranje in u\u010denje agentov za spodbujevalno u\u010denje ter standardizirane vmesnike za interakcijo z okolji.</li> <li><code>numpy</code>: Python knji\u017enica za numeri\u010dne izra\u010dune.</li> <li><code>math</code>: knji\u017enica z matemati\u010dnimi funkcijami.</li> <li><code>pygame</code>: knji\u017enica za razvoj iger v Pythonu. Pygame je med-platformni nabor modulov Python, zasnovan za pisanje iger. Vklju\u010duje funkcije za upravljanje grafike, zvoka, vhodnih naprav in omre\u017eja. Pygame lahko uporabite za ustvarjanje preprostih 2D iger ali bolj zapletenih iger s fiziko in umetno inteligenco.</li> <li><code>Box2D</code>: 2D fizikalni pogon za razvoj iger. Box2D je 2D fizikalni pogon za simulacije fizikalnih sistemov. Uporablja se lahko za simulacije v igrah, robotiki, ra\u010dunalni\u0161kem vidu in na drugih podro\u010djih. Zagotavlja realisti\u010dno simulacijo trkov, sil, trenja in drugih fizikalnih interakcij med predmeti v 2D-prostoru.</li> </ul>"},{"location":"circ_env_slo/#spremenljivke","title":"Spremenljivke","text":"<p>Okolje uporablja naslednje spremenljivke:</p> <ul> <li><code>PPM</code>: \u0161tevilo slikovnih pik na meter, uporabljenih za prikaz okolja.</li> <li><code>TARGET_FPS</code>: hitrost osve\u017eevanja slike, ki se uporablja za prikaz okolja.</li> <li><code>TIME_STEP</code>: dol\u017eina trajanja posameznega \u010dasovnega koraka simulacije.</li> <li><code>WORLD_WIDTH</code>: \u0161irina okolja v <code>Box2D</code> simulacijskih enotah [m].</li> <li><code>WORLD_HEIGHT</code>: vi\u0161ina okolja v <code>Box2D</code> simulacijskih enotah [m].</li> <li><code>metadata</code>: slovar (ang. dictionary), ki vsebuje metapodatke o okolju, vklju\u010dno s razpolo\u017eljivimi na\u010dini prikazovanje okolja in hitrostjo osve\u017eevanja slike.</li> <li><code>contact_listener</code>: an instance of a custom contact listener class that is used to detect collisions between the agent and the environment.</li> </ul>"},{"location":"circ_env_slo/#uporabljene-standardne-gym-funkcije","title":"Uporabljene standardne <code>gym</code> funkcije","text":"<p>Okolje vsebuje naslednje funkcije:</p> <ul> <li><code>__init__()</code>: funkcija inicializacije okolja. Nastavi parametre okolja, inicializira svet (ang. world) Pybox2D in nastavi dimenzije za stanja (observation space) in akcije (action space).</li> <li><code>_get_obs()</code>: funkcija, ki vrne trenutno stanje agenta oziroma okolja, tj. njegovo pozicijo v svetu.</li> <li><code>reset()</code>:  funkcija, ki ponastavi okolje na njegovo za\u010detno stanje in vrne za\u010detno stanje.</li> <li><code>step()</code>: funkcija, ki kot vhod sprejme akcijo, simulira okolje za en korak \u010dasa in vrne novo stanje, nagrado, zastavico <code>done</code> in dodatne informacije o simulaciji.</li> <li><code>render()</code>:  funkcija, ki prika\u017ee okolje z uporabo izbranega na\u010dina prikaza.</li> <li><code>_render_frame()</code>: funkcija, ki prika\u017ee posamezno sliko okolja.</li> <li><code>close()</code>: funkcija, ki pobri\u0161e svet Pybox2D in agenta ter zapre okno Pygame.</li> </ul>"},{"location":"circ_env_slo/#dodatne-funkcije","title":"Dodatne funkcije","text":"<ul> <li> <p><code>_is_collision(object, goal)</code>: Checks if there is a collision between the body object and the body goal. It takes in two b2Body objects, object and goal, and returns a boolean indicating whether there is a collision. It can take any two <code>Box2D</code> b2Body objects to check collison between them and not just body object and the body goal.</p> </li> <li> <p><code>create_agent(radius_px)</code>: Creates a new b2Body object representing the agent. It takes in the radius of the agent in pixels, <code>radius_px</code>.</p> </li> <li> <p><code>create_puck(radius_px, type)</code>: Creates a new b2Body object representing the puck. It takes in the radius of the puck in pixels, <code>radius_px</code>, and a string <code>type</code> that indicates whether the puck should be a dynamic object ('d') or a kinematic object ('k').</p> </li> <li> <p><code>create_goal(dim_px, position)</code>: Creates a new b2Body object representing the goal. It takes in a tuple <code>dim_px</code>representing the dimensions of the goal in pixels, and a tuple <code>position</code> representing the position of the goal in the simulation.</p> </li> <li> <p><code>create_circ_target(radius_px)</code>: Creates a new circular target. It takes in the radius of the target in pixels, radius_px.</p> </li> <li> <p><code>create_random_object(radius_px)</code>: Creates a new random object with a random shape. It takes in the radius of the object in pixels, radius_px..</p> </li> <li> <p><code>create_border()</code>: Creates the boundary of the simulation.</p> </li> <li> <p><code>get_agent_position()</code>: Returns the current position of the agent as a numpy array.</p> </li> <li> <p><code>get_puck_position()</code>: Returns the current position of the puck as a numpy array.</p> </li> <li> <p><code>get_puck_velocity()</code>: Returns the current velocity of the puck as a numpy array.</p> </li> <li> <p><code>get_target_position()</code>: Returns the current position of the target as a numpy array.</p> </li> <li> <p><code>reset_agent(position)</code>: Resets the position and velocity of the agent to a specified position.</p> </li> <li> <p><code>reset_puck(position)</code>: Resets the position and velocity of the puck to a specified position.</p> </li> <li> <p><code>reset_target(position)</code>: Resets the position of the target to a specified position.</p> </li> <li> <p><code>reset_random_object(position)</code>: Resets the position and velocity of the random object to a specified position.</p> </li> <li> <p><code>set_agent_velocity(vel)</code>: This function sets the linear and angular velocity of the agent in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the agent.</p> </li> <li> <p><code>set_puck_velocity(vel)</code>: This function sets the linear and angular velocity of the puck in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the puck.</p> </li> <li> <p><code>limit_puck_velocity(maximal_velocity)</code>: This function limits the velocity of a puck object to the maximum velocity specified by <code>maximal_velocity</code>. The function takes a single input parameter, <code>maximal_velocity</code>, which is a float value representing the maximum velocity allowed. The function does not return any values.</p> </li> <li> <p><code>draw_agent(color)</code>: This function draws the agent as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the agent is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_puck(color)</code>: This function draws the puck as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the puck is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_border(color)</code>: This function draws the border of the game as a rectangle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color.</p> </li> <li> <p><code>draw_target(color)</code>: This function draws the target as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the target is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_goal(color)</code>: This function draws the goal as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color.</p> </li> <li> <p><code>draw_random_object(color)</code>: This function draws a random object as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position, angle and vertices of the object are converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>is_agent_outside_screen()</code>: This function checks if the agent is outside the screen or not. It returns True if the agent is outside the screen, otherwise it returns False.</p> </li> <li> <p><code>calc_distance(pos1, pos2)</code>: This function calculates the Euclidean distance between two points. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points.</p> </li> <li> <p><code>unit_vector(pos1, pos2)</code>: This function calculates the unit vector pointing from pos1 to pos2. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. The function returns the unit vector as a numpy array.</p> </li> <li> <p><code>calculate_component(pos_agent, pos_target, vel)</code>: This function calculates the component of the velocity <code>vel</code> in the direction of the unit vector pointing from <code>pos_agent</code> to <code>pos_target</code>. The input parameters <code>pos_agent</code> and <code>pos_target</code> are tuples containing the x and y coordinates of the points, while <code>vel</code> is a numpy array representing the velocity vector. The function returns the component of <code>vel</code> as a float value.</p> </li> <li> <p><code>move_agent_mouse()</code>: This function calculates the movement action of the agent based on the position of the mouse relative to the agent's position. The function returns the <code>action</code> as a numpy array.</p> </li> </ul>"},{"location":"circ_env_slo/#koda","title":"Koda","text":""},{"location":"circ_env_slo/#hiter-pregled-kode","title":"Hiter pregled kode","text":""},{"location":"circ_env_slo/#ustvarimo-agenta-dolocimo-observation_space-in-action_space","title":"Ustvarimo agenta, dolo\u010dimo <code>observation_space</code> in <code>action_space</code>.","text":"<ul> <li><code>__init__()</code> <pre><code>        self.observation_space = spaces.Box(low=np.array([0.0, 0.0]), high=np.array([self.width, self.height]), dtype=np.float32)\nself.action_space = spaces.Box(low=-2.0, high=2.0, shape=(2,), dtype=np.float32)\nself.create_agent(agent_radius_px)        \n</code></pre></li> </ul>"},{"location":"circ_env_slo/#potrebujemo-vsaj-en-podatek-za-observacijo","title":"Potrebujemo vsaj en podatek za observacijo.","text":"<ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        agent_pos = self.get_agent_position()\nreturn agent_pos\n</code></pre>"},{"location":"circ_env_slo/#ponastavimo-agenta-v-reset-funkciji","title":"Ponastavimo agenta v <code>reset()</code> funkciji.","text":"<ul> <li><code>reset()</code></li> </ul> <pre><code>        self.reset_agent((np.random.uniform(self.agent_radius*1.5, self.width - self.agent_radius*1.5), np.random.uniform(self.agent_radius*1.5, self.height - self.agent_radius*1.5)))\n</code></pre>"},{"location":"circ_env_slo/#naredimo-korak-v-step-funkciji","title":"Naredimo korak v <code>step()</code> funkciji.","text":"<ul> <li><code>step()</code></li> </ul> <p>Premik agenta za <code>action</code></p> <pre><code>        self.set_agent_velocity(action)\n</code></pre> <p>Dolo\u010dimo <code>obs</code> spremenljivko.</p> <pre><code>        obs = self._get_obs()\n</code></pre> <p>Dolo\u010dimo, <code>reward</code> in <code>done</code> spremenljivki. Za za\u010detek inicializacija:</p> <pre><code>        reward = 0.0\ndone = False \n</code></pre> <p>Nato lahko dolo\u010dimo osnovne pogoje:</p> <pre><code>        if self.current_step &gt;= self.time_steps:\n#reward = -1.0\ndone = True\n</code></pre>"},{"location":"circ_env_slo/#izrisemo-agenta","title":"izri\u0161emo agenta","text":"<ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        self.draw_agent(green)\n</code></pre>"},{"location":"circ_env_slo/#na-koncu-izbrisemo-agenta-v-close-funkciji","title":"Na koncu izbri\u0161emo agenta v <code>close()</code> funkciji","text":"<ul> <li><code>close()</code></li> </ul> <pre><code>        self.world.DestroyBody(self.agent)\n</code></pre>"},{"location":"circ_env_slo/#zazenemo-okolje","title":"Za\u017eenemo okolje","text":"<p>Za\u017eeni skripto <code>playCirc.py</code></p> <pre><code>python3 playCirc.py\n</code></pre>"},{"location":"circ_env_slo/#premikanje-agenta-s-tipkovnico","title":"Premikanje agenta s tipkovnico","text":"<p>V skripti pove\u010dajte hitrost na maksimalno hitrost <code>2</code>.</p> playCirc.py<pre><code>while True:\nif any([event.type == pygame.QUIT for event in pygame.event.get()]): break\n#player controls\nkeys = pygame.key.get_pressed() \nif keys[pygame.K_LEFT]: x = -1\nelif keys[pygame.K_RIGHT]: x = 1\nelse: x = 0\nif keys[pygame.K_UP]: y = -1\nelif keys[pygame.K_DOWN]: y = 1\nelse: y = 0   \naction = np.array([x,y],dtype=np.float32)\n#action = env.action_space.sample()\n#print(action)\nobs, reward, done, _ = env.step(action)\n# Render the environment\nenv.render()\n# Check if the episode is finished\nif done:\nobs = env.reset()\n# Close the environment\nenv.close()\n</code></pre>"},{"location":"circ_env_slo/#dodajmo-osnovno-mejo","title":"Dodajmo osnovno mejo","text":"<ul> <li><code>__init__()</code></li> </ul> <p><pre><code>        self.create_border()    \n</code></pre> - <code>_render_frame()</code></p> <pre><code>        self.draw_border(black)\n</code></pre> <p>Zaklju\u010dimo episodo, ko se agent dotakne meje.</p> <ul> <li><code>step()</code></li> </ul> <pre><code>        if self._is_collision(self.agent, self.border):\nreward = -1.0\ndone = True  \n</code></pre>"},{"location":"circ_env_tocka/","title":"Premik agenta v ciljno to\u010dko","text":""},{"location":"circ_env_tocka/#koda-posameznih-funkcij-v-circle_worldpy","title":"Koda posameznih funkcij v <code>circle_world.py</code>","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        self.observation_space = spaces.Box(low=np.array([0.0, 0.0, 0.0, 0.0]), high=np.array([self.width, self.height, self.width, self.height]), dtype=np.float32)\ntarget_radius_px = 30\nself.create_circ_target(target_radius_px)    \n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        agent_pos = self.get_agent_position()\ntarget_pos = self.get_target_position()\nreturn np.concatenate((agent_pos, target_pos))\n</code></pre> <ul> <li><code>reset()</code></li> </ul> <pre><code>        self.reset_target((np.random.uniform(self.agent_radius*2, self.width - self.agent_radius*2), np.random.uniform(self.agent_radius*2, self.height - self.agent_radius*2)))\n</code></pre> <ul> <li><code>step()</code></li> </ul> <pre><code>        if self.calc_distance(self.get_agent_position(), self.get_target_position()) &lt; self.target_radius:\nreward = 1.0\ndone = True\nif self.current_step &gt;= self.time_steps:\nreward = -1.0\ndone = True\n</code></pre> <ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        self.draw_target(blue)\n</code></pre>"},{"location":"circ_env_tocka/#nastavitve-ucenja","title":"Nastavitve u\u010denja","text":"<p>Izhodi\u0161\u010dne vrednosti za u\u010denje:</p> <pre><code>model = PPO('MlpPolicy', \nenv=env, \ntensorboard_log=logdir, \nverbose=1, \nn_steps=512,   \nbatch_size=256,   \ngae_lambda=0.9,   \ngamma=0.99,  \nn_epochs=5,  \nent_coef=0.0,  \nlearning_rate=2.5e-4,  \nclip_range=0.3,\nseed = 2)\n</code></pre> <p>Primer izpisa uspe\u0161nega u\u010denja s PPO metodo:</p> <pre><code>-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 54.9        |\n|    ep_rew_mean          | 1           |\n| time/                   |             |\n|    fps                  | 2638        |\n|    iterations           | 8           |\n|    time_elapsed         | 1           |\n|    total_timesteps      | 577536      |\n| train/                  |             |\n|    approx_kl            | 0.012461845 |\n|    clip_fraction        | 0.052       |\n|    clip_range           | 0.3         |\n|    entropy_loss         | -2          |\n|    explained_variance   | 0.993       |\n|    learning_rate        | 0.00025     |\n|    loss                 | -0.00576    |\n|    n_updates            | 5635        |\n|    policy_gradient_loss | -0.00652    |\n|    std                  | 0.66        |\n|    value_loss           | 0.000254    |\n-----------------------------------------\n</code></pre> <p>Potek u\u010denja za tar\u010do velikosti 30 px </p> <p>Potek u\u010denja za tar\u010do velikosti 15 px </p> <p></p>"},{"location":"circ_env_train/","title":"U\u010denje s spodbujevalnim u\u010denjem","text":""},{"location":"circ_env_train/#skripta-za-ucenje-train_circpy","title":"Skripta za u\u010denje <code>train_circ.py</code>","text":"train_circ.py<pre><code>import gym\nimport numpy as np\nfrom sb3_contrib import TQC\n#from stable_baselines3 import PPO\nimport os\nimport circ_env\nenv = gym.make('circ_env/Circle-v0')\n# tensorboard logiranje\nmodels_dir = \"models/TQC01\"\nif not os.path.exists(models_dir):\nos.makedirs(models_dir)\nlogdir = \"logs\"\nif not os.path.exists(logdir):  \nos.makedirs(logdir)\n# deklaracija modela\npolicy_kwargs = dict(n_critics=2, n_quantiles=25)\nmodel = TQC('MlpPolicy', env=env, tensorboard_log=logdir, verbose=1,policy_kwargs=policy_kwargs)\n# Reset the environment\nobs = env.reset()\n# iteracija skozi u\u010denje in shranjevanje modela\nTIMESTEPS = 10000\niters = 0\nwhile True:\niters += 1\nmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False,tb_log_name=\"TQC_01\")\nmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n</code></pre>"},{"location":"circ_env_train/#zagon-ucenja-in-logiranja","title":"Zagon u\u010denja in logiranja","text":"<ul> <li> <p>Terminal 1 <pre><code>python3 train_circ.py\n</code></pre></p> </li> <li> <p>Terminal 2 <pre><code>tensorboard --logdir=\"logs\"\n</code></pre></p> </li> </ul>"},{"location":"circ_env_train/#zagon-naucenega-agenta-load_circ_modelpy","title":"Zagon nau\u010denega agenta <code>load_circ_model.py</code>","text":"load_circ_model.py<pre><code>import gym\nimport numpy as np\nfrom sb3_contrib import TQC\n#from stable_baselines3 import PPO\nimport os\nimport circ_env\nenv = gym.make('circ_env/Circle-v0', render_mode=\"human\")\nmodel = TQC.load(\"./models/TQC07/2260000\", env=env)\n# Reset the environment\nobs = env.reset()\nEPISODES = 1000\nfor episode in range(EPISODES):\nobs = env.reset()\ndone = False\nwhile not done:\naction, _state = model.predict(obs, deterministic=True)\nobs, reward, done, _ = env.step(action)\nenv.render()\n# Close the environment\nenv.close()\n</code></pre>"},{"location":"cliff/","title":"Primer okolja Cliff Walking","text":"<p>Open AI Cliff Walking</p> <ol> <li> <p>Za\u010dnemo z novo python skripto</p> </li> <li> <p>Uvoz potrebnih paketov</p> <pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\n</code></pre> </li> <li> <p>Deklaracija okolja</p> <ul> <li><code>your_env = gym.make(\"YourEnv \", some_kwarg=your_vars)</code></li> <li> <p><code>your_env = gym.make(\"YourEnv\")</code></p> </li> <li> <p>Seznam okolij     https://gymnasium.farama.org/environments/toy_text/</p> </li> </ul> <pre><code>env = gym.make('CliffWalking-v0')\n</code></pre> </li> <li> <p>Resetiramo in prika\u017eemo okolje</p> <pre><code>env.reset()\nenv.render()\n</code></pre> </li> <li> <p>Prika\u017eemo nekaj parametrov</p> <pre><code>print( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\n</code></pre> </li> <li> <p>Deklariramo Q tabelo</p> <pre><code>q_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\n</code></pre> </li> <li> <p>Deklariramo parametre</p> <pre><code>learning_rate = 0.1\ndiscount_factor = 0.95\nepochs = 60000\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\n</code></pre> </li> <li> <p>Za\u010dnemo z u\u010denjem z izbranim \u0161tevilom epoh</p> <pre><code>for episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\n</code></pre> </li> <li> <p>Izvedemo posamezni \u201esprehod \u010dez okolje\u201c z izbiro akcij</p> <ul> <li>raziskovanje: naklju\u010dna akcija</li> <li>uporabo zbranega znanja: akcija z maximalno q vrednostjo</li> </ul> <pre><code>\u00a0 \u00a0 while not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Use the action with the highest q-value\naction = np.argmax(q_table[state]) \n</code></pre> </li> <li> <p>Izvedemo akcijo v okolju in preberemo vrednosti novega stanja in nagrade</p> <pre><code>        next_state, reward, done, info = env.step(action)\n</code></pre> </li> <li> <p>Posodobimo stanje Q tabele</p> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 curr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\n</code></pre> </li> <li> <p>Posodobimo stanje</p> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 state = next_state\n</code></pre> </li> <li> <p>Shranimo dol\u017eino trenutnega \u201esprehoda\u201c</p> <pre><code>        if episode % SHOW_EVERY == 0:\ntrial_length += 1\n</code></pre> </li> <li> <p>Celotna koda u\u010denja do sedaj</p> <pre><code>for episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> </li> <li> <p>Testiramo nau\u010deno Q tabelo oziroma agenta</p> <ul> <li>Resetiramo okolje in ga izri\u0161emo</li> </ul> <pre><code>print(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\n</code></pre> <ul> <li>Izvedemo sprehod</li> </ul> <pre><code>while not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre> </li> <li> <p>Ve\u010d testov, da vidimo uspe\u0161nost</p> <pre><code>lengths=[]\nfor trialnum in range(1, 11):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done and trial_length &lt; 25:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\nprint(\"Trial number \" + str(trialnum) + \" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\ntrial_length += 1\nlengths.append(trial_length)\nsleep(.2)\navg_len=sum(lengths)/10\nprint(avg_len)\n</code></pre> </li> <li> <p>Celotna koda</p> cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\nfor episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nprint(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\nwhile not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre> </li> <li> <p>Zagon skripte</p> </li> </ol> <p>Ob zagonu skripte s <code>python cliff.py</code> se bo pojavila napaka</p> <pre><code>/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py:44: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n  self._cliff = np.zeros(self.shape, dtype=np.bool)\nTraceback (most recent call last):\n  File \"/home/student/RL/rl_pet/cliff.py\", line 7, in &lt;module&gt;\n    env = gym.make('CliffWalking-v0')\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make\n    return registry.make(id, **kwargs)\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 129, in make\n    env = spec.make(**kwargs)\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 90, in make\n    env = cls(**_kwargs)\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py\", line 44, in __init__\n    self._cliff = np.zeros(self.shape, dtype=np.bool)\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/numpy/__init__.py\", line 305, in __getattr__\n    raise AttributeError(__former_attrs__[attr])\nAttributeError: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n</code></pre> <p>Najbolj informativen del so vrstice</p> <pre><code>  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py\", line 44, in __init__\n    self._cliff = np.zeros(self.shape, dtype=np.bool)\n</code></pre> <p>in</p> <pre><code>AttributeError: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n</code></pre> <p>Napaka nastane, ker je v <code>cliffwalking.py</code> napaka v vrstici 44, ker se uporablja star zapis za spremenljivko <code>bool</code> in sicer zapis <code>dtype=np.bool</code>. Tega je potrebno spremeniti v samo <code>bool</code>. Potrebno je odpreti datoteko, ki se nahaja npr. v <code>/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py</code>. Polna pot je odvisna od ime va\u0161ega python okolja, npr. <code>/rl_pet</code>. Odprite python datoteko in spremenite kodo.</p> <p>Stara koda cliffwalking.py<pre><code>        # Cliff Location\nself._cliff = np.zeros(self.shape, dtype=np.bool)\nself._cliff[3, 1:-1] = True\n</code></pre></p> <p>Nova koda cliffwalking.py<pre><code>        # Cliff Location\nself._cliff = np.zeros(self.shape, bool)\nself._cliff[3, 1:-1] = True\n</code></pre></p> <ol> <li>Celotna koda verzija 2</li> </ol> <p>V tej verziji kode je dodana pred\u010dasna zaustevative u\u010denja za posamezno epoho, \u010de u\u010denje prese\u017ee 99 korakov u\u010denja v posamezni epohi.</p> cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nmax_steps = 99\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\nfor episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\n#while not done:\nfor step in range(max_steps):\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif done:\nbreak\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nprint(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\nwhile not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre>"},{"location":"cliff2/","title":"Prikaz Q tabele","text":""},{"location":"cliff2/#prikaz-okolja","title":"Prikaz okolja","text":""},{"location":"cliff2/#tekstovni-izpis-okolja","title":"Tekstovni izpis okolja","text":"<pre><code>env.render()\n</code></pre>"},{"location":"cliff2/#graficni-prikaz-okolja","title":"Grafi\u010dni prikaz okolja","text":"<p>Okolje bomo prikazali grafi\u010dno z matplotlib knji\u017enico.</p> <p><pre><code>import matplotlib.pyplot as plt\n</code></pre> Nato na konec datoteke dodamo kodo:</p> <pre><code>fig1, ax1 = plt.subplots()\nax1.axis('off')\nax1.axis('tight')\nokolje = [[\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"S\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"G\"]]\nokolje_colours = np.asarray(okolje,dtype='U25')\nokolje_rows = len(okolje[0][:])\nokolje_columns = len(okolje[:][0])\nokolje_colours[okolje_colours == \"x\"] = \"firebrick\"\nokolje_colours[okolje_colours == \"G\"] = \"gold\"\nokolje_colours[okolje_colours == \"S\"] = \"limegreen\"\nokolje_colours[okolje_colours == \"o\"] = \"cornflowerblue\"\nokolje = [[\"o,1\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,12\"], [\"o,13\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,24\"], [\"o,25\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,36\"], [\"S,37\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"G,48\"]]\ntable_okolje = ax1.table(cellText = okolje,cellColours=okolje_colours, loc = 'center', cellLoc = 'center', rowLoc = 'center',colLoc = 'center')\ntable_okolje.scale(1,1.5)\ntable_okolje.auto_set_font_size(True)\ntable_okolje.set_fontsize(10)\nplt.show()\n</code></pre>"},{"location":"cliff2/#tekstovni-izpis-q-tabele","title":"Tekstovni izpis Q tabele","text":"<pre><code>print(q_table)\n</code></pre>"},{"location":"cliff2/#graficni-izpis-q-tabele","title":"Grafi\u010dni izpis Q tabele","text":"<pre><code>fig2, ax2 = plt.subplots()\nfig2.patch.set_visible(False)\nax2.axis('off')\ncolumns = [\"GOR\",\"DESNO\",\"DOL\",\"LEVO\"]\nrows = [\"STANJE %d\" %(i+1) for i in range(env.observation_space.n)]\nqtable = np.around(qtable,3)\nqtable_s = qtable[:][0:22]\nrows_s = rows[0:22]\nnorm = plt.Normalize(qtable_s.min(), qtable_s.max()+0.1)\ncolours = plt.cm.YlGn(norm(qtable_s))\ntable = ax2.table(cellText=qtable_s, rowLabels=rows_s, colLabels=columns, loc = 'center', cellColours=colours,cellLoc ='center',rowLoc='center', colLoc ='center',colWidths=[0.1,0.1,0.1,0.1,0.1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(8)\nfig3, ax3 = plt.subplots()\nfig3.patch.set_visible(False)\nax3.axis('off')\nqtable_s = qtable[:][23:48]\nrows_s = rows[23:48]\ncolours = plt.cm.YlGn(norm(qtable_s))\ntable = ax3.table(cellText=qtable_s, rowLabels=rows_s, colLabels=columns, loc = 'center', cellColours=colours,cellLoc ='center',rowLoc='center', colLoc ='center',colWidths=[0.1,0.1,0.1,0.1,0.1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(8)\nplt.show()\n</code></pre>"},{"location":"cliff3/","title":"Eksploracija","text":""},{"location":"cliff3/#vizualizacija-funkcije-za-epsilon","title":"Vizualizacija funkcije za epsilon","text":"epsilon.py<pre><code>import matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider, Button\nimport numpy as np\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.01            # Minimum exploration probability\ndecay_rate = 0.001            # Exponential decay rate for exploration prob\ntotal_episodes = 10000\ndef plot_fcn(min_epsilon_fcn, max_epsilon_fcn, decay_rate_fcn, total_episodes_fcn):\nepsilon_values = [(min_epsilon_fcn + (max_epsilon_fcn - min_epsilon_fcn) * np.exp(-decay_rate_fcn * episode)) for episode in range(total_episodes_fcn)]\nep_num = range(total_episodes_fcn)\nreturn epsilon_values\n#define inital values for sliders\ninit_min_epsilon = min_epsilon\ninit_max_epsilon = max_epsilon\ninit_decay_rate = decay_rate\ninit_total_episodes = total_episodes\nfig1,ax1 = plt.subplots()\nline, = plt.plot(plot_fcn(init_min_epsilon,init_max_epsilon,init_decay_rate,init_total_episodes), lw=2)\nax1.set_xlabel(\"Epizoda\", fontsize = 15)\nax1.set_ylabel(\"Epsilon vrednost\",fontsize = 15)\nplt.subplots_adjust( bottom=0.3)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"Vrednost parametra epsilon v odvisnosti od \u0161tevila epizod (u\u010denje Q)\",fontsize = 18)\nplt.gcf().text(0.4,0.05,r'$\\epsilon = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min})*e^{(\\lambda*N)}$',fontsize=18)\n#slider for decay rate\nax_decay_rate = plt.axes([0.25, 0.1, 0.65, 0.03])\ndecay_rate_slider = Slider(\nax=ax_decay_rate,\nlabel='Lambda',\nvalmin=0.0001,\nvalmax=0.01,\nvalinit=init_decay_rate,\n)\n#slider for max epsilon value\nax_max_epsilon= plt.axes([0.25, 0.13, 0.65, 0.03])\nmax_epsilon_slider = Slider(\nax=ax_max_epsilon,\nlabel='Epsilon (max)',\nvalmin=0.1,\nvalmax=1,\nvalinit=init_max_epsilon,\n)\n#slider for min epsilon value\nax_min_epsilon= plt.axes([0.25, 0.16, 0.65, 0.03])\nmin_epsilon_slider = Slider(\nax=ax_min_epsilon,\nlabel='Epsilon (min)',\nvalmin=0,\nvalmax=0.5,\nvalinit=init_min_epsilon,\n)\n#slider for max episode value\nax_total_episodes= plt.axes([0.25, 0.19, 0.65, 0.03])\ntotal_episodes_slider = Slider(\nax=ax_total_episodes,\nlabel='Skupno \u0161tevilo epizod - N',\nvalmin=1,\nvalstep=1,\nvalmax=10000,\nvalinit=init_total_episodes,\n)\ndef update(val):\nline.set_data(range(total_episodes_slider.val),plot_fcn(min_epsilon_slider.val,max_epsilon_slider.val, decay_rate_slider.val,total_episodes_slider.val))\n#ax3.set_xlim(0,total_episodes_slider.val)\nax3.autoscale_view(True,True,True)\nax3.relim()\nfig.canvas.draw_idle()\ndecay_rate_slider.on_changed(update)\nmax_epsilon_slider.on_changed(update)\nmin_epsilon_slider.on_changed(update)\ntotal_episodes_slider.on_changed(update)\ndecay_rate_slider.label.set_size(16)\nmax_epsilon_slider.label.set_size(16)\nmin_epsilon_slider.label.set_size(16)\ntotal_episodes_slider.label.set_size(16)\nresetax = plt.axes([0.8, 0.025, 0.1, 0.04])\nbutton = Button(resetax, 'Reset', hovercolor='0.975')\nbutton.label.set_size(16)\ndef reset(event):\ndecay_rate_slider.reset()\nmax_epsilon_slider.reset()\nmin_epsilon_slider.reset()\ntotal_episodes_slider.reset()\nax3.autoscale_view(True, True, True)\nax3.relim()\nbutton.on_clicked(reset)\nplt.show()\n</code></pre>"},{"location":"cliff3/#sprememba-parametrov-za-izbolsanje-ucenja","title":"Sprememba parametrov za izbol\u0161anje u\u010denja","text":"cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nimport matplotlib.pyplot as plt\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nmax_steps = 99\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n</code></pre> <p>Zamenjamo z </p> <pre><code># Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.01            # Minimum exploration probability\ndecay_rate = 0.001            # Exponential decay rate for exploration prob\n</code></pre> <p>Zamenjamo krivuljo za <code>epsilon</code> spremenljivko </p> <pre><code>    if END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> <p>s kodo</p> <pre><code>    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n</code></pre>"},{"location":"conda_env/","title":"Priprava python okolja","text":""},{"location":"conda_env/#paket-conda","title":"Paket conda","text":"<p>Paket conda je priljubljen upravitelj paketov za Python, ki se pogosto uporablja za razvoj in testiranje Python programov. Je tudi odli\u010dno orodje za upravljanje in vzdr\u017eevanje okolij Pythona, zlasti za usposabljanje in razvoj algoritmov spodbujevalnega u\u010denja (RL).</p> <p>S paketom conda lahko preprosto ustvarite in upravljate okolja Python, kar vam omogo\u010da delo na razli\u010dnih projektih z razli\u010dnimi knji\u017enicami in algoritmi, ne da bi vas skrbelo upravljanje ve\u010d kopij istega paketa. Tako lahko na primer ustvarite novo okolje za projekt RL in vanj namestite potrebne knji\u017enice in algoritme. To okolje lahko nato uporabite za u\u010denje in razvoj modela RL. Paket conda zagotavlja tudi veliko vgrajenih funkcij za upravljanje in organizacijo okolja, kot so spremenljivke okolja in upravitelji paketov. Paket conda je torej zmogljivo orodje za upravljanje in vzdr\u017eevanje okolij Pythona, zlasti za u\u010denje in razvoj algoritmov spodbujevalnega u\u010denja. Omogo\u010da enostavno ustvarjanje in upravljanje okolij, namestitev potrebnih knji\u017enic in algoritmov ter obdelavo velikih koli\u010din podatkov. S paketom conda lahko preprosto zgradite in namestite svoj model RL ter hitro za\u010dnete s svojim prvim projektom.</p> <p>Bistvene to\u010dke:</p> <ul> <li>Ustvarjanje razli\u010dnih okolij v katerih imate lahko in\u0161talirane programske pakete za rezli\u010dne pakete.</li> <li>In\u0161talacije so lo\u010dene med okolji in ne vplivajo drugo na drugo.</li> <li>Deluje na operacijskih sistemih Windowsi in Linux.</li> <li>Spletna stran: https://www.anaconda.com/</li> <li>In\u0161talacija po navodilih<ul> <li>dodatna nastavitev <code>conda config --set auto_activate_base false</code></li> </ul> </li> </ul>"},{"location":"conda_env/#ustvarjanje-python-okolja-s-conda-paketom","title":"Ustvarjanje python okolja s conda paketom","text":"<p><code>conda create --name rl_test python=3.9</code></p>"},{"location":"conda_env/#aktivacija-okolja","title":"Aktivacija okolja","text":"<p><code>conda activate rl_test</code></p>"},{"location":"conda_env/#deaktivacija-okolja","title":"Deaktivacija okolja","text":"<p><code>conda deactivate</code></p>"},{"location":"conda_env/#organizacija-vaj","title":"Organizacija vaj","text":""},{"location":"conda_env/#cetrtkova-skupina","title":"\u010cetrtkova skupina","text":"<ul> <li>conda okolje <code>rl_cet</code></li> </ul>"},{"location":"conda_env/#petkova-skupina","title":"Petkova skupina","text":"<ul> <li>conda okolje <code>rl_pet</code></li> </ul>"},{"location":"conda_env/#navodila","title":"Navodila","text":"<ol> <li>Odprite terminal z bli\u017enjico CTRL+ALT+T</li> <li>Pomaknite se v mapo <code>RL</code> <pre><code>cd RL\n</code></pre></li> <li>Ustvarite va\u0161e mapo, npr. <code>rl_cet</code>:    <pre><code>mkdir rl_cet\ncd rl_cet\n</code></pre></li> <li>Ustvarite python okolje <code>rl_cet</code> s conda paketom    <pre><code>conda create --name rl_cet python=3.9\n</code></pre></li> <li>Na vpra\u0161anje <code>Proceed ([y]/n)?</code> odgovorite z <code>y</code>.</li> <li>Aktivirajte okolje     <pre><code>conda activate rl_cet\n</code></pre></li> <li>In\u0161talacija python paketov za spodbujevalno u\u010denje    <pre><code>pip install gym==0.21.0\npip install pyglet==1.5.27\npip install stable-baselines3\npip install sb3_contrib\npip install tensorboard\n</code></pre></li> </ol>"},{"location":"hockey_env/","title":"Hockey Environment","text":""},{"location":"hockey_env/#opis","title":"Opis","text":"<p>Koda definira okolje za simulacijo igre zra\u010dnega hokeja. Okolje se izvaja z uporabo fizikalnega motorja Pybox2D in API OpenAI Gym. Okolje je sestavljeno iz pravokotnega igralnega polja z osrednjo \u010drto, ki polje deli na dve polovici. Na igri\u0161\u010du sta dva igralca, enega upravlja agent umetne inteligence, drugega pa hevristi\u010dni algoritem. Cilj igre je izstreliti plo\u0161\u010dek v nasprotnikova vrata in hkrati braniti svoja vrata. </p> <p>Koda definira razred z imenom <code>HockeyEnv</code>, ki deduje iz razreda <code>gym.Env</code>. Konstruktor razreda inicializira razli\u010dne parametre okolja, kot so dimenzije igralnega polja, velikosti plo\u0161\u010dka in igralcev, najve\u010dje hitrosti igralcev in plo\u0161\u010dka ter \u0161tevilo \u010dasovnih korakov za vsako epizodo. Opredeljena sta tudi prostor stanj in akcij okolja. Prostor stanj je 12-razse\u017eni zvezni prostor, ki predstavlja polo\u017eaje in hitrosti agenta, nasprotnika in plo\u0161\u010dka. Prostor stanj je dvodimenzionalni zvezni prostor, ki predstavlja hitrosti x in y agenta.</p> <p>Razred <code>HockeyEnv</code> ustvari svet Pybox2D in inicializira entitete igre, kot so igralci, plo\u0161\u010dek in vratnice. Okolje ustvari tudi dva objekta umetne inteligence, <code>top_ai</code> in <code>bottom_ai</code>, ki nadzorujeta gibanje nasprotnika. Ta objekta uporabljata preprosto hevristi\u010dno strategijo sledenja plo\u0161\u010dku in premikanja proti njemu za obrambo gola.</p> <p>Koda opredeljuje tudi razred <code>ContactListener</code>, ki podeduje razred <code>b2ContactListener</code>, ki ga zagotavlja Pybox2D. Ta razred se uporablja za zaznavanje trkov med entitetami igre. </p> <p>Na koncu razred <code>HockeyEnv</code> definira ve\u010d metod za pridobivanje polo\u017eajev in hitrosti igralnih entitet, posodabljanje stanja igre in prikazovanja igre. Igra se lahko v realnem \u010dasu prika\u017ee v oknu Pygame, zaslon pa se lahko shrani v obliki videoposnetka.</p> <p></p>"},{"location":"load_car/","title":"Zagon agenta","text":"<ol> <li>Skripta za izvajanje agenta na podlagi shranjene Q tabele    Ustvarite novo skripto <code>load_car.py</code>.</li> <li> <p>Inicializacija okolja</p> <pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\n</code></pre> </li> <li> <p>Inicializacija agenta: nalo\u017eimo shranjeno Q tabelo</p> <p><pre><code>q_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\n</code></pre> 4. Za\u017eenemo okolje in agenta <pre><code>state = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\n</code></pre></p> </li> <li> <p>Celotna koda</p> </li> </ol> load_car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\n</code></pre>"},{"location":"move_agent_mouse/","title":"Premik agenta z mi\u0161ko","text":"<ul> <li><code>step()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\naction = self.move_agent_mouse()\n</code></pre>"},{"location":"playCirc_mouse/","title":"playCirc mouse","text":"playCirc.py<pre><code>import gym\nimport numpy as np\nimport pygame\nimport circ_env\nenv = gym.make('circ_env/Circle-v0', render_mode=\"human\")\n# Reset the environment\nobs = env.reset()\nenv.render()\n# Run the environment with random actions\n#for i in range(500):\nstart_pos = None\nreset_time = 0\nwhile True:\n#if any([event.type == pygame.QUIT for event in pygame.event.get()]): break\n#player controls\nkeys = pygame.key.get_pressed() \nif keys[pygame.K_LEFT]: x = -1\nelif keys[pygame.K_RIGHT]: x = 1\nelse: x = 0\nif keys[pygame.K_UP]: y = -1\nelif keys[pygame.K_DOWN]: y = 1\nelse: y = 0   \nfor event in pygame.event.get():\nif event.type == pygame.MOUSEBUTTONDOWN:\n# Set the starting position\nstart_pos = pygame.mouse.get_pos()\nelif event.type == pygame.MOUSEBUTTONUP:\n# Reset the starting position\nstart_pos = None\nif start_pos is not None:\ncurrent_pos = pygame.mouse.get_pos()\nif pygame.time.get_ticks() - reset_time &gt;= 100:\n# Reset the starting position every second\nstart_pos = current_pos\nreset_time = pygame.time.get_ticks()        \ndx = current_pos[0] - start_pos[0]\ndy = current_pos[1] - start_pos[1]\nx = np.sign(dx)\ny = np.sign(dy)\naction = np.array([x,y],dtype=np.float32)\n#action = env.action_space.sample()\n#print(action)\nobs, reward, done, _ = env.step(action)\n# Render the environment\nenv.render()\n# Check if the episode is finished\nif done:\nobs = env.reset()\nstart_pos = None\nreset_time = 0\n# Close the environment\nenv.close()\n</code></pre>"},{"location":"save_models/","title":"Logiranje in spremljanje u\u010denja","text":""},{"location":"save_models/#python-skripta-za-testiranje-agenta","title":"Python skripta za testiranje agenta","text":"load_carDQN2.py<pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport matplotlib.pyplot as plt\nimport numpy as np\nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\nmodel = DQN.load(\"dqn_car\", env=env)\nmean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\nprint(f'Mean reward: {mean_reward}, Std reward: {std_reward}')\nreward_list = [] ###\nfor episode in range(50): ###\nstate = env.reset()\ndone = False\nprint(\"EPISODE \", episode)\ntot_reward, reward = 0, 0 ###\nwhile not done:\naction, _state = model.predict(state, deterministic=True)\nstate, reward, done, info = env.step(action)\ntot_reward += reward ###\nreward_list.append(tot_reward) ###\nenv.close()\nf = plt.figure()\nplt.plot(reward_list)\nplt.xticks(range(0,len(reward_list),2),range(1,len(reward_list)+1,2))\nplt.axhline(y = -200, color = 'k', linestyle = '--')\nplt.axhline(y = np.average(reward_list), color = 'blue', linestyle = '--', label = 'povpre\u010dna nagrada')\nplt.annotate(str(np.average(reward_list)), xy= (-2,np.average(reward_list)+0.7), color = 'blue',fontsize = 13, weight = 'bold')\nax=plt.gca()\nax.tick_params(axis=\"both\", labelsize=12)\nf.legend(loc = 'right', fontsize = 13)\nplt.xlabel('Epizoda', fontsize=14)\nplt.ylabel('Nagrada', fontsize=14)\nplt.title('Vrednost nagrade v posamezni epizodi', fontsize=16)\n#f.savefig('reward3.jpg') ###\nplt.show()\n</code></pre>"},{"location":"save_models/#logiranje-in-sprotno-shranjevanje-agentov","title":"Logiranje in sprotno shranjevanje agentov","text":"<ul> <li>Paket Tensorboard</li> </ul>"},{"location":"save_models/#prilagojena-python-skripta","title":"Prilagojena Python skripta","text":"<ol> <li> <p>Pripravimo mape za shranjevanje modelov</p> <pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport os\nmodels_dir = \"models/DQN\"\nif not os.path.exists(models_dir):\nos.makedirs(models_dir)\nlogdir = \"logs\"\nif not os.path.exists(logdir):\nos.makedirs(logdir) \nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Inicializiramo u\u010denje agenta</p> </li> <li> <p>Podatki za inicializacijo so na spletni strani</p> <pre><code>policy_kwargs = dict(net_arch=[256, 256])\nmodel = DQN('MlpPolicy', \nenv=env,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.99,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs,\nseed=2,\ntensorboard_log=logdir,\nverbose=1\n)\n</code></pre> </li> <li> <p>U\u010denje in shranjevanje</p> <pre><code>TIMESTEPS = 2000\niters = 0\nwhile True:\niters += 1\nmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"DQN\")\nmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n</code></pre> </li> </ol>"},{"location":"save_models/#uporaba-logiranja-za-spremljanje-ucenja","title":"Uporaba logiranja za spremljanje u\u010denja","text":"<p>tensorboard --logdir=logs</p> <p>python3 -m tensorboard.main --logdir=logs</p> <p>python3 -m tensorboard.main --logdir=logs</p> <p></p> <p></p> <p></p>"}]}