{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spodbujevalno u\u010denje","text":""},{"location":"#strojno-ucenje","title":"Strojno u\u010denje","text":""},{"location":"#spodbujevalno-ucenje_1","title":"Spodbujevalno u\u010denje","text":"<ul> <li>U\u010denje robotskih strategij v simuliranem okolju<ul> <li>OpenAI Gym + Stable Baselines3 </li> <li>Preprosta okolja</li> <li>AirHockey</li> </ul> </li> <li>Prenos na dejanskega robota</li> <li>Vodene + samostojne vaje</li> </ul> <ul> <li>Okolje<ul> <li>Stanje okolja</li> <li>Nagrada ta agenta, za dolo\u010deno stanje</li> <li>Seznam mo\u017enih akcij</li> </ul> </li> <li>Agent<ul> <li>Strategija za izbiro najbolj\u0161ih akciji</li> <li>Izvaja in se odlo\u010da o najbolj\u0161i akciji</li> </ul> </li> </ul>"},{"location":"#okolje","title":"Okolje","text":"<ul> <li>Spodbujevalno u\u010denje zahteva veliko ponovitev<ul> <li>Zelo neprakti\u010dno za razvoj na robotu</li> <li>U\u010denje v simulaciji okolja -&gt; Env</li> <li>Zbirka okolij -&gt; Gym</li> </ul> </li> </ul> <p>OpenAI Gym okolja za izvajanje spodbujevalnega u\u010denja (https://gym.openai.com/)</p>"},{"location":"#programsko-okolje","title":"Programsko okolje","text":""},{"location":"#openai-gym","title":"OpenAI Gym","text":"<ul> <li>OpenAI Gym<ul> <li>Simulacij razli\u010dnih okolij</li> <li>Pred pripravljena okolja</li> <li>Struktura za razvoj lastnih okolij</li> </ul> </li> </ul> <p>(https://gymnasium.farama.org/environments/toy_text/cliff_walking/)</p> <p>https://gymnasium.farama.org/environments/mujoco/</p>"},{"location":"#stable-baselines3","title":"Stable Baselines3","text":"<ul> <li>Knji\u017enica za algoritme za spodbujevalno u\u010denje<ul> <li>Globoko spodbujevalno u\u010denje</li> </ul> </li> <li>Stable Baselines3 (SB3) algoritmi za spodbujevalno u\u010denje<ul> <li>https://stable-baselines3.readthedocs.io/en/master/#</li> <li>vsebuje \u0161e druga okolja v skladu s strukturo OpenAI Gym</li> </ul> </li> </ul>"},{"location":"#priprava-python-okolja","title":"Priprava python okolja","text":"<ul> <li>https://www.anaconda.com/</li> <li>conda<ul> <li>Ustvarjanje razli\u010dnih okolij v katerih imate lahko in\u0161talirane programske pakete za rezli\u010dne pakete -In\u0161talacije so lo\u010dene med okolji in ne vplivajo drugo na drugo</li> <li>Windowsi, Linux, ...</li> </ul> </li> <li>In\u0161talacija po navodilih<ul> <li>conda config --set auto_activate_base false</li> </ul> </li> </ul>"},{"location":"#ustvarjanje-okolja","title":"Ustvarjanje okolja","text":"<p><code>conda create --name rl_smpl_2 python=3.9</code></p>"},{"location":"#aktivacija-okolja","title":"Aktivacija okolja","text":"<p><code>conda activate rl_smpl_2</code></p>"},{"location":"#deaktivacija-okolja","title":"Deaktivacija okolja","text":"<p><code>conda deactivate</code></p> <p>\u010cetrtkova skupina - conda okolje rl_cet</p> <p>Petkova skupina - conda okolje rl_pet</p>"},{"location":"#q-ucenje","title":"Q u\u010denje","text":""},{"location":"#primer-okolja-cliff-walking","title":"Primer okolja Cliff Walking","text":"<p>https://gymnasium.farama.org/environments/toy_text/cliff_walking/</p> <ol> <li> <p>Za\u010dnemo z novo python skripto</p> </li> <li> <p>Uvoz potrebnih paketov</p> </li> </ol> <pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\n</code></pre> <ol> <li> <p>Deklaracija okolja</p> <ul> <li>your_env = gym.make(\"YourEnv \", some_kwarg=your_vars)</li> <li> <p>your_env = gym.make(\"YourEnv\")</p> </li> <li> <p>Seznam okolij     https://gymnasium.farama.org/environments/toy_text/</p> </li> </ul> </li> </ol> <pre><code>env = gym.make('CliffWalking-v0')\n</code></pre> <ol> <li>Resetiramo in prika\u017eemo okolje</li> </ol> <pre><code>env.reset()\nenv.render()\n</code></pre> <ol> <li>Prika\u017eemo nekaj parametrov</li> </ol> <pre><code>print( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\n</code></pre> <ol> <li>Deklariramo Q tabelo</li> </ol> <pre><code>q_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\n</code></pre> <ol> <li>Deklariramo parametre</li> </ol> <pre><code>learning_rate = 0.1\ndiscount_factor = 0.95\nepochs = 60000\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\n</code></pre> <ol> <li>Za\u010dnemo z u\u010denjem z izbranim \u0161tevilom epoh</li> </ol> <pre><code>for episode in range(epochs):\n    state = env.reset()\n    done = False\n\n    trial_length = 0\n</code></pre> <ol> <li>Izvedemo posamezni \u201esprehod \u010dez okolje\u201c z izbiro akcij<ul> <li>raziskovanje: naklju\u010dna akcija</li> <li>uporabo zbranega znanja: akcija z maximalno q vrednostjo</li> </ul> </li> </ol> <pre><code>\u00a0 \u00a0 while not done:\n\n        if (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\n            action = env.action_space.sample()\n        else: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Use the action with the highest q-value\n            action = np.argmax(q_table[state]) \n</code></pre> <ol> <li>Izvedemo akcijo v okolju in preberemo vrednosti novega stanja in nagrade</li> </ol> <pre><code>        next_state, reward, done, info = env.step(action)\n</code></pre> <ol> <li>Posodobimo stanje Q tabele</li> </ol> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 curr_q = q_table[state, action]\n\u00a0 \u00a0 \u00a0 \u00a0 next_max_q = np.max(q_table[next_state])\n\u00a0 \u00a0 \u00a0 \u00a0 new_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\n\u00a0 \u00a0 \u00a0 \u00a0 q_table[state, action] = new_q\n</code></pre> <ol> <li>Posodobimo stanje</li> </ol> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 state = next_state\n</code></pre> <ol> <li>Shranimo dol\u017eino trenutnega \u201esprehoda\u201c</li> </ol> <pre><code>        if episode % SHOW_EVERY == 0:\n            trial_length += 1\n</code></pre> <ol> <li>Celotna koda u\u010denja do sedaj</li> </ol> <pre><code>for episode in range(epochs):\n    state = env.reset()\n    done = False\n\n    trial_length = 0\n\n\n    while not done:\n        if (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\n            action = env.action_space.sample()\n        else: # Use the action with the highest q-value\n            action = np.argmax(q_table[state]) \n\n        next_state, reward, done, info = env.step(action)\n\n        curr_q = q_table[state, action]\n        next_max_q = np.max(q_table[next_state])\n        new_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\n        q_table[state, action] = new_q\n\n        state = next_state\n\n        if episode % SHOW_EVERY == 0:\n            trial_length += 1\n\n    if episode % SHOW_EVERY == 0:\n        print(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\n\n    if END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\n        epsilon -= epsilon_decay_value\n</code></pre> <ol> <li>Testiramo nau\u010deno Q tabelo oziroma agenta</li> <li>Resetiramo okolje in ga izri\u0161emo</li> </ol> <pre><code>print(q_table)\n\nstate = env.reset()\nenv.render()\n\u00a0 \u00a0\ndone = False\ntrial_length = 0\n</code></pre> <ul> <li>Izvedemo sprehod</li> </ul> <pre><code>while not done:\n\u00a0 \u00a0 action = np.argmax(q_table[state])\n\u00a0 \u00a0 state, reward, done, info = env.step(action)\n    trial_length += 1\n\u00a0   print(\" Step \" + str(trial_length))\n\u00a0   env.render()\n\u00a0 \u00a0 sleep(.2)\n</code></pre> <ol> <li>Ve\u010d testov, da vidimo uspe\u0161nost</li> </ol> <pre><code>lengths=[]\nfor trialnum in range(1, 11):\n\u00a0 \u00a0 state = env.reset()\n\u00a0 \u00a0\n\u00a0 \u00a0 done = False\n\u00a0 \u00a0 trial_length = 0\n\u00a0 \u00a0 \n\u00a0 \u00a0 while not done and trial_length &lt; 25:\n\u00a0 \u00a0 \u00a0 \u00a0 action = np.argmax(q_table[state])\n\u00a0 \u00a0 \u00a0 \u00a0 state, reward, done, info = env.step(action)\n\u00a0 \u00a0 \u00a0 \u00a0 print(\"Trial number \" + str(trialnum) + \" Step \" + str(trial_length))\n\u00a0 \u00a0 \u00a0 \u00a0 env.render()\n\u00a0 \u00a0 \u00a0 \u00a0 sleep(.2)\n\u00a0 \u00a0 \u00a0 \u00a0 trial_length += 1\n\u00a0 \u00a0 lengths.append(trial_length)\n\u00a0 \u00a0 \n\u00a0 \u00a0 sleep(.2)\navg_len=sum(lengths)/10\nprint(avg_len)\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"}]}