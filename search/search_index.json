{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spodbujevalno u\u010denje","text":""},{"location":"#strojno-ucenje","title":"Strojno u\u010denje","text":""},{"location":"#spodbujevalno-ucenje_1","title":"Spodbujevalno u\u010denje","text":"<ul> <li>U\u010denje robotskih strategij v simuliranem okolju<ul> <li>OpenAI Gym + Stable Baselines3 </li> <li>Preprosta okolja</li> <li>AirHockey</li> </ul> </li> <li>Prenos na dejanskega robota</li> <li>Vodene + samostojne vaje</li> </ul> <ul> <li>Okolje<ul> <li>Stanje okolja</li> <li>Nagrada ta agenta, za dolo\u010deno stanje</li> <li>Seznam mo\u017enih akcij</li> </ul> </li> <li>Agent<ul> <li>Strategija za izbiro najbolj\u0161ih akciji</li> <li>Izvaja in se odlo\u010da o najbolj\u0161i akciji</li> </ul> </li> </ul>"},{"location":"#okolje","title":"Okolje","text":"<ul> <li>Spodbujevalno u\u010denje zahteva veliko ponovitev<ul> <li>Zelo neprakti\u010dno za razvoj na robotu</li> <li>U\u010denje v simulaciji okolja -&gt; Env</li> <li>Zbirka okolij -&gt; Gym</li> </ul> </li> </ul> <p>OpenAI Gym okolja za izvajanje spodbujevalnega u\u010denja (https://gym.openai.com/)</p>"},{"location":"#programsko-okolje","title":"Programsko okolje","text":""},{"location":"#openai-gym","title":"OpenAI Gym","text":"<ul> <li>OpenAI Gym<ul> <li>Simulacij razli\u010dnih okolij</li> <li>Pred pripravljena okolja</li> <li>Struktura za razvoj lastnih okolij</li> </ul> </li> </ul> <p>(https://gymnasium.farama.org/environments/toy_text/cliff_walking/)</p> <p>https://gymnasium.farama.org/environments/mujoco/</p>"},{"location":"#stable-baselines3","title":"Stable Baselines3","text":"<ul> <li>Knji\u017enica za algoritme za spodbujevalno u\u010denje<ul> <li>Globoko spodbujevalno u\u010denje</li> </ul> </li> <li>Stable Baselines3 (SB3) algoritmi za spodbujevalno u\u010denje<ul> <li>https://stable-baselines3.readthedocs.io/en/master/#</li> <li>vsebuje \u0161e druga okolja v skladu s strukturo OpenAI Gym</li> </ul> </li> </ul>"},{"location":"#priprava-python-okolja","title":"Priprava python okolja","text":"<ul> <li>https://www.anaconda.com/</li> <li>conda<ul> <li>Ustvarjanje razli\u010dnih okolij v katerih imate lahko in\u0161talirane programske pakete za rezli\u010dne pakete -In\u0161talacije so lo\u010dene med okolji in ne vplivajo drugo na drugo</li> <li>Windowsi, Linux, ...</li> </ul> </li> <li>In\u0161talacija po navodilih<ul> <li>conda config --set auto_activate_base false</li> </ul> </li> </ul>"},{"location":"#ustvarjanje-okolja","title":"Ustvarjanje okolja","text":"<p><code>conda create --name rl_smpl_2 python=3.9</code></p>"},{"location":"#aktivacija-okolja","title":"Aktivacija okolja","text":"<p><code>conda activate rl_smpl_2</code></p>"},{"location":"#deaktivacija-okolja","title":"Deaktivacija okolja","text":"<p><code>conda deactivate</code></p> <p>\u010cetrtkova skupina - conda okolje rl_cet</p> <p>Petkova skupina - conda okolje rl_pet</p>"},{"location":"#q-ucenje","title":"Q u\u010denje","text":""},{"location":"car/","title":"MountainCar primer","text":"<ul> <li>Cilj je priti iz doline do zastavice s prenihavanjem na klan\u010dinah,</li> <li>github povezava na py skripto za okolje</li> <li>Mountain Car</li> <li>Diskretne akcije<ul> <li>Potisk levo</li> <li>Potisk desno</li> <li>Brez potiska</li> </ul> </li> </ul> <ul> <li>Zvezna opazovanja<ul> <li>pozicija</li> <li>hitrost</li> <li>Za Q tabelo potrebujemo diskretna opazovanja!</li> </ul> </li> <li>Diskretizacija opazovanj na obmo\u010dja</li> <li>Nagrada: -1 za vsak korak</li> <li>Optimiziramo, da dose\u017ee gol v najkraj\u0161em \u010dasu</li> </ul>"},{"location":"car/#python-skripta","title":"Python skripta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 15000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n</code></pre> </li> <li> <p>Preverimo prostor akcij in opazovanja</p> <pre><code>print( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Diskretizacija opazovanj</p> <pre><code>#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n</code></pre> </li> <li> <p>Inicializacija Q tabele</p> <pre><code>#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\n</code></pre> </li> <li> <p>Funkcija, ki vrne indeks diskretnega stanja glede na vrednost opazovanj</p> <pre><code>def get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\n</code></pre> </li> <li> <p>Za\u010dnemo izvajati epizode u\u010denja</p> <pre><code>for episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\n</code></pre> </li> <li> <p>Akcija, korak, stanje, Q tabela</p> <ul> <li>Izberemo akcijo</li> <li>Izvedemo korak</li> <li>Dolo\u010dimo novo diskretno stanje</li> </ul> <pre><code>\u00a0 \u00a0 while not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\n</code></pre> <ul> <li>Posodobimo Q tabelo</li> <li>Posodobimo stanje</li> </ul> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 if not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state\n</code></pre> <ul> <li>Posodobimo epsilon</li> </ul> <pre><code>\u00a0 \u00a0 if END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> <ul> <li>Shranimo trenutno vrednost Q tabele</li> </ul> <pre><code>\u00a0 \u00a0 if episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\n</code></pre> <ul> <li>Po kon\u010danih epizodah shranimo kon\u010dno Q tabelo </li> </ul> <pre><code>np.save(f\"cart_e{episode}-qtable.npy\", q_table) \n</code></pre> </li> <li> <p>Celotna koda</p> </li> </ol> car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 15000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nfor episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\nwhile not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\nif not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state     \nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nif episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table) \n</code></pre>"},{"location":"carSB3Contrib/","title":"Uporaba drugih algoritmov za u\u010denje","text":"<ul> <li>Stable-Baselines3 vsebuje veliko implementacij algoritmov<ul> <li>RL Algorithms</li> </ul> </li> <li>Dodatni algoritmi so v SB3 Contrib<ul> <li>SB3 Contrib</li> </ul> </li> <li>Hiperparametri za posamezna Gym okolja in RL algoritme so v RL Baselines3 Zoo<ul> <li>RL Baselines3 Zoo hiperparametri</li> </ul> </li> </ul>"},{"location":"carSB3Contrib/#python-skripta-za-ucenje-agenta-z-qrdqn-algoritmom","title":"Python skripta za u\u010denje agenta z QRDQN algoritmom","text":"carQRDQN.py<pre><code>import gym\nfrom sb3_contrib import QRDQN\nimport os\nmodels_dir = \"models/QRDQN\"\nif not os.path.exists(models_dir):\nos.makedirs(models_dir)\nlogdir = \"logs\"\nif not os.path.exists(logdir):\nos.makedirs(logdir) \nenv = gym.make(\"MountainCar-v0\")\npolicy_kwargs = dict(net_arch=[256, 256], n_quantiles=25)\nmodel = QRDQN('MlpPolicy', \nenv=env, \ntensorboard_log=logdir,\nverbose=1,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.98,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs)\nTIMESTEPS = 2000\niters = 0\nwhile True:\niters += 1\nmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"QRDQN\")\nmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n</code></pre>"},{"location":"carSB3Contrib/#ppo-algoritem","title":"PPO algoritem","text":"<p>Primer 1 Primer 2</p>"},{"location":"car_DQN/","title":"U\u010denje z nevronskimi mre\u017eami z metodo DQN","text":"<ul> <li>U\u010denje z nevronskimi mre\u017eami z metodo DQN</li> <li>Uporabimo python paket Stable Baselines3 (SB3)</li> <li>Stable-Baselines3 Docs</li> <li>Stable Baselines3 omogo\u010da celo vrsto drugih algoritmov<ul> <li>A2C</li> <li>PPO</li> </ul> </li> <li>Raz\u0161iritev SB3 Contrib<ul> <li>dodatni sodobnej\u0161i algoritmi</li> <li>SB3 Contrib dokumentacije</li> <li>Github repozitorij</li> </ul> </li> </ul>"},{"location":"car_DQN/#python-skripta-za-ucenje-agenta","title":"Python skripta za u\u010denje agenta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <p><pre><code>import gym\nfrom stable_baselines3 import DQN \nenv = gym.make(\"MountainCar-v0\")\n</code></pre> 3. Preverimo prostor akcij in opazovanja</p> <pre><code>print( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Inicializiramo u\u010denje agenta</p> </li> <li> <p>Podatki za inicializacijo so na spletni strani ter na rl-baselines3-zoo</p> <pre><code>policy_kwargs = dict(net_arch=[256, 256])\nmodel = DQN('MlpPolicy', \nenv=env,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.99,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs,\nseed=2,\nverbose=1\n)\n</code></pre> </li> <li> <p>U\u010denje agenta</p> <pre><code>model.learn(total_timesteps=1.2e5)\n</code></pre> </li> <li> <p>Shranimo model</p> <pre><code>model.save(\"dqn_car\")\n</code></pre> </li> </ol>"},{"location":"car_DQN/#python-skripta-za-testiranje-agenta","title":"Python skripta za testiranje agenta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Nalo\u017eimo in incializiramo agenta</p> <pre><code>model = DQN.load(\"dqn_car\", env=env)\n</code></pre> </li> <li> <p>Za\u017eenemo in testiramo agenta</p> <pre><code>mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\nprint(f'Mean reward: {mean_reward}, Std reward: {std_reward}')\nobs = env.reset()\nwhile True:\naction, _state = model.predict(obs, deterministic=True)\nobs, reward, done, info = env.step(action)\nenv.render()\nif done:\nobs = env.reset()\n</code></pre> </li> </ol>"},{"location":"car_action/","title":"Prikaz akcij glede na Q tabelo","text":"load_car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nimport sys ###\nimport matplotlib.pyplot as plt ###\nimport matplotlib.patches as mpatches ###\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\ndef get_actions(dataset):\nstolpec = 0\n#print(type(data))\nactions = np.ndarray([GRID_SIZE, GRID_SIZE])\nfor stolpec in range(GRID_SIZE):\nvrstica = 0\nfor vrstica in range(GRID_SIZE):\nif dataset[stolpec, vrstica, 0] == dataset[stolpec, vrstica, 1] == dataset[stolpec, vrstica, 2] == 0:\nactions[stolpec, vrstica] = -1\nelse:\nactions[stolpec, vrstica] = np.argmax(dataset[stolpec, vrstica])\nreturn actions\ndef plot_graphs(ep_list):\nep = 0\nfig,axs = plt.subplots(2,2,figsize = (15,15))\nfig.suptitle(\"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\", fontsize = 16)\nprint(enumerate(axs.flat))\nfor i, ax in enumerate(axs.flat):\ndata = np.load('cart_e'+str(ep_list[ep])+'-qtable.npy')\nnp.set_printoptions(threshold=sys.maxsize)\npositions = np.arange(-1.2,0.6+discrete_os_win_size[0],discrete_os_win_size[0])\nvelocities = np.arange(-0.07,0.07+discrete_os_win_size[1],discrete_os_win_size[1])\n#ax = fig.add_subplot(2,2)\nlabels = [\"Neobiskana stanja\",\"Premik levo\", \"Ne naredimo ni\u010desar\", \"Premik desno\"]\ncmap = plt.colormaps.get_cmap('Blues') #matplotlib.colormaps\nax = plt.subplot(2,2,i+1)\nactions = get_actions(data)\nax.pcolor(velocities,positions, actions, cmap = cmap)\nax.set_ylabel(\"Pozicija\", fontsize = 14)\nax.set_xlabel(\"Hitrost\", fontsize = 14)        \n#ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r')\n#ax.plot(velocities_0[0],positions_0[0],'ro')\nax.set_title('Epizoda '+str(ep_list[ep]+1), fontsize = 13)\nep += 1\nbound = np.linspace(0, 1, 5)\nprint(bound)\nfig.legend([mpatches.Patch(color=cmap(b)) for b in bound[:-1]],\n[labels[i] for i in range(4)], loc = 'upper right')\nplt.subplots_adjust(wspace=0.4,hspace=0.4)\nfig.savefig('Qtable.jpg') ###\n#plt.show()\nplot_graphs([0,5000,10000,14999])\n</code></pre>"},{"location":"car_reward/","title":"Izpis nagrade agenta med u\u010denjem","text":"car.py<pre><code>import gym\nimport numpy as np\nimport matplotlib.pyplot as plt ###\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 25000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nreward_list = [] ###\nave_reward_list = [] ###\nfor episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\ntot_reward, reward = 0, 0 ###\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\nwhile not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\nif not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state\ntot_reward += reward ###\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nif episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\nreward_list.append(tot_reward) ###\nif episode % SHOW_EVERY == 0: ###\nave_reward = np.mean(reward_list) ###\nave_reward_list.append(ave_reward) ###\nreward_list = [] ###\nprint('Episode {} Average Reward: {}'.format(episode, ave_reward)) ###\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table) \n# Plot Rewards\nfig, ax = plt.subplots()\nax.plot(100 * (np.arange(len(ave_reward_list)) + 1), ave_reward_list) ###\nax.set_xlabel('Episodes') ###\nax.set_ylabel('Average Reward') ###\nax.set_title('Average Reward vs Episodes') ###\nfig.savefig('rewards.jpg') ###\nplt.show() ###\n</code></pre>"},{"location":"car_reward/#izpis-nagrade-za-vec-ponovitev","title":"Izpis nagrade za ve\u010d ponovitev","text":"load_car.py<pre><code>import gym\nimport numpy as np\nimport matplotlib.pyplot as plt ###\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e24999-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nreward_list = [] ###\nfor episode in range(50): ###\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\ndone = False\nprint(\"EPISODE \", episode)\ntot_reward, reward = 0, 0 ###\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\ntot_reward += reward ###\nreward_list.append(tot_reward) ###\nenv.close()\nf = plt.figure()\nplt.plot(reward_list)\nplt.xticks(range(0,len(reward_list),2),range(1,len(reward_list)+1,2))\nplt.axhline(y = -200, color = 'k', linestyle = '--')\nplt.axhline(y = np.average(reward_list), color = 'blue', linestyle = '--', label = 'povpre\u010dna nagrada')\nplt.annotate(str(np.average(reward_list)), xy= (-2,np.average(reward_list)+0.7), color = 'blue',fontsize = 13, weight = 'bold')\nax=plt.gca()\nax.tick_params(axis=\"both\", labelsize=12)\nf.legend(loc = 'right', fontsize = 13)\nplt.xlabel('Epizoda', fontsize=14)\nplt.ylabel('Nagrada', fontsize=14)\nplt.title('Vrednost nagrade v posamezni epizodi', fontsize=16)\nplt.show()\n</code></pre>"},{"location":"cart/","title":"Cartpole primr","text":"<ul> <li>github povezava na py skripto za okolje</li> <li> <p>Cart Pole</p> </li> <li> <p>Diskretne akcije (2 akciji)</p> </li> <li> <p>Zvezna opazovanja (4 spremenljivke)</p> <ul> <li>Nujna diskretizacija za uporabo Q u\u010denja</li> <li>Nekatera opazovanje imajo meje od -\u221e do +\u221e -&gt; dolo\u010ditev mej na roke</li> </ul> </li> <li> <p>Nagrada: +1 za vsak korak     -Optimizacija, da vodenje \u010dim dlje obdr\u017ei nihalo in vozi\u010dek pokonci in znotraj okolja</p> </li> </ul>"},{"location":"cliff/","title":"Primer okolja Cliff Walking","text":"<p>Open AI Cliff Walking</p> <ol> <li> <p>Za\u010dnemo z novo python skripto</p> </li> <li> <p>Uvoz potrebnih paketov</p> <pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\n</code></pre> </li> <li> <p>Deklaracija okolja</p> <ul> <li>your_env = gym.make(\"YourEnv \", some_kwarg=your_vars)</li> <li> <p>your_env = gym.make(\"YourEnv\")</p> </li> <li> <p>Seznam okolij     https://gymnasium.farama.org/environments/toy_text/</p> </li> </ul> <pre><code>env = gym.make('CliffWalking-v0')\n</code></pre> </li> <li> <p>Resetiramo in prika\u017eemo okolje</p> <pre><code>env.reset()\nenv.render()\n</code></pre> </li> <li> <p>Prika\u017eemo nekaj parametrov</p> <pre><code>print( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\n</code></pre> </li> <li> <p>Deklariramo Q tabelo</p> <pre><code>q_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\n</code></pre> </li> <li> <p>Deklariramo parametre</p> <pre><code>learning_rate = 0.1\ndiscount_factor = 0.95\nepochs = 60000\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\n</code></pre> </li> <li> <p>Za\u010dnemo z u\u010denjem z izbranim \u0161tevilom epoh</p> <pre><code>for episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\n</code></pre> </li> <li> <p>Izvedemo posamezni \u201esprehod \u010dez okolje\u201c z izbiro akcij</p> <ul> <li>raziskovanje: naklju\u010dna akcija</li> <li>uporabo zbranega znanja: akcija z maximalno q vrednostjo</li> </ul> <pre><code>\u00a0 \u00a0 while not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Use the action with the highest q-value\naction = np.argmax(q_table[state]) \n</code></pre> </li> <li> <p>Izvedemo akcijo v okolju in preberemo vrednosti novega stanja in nagrade</p> <pre><code>        next_state, reward, done, info = env.step(action)\n</code></pre> </li> <li> <p>Posodobimo stanje Q tabele</p> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 curr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\n</code></pre> </li> <li> <p>Posodobimo stanje</p> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 state = next_state\n</code></pre> </li> <li> <p>Shranimo dol\u017eino trenutnega \u201esprehoda\u201c</p> <pre><code>        if episode % SHOW_EVERY == 0:\ntrial_length += 1\n</code></pre> </li> <li> <p>Celotna koda u\u010denja do sedaj</p> <pre><code>for episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> </li> <li> <p>Testiramo nau\u010deno Q tabelo oziroma agenta</p> <ul> <li>Resetiramo okolje in ga izri\u0161emo</li> </ul> <pre><code>print(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\n</code></pre> <ul> <li>Izvedemo sprehod</li> </ul> <pre><code>while not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre> </li> <li> <p>Ve\u010d testov, da vidimo uspe\u0161nost</p> <pre><code>lengths=[]\nfor trialnum in range(1, 11):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done and trial_length &lt; 25:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\nprint(\"Trial number \" + str(trialnum) + \" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\ntrial_length += 1\nlengths.append(trial_length)\nsleep(.2)\navg_len=sum(lengths)/10\nprint(avg_len)\n</code></pre> </li> <li> <p>Celotna koda</p> cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\nfor episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nprint(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\nwhile not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre> </li> <li> <p>Celotna koda verzija 2</p> </li> </ol> <p>V tej verziji kode je dodana pred\u010dasna zaustevative u\u010denja za posamezno epoho, \u010de u\u010denje prese\u017ee 99 korakov u\u010denja v posamezni epohi.</p> cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nmax_steps = 99\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\nfor episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\n#while not done:\nfor step in range(max_steps):\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif done:\nbreak\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nprint(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\nwhile not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre>"},{"location":"cliff2/","title":"Prikaz Q tabele","text":""},{"location":"cliff2/#prikaz-okolja","title":"Prikaz okolja","text":""},{"location":"cliff2/#tekstovni-izpis-okolja","title":"Tekstovni izpis okolja","text":"<pre><code>env.render()\n</code></pre>"},{"location":"cliff2/#graficni-prikaz-okolja","title":"Grafi\u010dni prikaz okolja","text":"<p>Okolje bomo prikazali grafi\u010dno z matplotlib knji\u017enico.</p> <p><pre><code>import matplotlib.pyplot as plt\n</code></pre> Nato na konec datoteke dodamo kodo:</p> <pre><code>fig1, ax1 = plt.subplots()\nax1.axis('off')\nax1.axis('tight')\nokolje = [[\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"S\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"G\"]]\nokolje_colours = np.asarray(okolje,dtype='U25')\nokolje_rows = len(okolje[0][:])\nokolje_columns = len(okolje[:][0])\nokolje_colours[okolje_colours == \"x\"] = \"firebrick\"\nokolje_colours[okolje_colours == \"G\"] = \"gold\"\nokolje_colours[okolje_colours == \"S\"] = \"limegreen\"\nokolje_colours[okolje_colours == \"o\"] = \"cornflowerblue\"\nokolje = [[\"o,1\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,12\"], [\"o,13\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,24\"], [\"o,25\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,36\"], [\"S,37\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"G,48\"]]\ntable_okolje = ax1.table(cellText = okolje,cellColours=okolje_colours, loc = 'center', cellLoc = 'center', rowLoc = 'center',colLoc = 'center')\ntable_okolje.scale(1,1.5)\ntable_okolje.auto_set_font_size(True)\ntable_okolje.set_fontsize(10)\nplt.show()\n</code></pre>"},{"location":"cliff2/#tekstovni-izpis-q-tabele","title":"Tekstovni izpis Q tabele","text":"<pre><code>print(q_table)\n</code></pre>"},{"location":"cliff2/#graficni-izpis-q-tabele","title":"Grafi\u010dni izpis Q tabele","text":"<pre><code>fig2, ax2 = plt.subplots()\nfig2.patch.set_visible(False)\nax2.axis('off')\ncolumns = [\"GOR\",\"DESNO\",\"DOL\",\"LEVO\"]\nrows = [\"STANJE %d\" %(i+1) for i in range(env.observation_space.n)]\nqtable = np.around(qtable,3)\nqtable_s = qtable[:][0:22]\nrows_s = rows[0:22]\nnorm = plt.Normalize(qtable_s.min(), qtable_s.max()+0.1)\ncolours = plt.cm.YlGn(norm(qtable_s))\ntable = ax2.table(cellText=qtable_s, rowLabels=rows_s, colLabels=columns, loc = 'center', cellColours=colours,cellLoc ='center',rowLoc='center', colLoc ='center',colWidths=[0.1,0.1,0.1,0.1,0.1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(8)\nfig3, ax3 = plt.subplots()\nfig3.patch.set_visible(False)\nax3.axis('off')\nqtable_s = qtable[:][23:48]\nrows_s = rows[23:48]\ncolours = plt.cm.YlGn(norm(qtable_s))\ntable = ax3.table(cellText=qtable_s, rowLabels=rows_s, colLabels=columns, loc = 'center', cellColours=colours,cellLoc ='center',rowLoc='center', colLoc ='center',colWidths=[0.1,0.1,0.1,0.1,0.1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(8)\nplt.show()\n</code></pre>"},{"location":"cliff3/","title":"Eksploracija","text":""},{"location":"cliff3/#vizualizacija-funkcije-za-epsilon","title":"Vizualizacija funkcije za epsilon","text":"epsilon.py<pre><code>import matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider, Button\nimport numpy as np\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.01            # Minimum exploration probability\ndecay_rate = 0.001            # Exponential decay rate for exploration prob\ntotal_episodes = 10000\ndef plot_fcn(min_epsilon_fcn, max_epsilon_fcn, decay_rate_fcn, total_episodes_fcn):\nepsilon_values = [(min_epsilon_fcn + (max_epsilon_fcn - min_epsilon_fcn) * np.exp(-decay_rate_fcn * episode)) for episode in range(total_episodes_fcn)]\nep_num = range(total_episodes_fcn)\nreturn epsilon_values\n#define inital values for sliders\ninit_min_epsilon = min_epsilon\ninit_max_epsilon = max_epsilon\ninit_decay_rate = decay_rate\ninit_total_episodes = total_episodes\nfig1,ax1 = plt.subplots()\nline, = plt.plot(plot_fcn(init_min_epsilon,init_max_epsilon,init_decay_rate,init_total_episodes), lw=2)\nax1.set_xlabel(\"Epizoda\", fontsize = 15)\nax1.set_ylabel(\"Epsilon vrednost\",fontsize = 15)\nplt.subplots_adjust( bottom=0.3)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"Vrednost parametra epsilon v odvisnosti od \u0161tevila epizod (u\u010denje Q)\",fontsize = 18)\nplt.gcf().text(0.4,0.05,r'$\\epsilon = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min})*e^{(\\lambda*N)}$',fontsize=18)\n#slider for decay rate\nax_decay_rate = plt.axes([0.25, 0.1, 0.65, 0.03])\ndecay_rate_slider = Slider(\nax=ax_decay_rate,\nlabel='Lambda',\nvalmin=0.0001,\nvalmax=0.01,\nvalinit=init_decay_rate,\n)\n#slider for max epsilon value\nax_max_epsilon= plt.axes([0.25, 0.13, 0.65, 0.03])\nmax_epsilon_slider = Slider(\nax=ax_max_epsilon,\nlabel='Epsilon (max)',\nvalmin=0.1,\nvalmax=1,\nvalinit=init_max_epsilon,\n)\n#slider for min epsilon value\nax_min_epsilon= plt.axes([0.25, 0.16, 0.65, 0.03])\nmin_epsilon_slider = Slider(\nax=ax_min_epsilon,\nlabel='Epsilon (min)',\nvalmin=0,\nvalmax=0.5,\nvalinit=init_min_epsilon,\n)\n#slider for max episode value\nax_total_episodes= plt.axes([0.25, 0.19, 0.65, 0.03])\ntotal_episodes_slider = Slider(\nax=ax_total_episodes,\nlabel='Skupno \u0161tevilo epizod - N',\nvalmin=1,\nvalstep=1,\nvalmax=10000,\nvalinit=init_total_episodes,\n)\ndef update(val):\nline.set_data(range(total_episodes_slider.val),plot_fcn(min_epsilon_slider.val,max_epsilon_slider.val, decay_rate_slider.val,total_episodes_slider.val))\n#ax3.set_xlim(0,total_episodes_slider.val)\nax3.autoscale_view(True,True,True)\nax3.relim()\nfig.canvas.draw_idle()\ndecay_rate_slider.on_changed(update)\nmax_epsilon_slider.on_changed(update)\nmin_epsilon_slider.on_changed(update)\ntotal_episodes_slider.on_changed(update)\ndecay_rate_slider.label.set_size(16)\nmax_epsilon_slider.label.set_size(16)\nmin_epsilon_slider.label.set_size(16)\ntotal_episodes_slider.label.set_size(16)\nresetax = plt.axes([0.8, 0.025, 0.1, 0.04])\nbutton = Button(resetax, 'Reset', hovercolor='0.975')\nbutton.label.set_size(16)\ndef reset(event):\ndecay_rate_slider.reset()\nmax_epsilon_slider.reset()\nmin_epsilon_slider.reset()\ntotal_episodes_slider.reset()\nax3.autoscale_view(True, True, True)\nax3.relim()\nbutton.on_clicked(reset)\nplt.show()\n</code></pre>"},{"location":"cliff3/#sprememba-parametrov-za-izbolsanje-ucenja","title":"Sprememba parametrov za izbol\u0161anje u\u010denja","text":"cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nimport matplotlib.pyplot as plt\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nmax_steps = 99\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n</code></pre> <p>Zamenjamo z </p> <pre><code># Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.01            # Minimum exploration probability\ndecay_rate = 0.001            # Exponential decay rate for exploration prob\n</code></pre> <p>Zamenjamo krivuljo za <code>epsilon</code> spremenljivko </p> <pre><code>    if END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> <p>s kodo</p> <pre><code>    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n</code></pre>"},{"location":"load_car/","title":"Zagon agenta","text":"<ol> <li>Skripta za izvajanje agenta na podlagi shranjene Q tabele    Ustvarite novo skripto <code>load_car.py</code>.</li> <li> <p>Inicializacija okolja</p> <pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\n</code></pre> </li> <li> <p>Inicializacija agenta: nalo\u017eimo shranjeno Q tabelo</p> <p><pre><code>q_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\n</code></pre> 4. Za\u017eenemo okolje in agenta <pre><code>state = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\n</code></pre></p> </li> <li> <p>Celotna koda</p> </li> </ol> load_car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\n</code></pre>"},{"location":"save_models/","title":"Logiranje in spremljanje u\u010denja","text":""},{"location":"save_models/#python-skripta-za-testiranje-agenta","title":"Python skripta za testiranje agenta","text":"load_carDQN2.py<pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport matplotlib.pyplot as plt\nimport numpy as np\nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\nmodel = DQN.load(\"dqn_car\", env=env)\nmean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\nprint(f'Mean reward: {mean_reward}, Std reward: {std_reward}')\nreward_list = [] ###\nfor episode in range(50): ###\nstate = env.reset()\ndone = False\nprint(\"EPISODE \", episode)\ntot_reward, reward = 0, 0 ###\nwhile not done:\naction, _state = model.predict(state, deterministic=True)\nstate, reward, done, info = env.step(action)\ntot_reward += reward ###\nreward_list.append(tot_reward) ###\nenv.close()\nf = plt.figure()\nplt.plot(reward_list)\nplt.xticks(range(0,len(reward_list),2),range(1,len(reward_list)+1,2))\nplt.axhline(y = -200, color = 'k', linestyle = '--')\nplt.axhline(y = np.average(reward_list), color = 'blue', linestyle = '--', label = 'povpre\u010dna nagrada')\nplt.annotate(str(np.average(reward_list)), xy= (-2,np.average(reward_list)+0.7), color = 'blue',fontsize = 13, weight = 'bold')\nax=plt.gca()\nax.tick_params(axis=\"both\", labelsize=12)\nf.legend(loc = 'right', fontsize = 13)\nplt.xlabel('Epizoda', fontsize=14)\nplt.ylabel('Nagrada', fontsize=14)\nplt.title('Vrednost nagrade v posamezni epizodi', fontsize=16)\n#f.savefig('reward3.jpg') ###\nplt.show()\n</code></pre>"},{"location":"save_models/#logiranje-in-sprotno-shranjevanje-agentov","title":"Logiranje in sprotno shranjevanje agentov","text":"<ul> <li>Paket Tensorboard</li> </ul>"},{"location":"save_models/#prilagojena-python-skripta","title":"Prilagojena Python skripta","text":"<ol> <li> <p>Pripravimo mape za shranjevanje modelov</p> <pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport os\nmodels_dir = \"models/DQN\"\nif not os.path.exists(models_dir):\nos.makedirs(models_dir)\nlogdir = \"logs\"\nif not os.path.exists(logdir):\nos.makedirs(logdir) \nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Inicializiramo u\u010denje agenta</p> </li> <li> <p>Podatki za inicializacijo so na spletni strani</p> <pre><code>policy_kwargs = dict(net_arch=[256, 256])\nmodel = DQN('MlpPolicy', \nenv=env,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.99,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs,\nseed=2,\ntensorboard_log=logdir,\nverbose=1\n)\n</code></pre> </li> <li> <p>U\u010denje in shranjevanje</p> <pre><code>TIMESTEPS = 2000\niters = 0\nwhile True:\niters += 1\nmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"DQN\")\nmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n</code></pre> </li> </ol>"},{"location":"save_models/#uporaba-logiranja-za-spremljanje-ucenja","title":"Uporaba logiranja za spremljanje u\u010denja","text":""}]}