{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spodbujevalno u\u010denje Strojno u\u010denje Spodbujevalno u\u010denje U\u010denje robotskih strategij v simuliranem okolju OpenAI Gym + Stable Baselines3 Preprosta okolja AirHockey Prenos na dejanskega robota Vodene + samostojne vaje Okolje Stanje okolja Nagrada za agenta, za dolo\u010deno stanje Seznam mo\u017enih akcij Agent Strategija za izbiro najbolj\u0161ih akciji Izvaja in se odlo\u010da o najbolj\u0161i akciji Okolje Spodbujevalno u\u010denje zahteva veliko ponovitev Zelo neprakti\u010dno za razvoj na robotu U\u010denje v simulaciji okolja -> Env Zbirka okolij -> Gym OpenAI Gym okolja za izvajanje spodbujevalnega u\u010denja (https://gym.openai.com/) Programsko okolje OpenAI Gym OpenAI Gym Simulacij razli\u010dnih okolij Pred pripravljena okolja Struktura za razvoj lastnih okolij https://gymnasium.farama.org/environments/toy_text/cliff_walking/ https://gymnasium.farama.org/environments/mujoco/ Stable Baselines3 Knji\u017enica za algoritme za spodbujevalno u\u010denje Globoko spodbujevalno u\u010denje Stable Baselines3 (SB3) algoritmi za spodbujevalno u\u010denje https://stable-baselines3.readthedocs.io/en/master/ vsebuje \u0161e druga okolja v skladu s strukturo OpenAI Gym Q u\u010denje","title":"Uvod"},{"location":"#spodbujevalno-ucenje","text":"","title":"Spodbujevalno u\u010denje"},{"location":"#strojno-ucenje","text":"","title":"Strojno u\u010denje"},{"location":"#spodbujevalno-ucenje_1","text":"U\u010denje robotskih strategij v simuliranem okolju OpenAI Gym + Stable Baselines3 Preprosta okolja AirHockey Prenos na dejanskega robota Vodene + samostojne vaje Okolje Stanje okolja Nagrada za agenta, za dolo\u010deno stanje Seznam mo\u017enih akcij Agent Strategija za izbiro najbolj\u0161ih akciji Izvaja in se odlo\u010da o najbolj\u0161i akciji","title":"Spodbujevalno u\u010denje"},{"location":"#okolje","text":"Spodbujevalno u\u010denje zahteva veliko ponovitev Zelo neprakti\u010dno za razvoj na robotu U\u010denje v simulaciji okolja -> Env Zbirka okolij -> Gym OpenAI Gym okolja za izvajanje spodbujevalnega u\u010denja (https://gym.openai.com/)","title":"Okolje"},{"location":"#programsko-okolje","text":"","title":"Programsko okolje"},{"location":"#openai-gym","text":"OpenAI Gym Simulacij razli\u010dnih okolij Pred pripravljena okolja Struktura za razvoj lastnih okolij https://gymnasium.farama.org/environments/toy_text/cliff_walking/ https://gymnasium.farama.org/environments/mujoco/","title":"OpenAI Gym"},{"location":"#stable-baselines3","text":"Knji\u017enica za algoritme za spodbujevalno u\u010denje Globoko spodbujevalno u\u010denje Stable Baselines3 (SB3) algoritmi za spodbujevalno u\u010denje https://stable-baselines3.readthedocs.io/en/master/ vsebuje \u0161e druga okolja v skladu s strukturo OpenAI Gym","title":"Stable Baselines3"},{"location":"#q-ucenje","text":"","title":"Q u\u010denje"},{"location":"car/","text":"MountainCar primer Cilj je priti iz doline do zastavice s prenihavanjem na klan\u010dinah, github povezava na py skripto za okolje Mountain Car Diskretne akcije Potisk levo Potisk desno Brez potiska Zvezna opazovanja pozicija hitrost Za Q tabelo potrebujemo diskretna opazovanja! Diskretizacija opazovanj na obmo\u010dja Nagrada: -1 za vsak korak Optimiziramo, da dose\u017ee cilj v najkraj\u0161em \u010dasu Python skripta Nova python skripta Incializacija okolja in u\u010denja import gym import numpy as np from time import sleep env = gym . make ( \"MountainCar-v0\" ) #env = gym.make(\"Acrobot-v1\") #env = gym.make(\"CartPole-v1\") LEARNING_RATE = 0.1 DISCOUNT = 0.95 EPISODES = 15000 SHOW_EVERY = 100 epsilon = 1.0 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = EPISODES // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) Preverimo prostor akcij in opazovanja print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) Diskretizacija opazovanj #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) Inicializacija Q tabele #q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n])) q_table = np . zeros ( DISCRETE_OS_SIZE + [ env . action_space . n ]) print ( \"Q table size = \" , q_table . shape ) Funkcija, ki vrne indeks diskretnega stanja glede na vrednost opazovanj def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) Za\u010dnemo izvajati epizode u\u010denja for episode in range ( EPISODES ): discrete_state = get_discrete_state ( env . reset ()) done = False if episode % SHOW_EVERY == 0 : render = True print ( episode ) else : render = False Akcija, korak, stanje, Q tabela Izberemo akcijo Izvedemo korak Dolo\u010dimo novo diskretno stanje while not done : if np . random . random () > epsilon : action = np . argmax ( q_table [ discrete_state ]) else : action = np . random . randint ( 0 , env . action_space . n ) new_state , reward , done , _ = env . step ( action ) new_discrete_state = get_discrete_state ( new_state ) if episode % SHOW_EVERY == 0 : env . render () Posodobimo Q tabelo Posodobimo stanje if not done : max_future_q = np . max ( q_table [ new_discrete_state ]) current_q = q_table [ discrete_state + ( action ,)] new_q = ( 1 - LEARNING_RATE ) * current_q + LEARNING_RATE * ( reward + DISCOUNT * max_future_q ) q_table [ discrete_state + ( action ,)] = new_q discrete_state = new_discrete_state Posodobimo epsilon if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value Shranimo trenutno vrednost Q tabele if episode % SHOW_EVERY == 0 : np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) Po kon\u010danih epizodah shranimo kon\u010dno Q tabelo np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) Celotna koda car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 import gym import numpy as np from time import sleep env = gym . make ( \"MountainCar-v0\" ) #env = gym.make(\"Acrobot-v1\") #env = gym.make(\"CartPole-v1\") LEARNING_RATE = 0.1 DISCOUNT = 0.95 EPISODES = 15000 SHOW_EVERY = 100 epsilon = 1.0 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = EPISODES // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) #q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n])) q_table = np . zeros ( DISCRETE_OS_SIZE + [ env . action_space . n ]) print ( \"Q table size = \" , q_table . shape ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) for episode in range ( EPISODES ): discrete_state = get_discrete_state ( env . reset ()) done = False if episode % SHOW_EVERY == 0 : render = True print ( episode ) else : render = False while not done : if np . random . random () > epsilon : action = np . argmax ( q_table [ discrete_state ]) else : action = np . random . randint ( 0 , env . action_space . n ) new_state , reward , done , _ = env . step ( action ) new_discrete_state = get_discrete_state ( new_state ) if episode % SHOW_EVERY == 0 : env . render () if not done : max_future_q = np . max ( q_table [ new_discrete_state ]) current_q = q_table [ discrete_state + ( action ,)] new_q = ( 1 - LEARNING_RATE ) * current_q + LEARNING_RATE * ( reward + DISCOUNT * max_future_q ) q_table [ discrete_state + ( action ,)] = new_q discrete_state = new_discrete_state if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value if episode % SHOW_EVERY == 0 : np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) np . save ( f \"cart_e { episode } -qtable.npy\" , q_table )","title":"MountainCar primer"},{"location":"car/#mountaincar-primer","text":"Cilj je priti iz doline do zastavice s prenihavanjem na klan\u010dinah, github povezava na py skripto za okolje Mountain Car Diskretne akcije Potisk levo Potisk desno Brez potiska Zvezna opazovanja pozicija hitrost Za Q tabelo potrebujemo diskretna opazovanja! Diskretizacija opazovanj na obmo\u010dja Nagrada: -1 za vsak korak Optimiziramo, da dose\u017ee cilj v najkraj\u0161em \u010dasu","title":"MountainCar primer"},{"location":"car/#python-skripta","text":"Nova python skripta Incializacija okolja in u\u010denja import gym import numpy as np from time import sleep env = gym . make ( \"MountainCar-v0\" ) #env = gym.make(\"Acrobot-v1\") #env = gym.make(\"CartPole-v1\") LEARNING_RATE = 0.1 DISCOUNT = 0.95 EPISODES = 15000 SHOW_EVERY = 100 epsilon = 1.0 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = EPISODES // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) Preverimo prostor akcij in opazovanja print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) Diskretizacija opazovanj #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) Inicializacija Q tabele #q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n])) q_table = np . zeros ( DISCRETE_OS_SIZE + [ env . action_space . n ]) print ( \"Q table size = \" , q_table . shape ) Funkcija, ki vrne indeks diskretnega stanja glede na vrednost opazovanj def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) Za\u010dnemo izvajati epizode u\u010denja for episode in range ( EPISODES ): discrete_state = get_discrete_state ( env . reset ()) done = False if episode % SHOW_EVERY == 0 : render = True print ( episode ) else : render = False Akcija, korak, stanje, Q tabela Izberemo akcijo Izvedemo korak Dolo\u010dimo novo diskretno stanje while not done : if np . random . random () > epsilon : action = np . argmax ( q_table [ discrete_state ]) else : action = np . random . randint ( 0 , env . action_space . n ) new_state , reward , done , _ = env . step ( action ) new_discrete_state = get_discrete_state ( new_state ) if episode % SHOW_EVERY == 0 : env . render () Posodobimo Q tabelo Posodobimo stanje if not done : max_future_q = np . max ( q_table [ new_discrete_state ]) current_q = q_table [ discrete_state + ( action ,)] new_q = ( 1 - LEARNING_RATE ) * current_q + LEARNING_RATE * ( reward + DISCOUNT * max_future_q ) q_table [ discrete_state + ( action ,)] = new_q discrete_state = new_discrete_state Posodobimo epsilon if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value Shranimo trenutno vrednost Q tabele if episode % SHOW_EVERY == 0 : np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) Po kon\u010danih epizodah shranimo kon\u010dno Q tabelo np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) Celotna koda car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 import gym import numpy as np from time import sleep env = gym . make ( \"MountainCar-v0\" ) #env = gym.make(\"Acrobot-v1\") #env = gym.make(\"CartPole-v1\") LEARNING_RATE = 0.1 DISCOUNT = 0.95 EPISODES = 15000 SHOW_EVERY = 100 epsilon = 1.0 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = EPISODES // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) #q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n])) q_table = np . zeros ( DISCRETE_OS_SIZE + [ env . action_space . n ]) print ( \"Q table size = \" , q_table . shape ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) for episode in range ( EPISODES ): discrete_state = get_discrete_state ( env . reset ()) done = False if episode % SHOW_EVERY == 0 : render = True print ( episode ) else : render = False while not done : if np . random . random () > epsilon : action = np . argmax ( q_table [ discrete_state ]) else : action = np . random . randint ( 0 , env . action_space . n ) new_state , reward , done , _ = env . step ( action ) new_discrete_state = get_discrete_state ( new_state ) if episode % SHOW_EVERY == 0 : env . render () if not done : max_future_q = np . max ( q_table [ new_discrete_state ]) current_q = q_table [ discrete_state + ( action ,)] new_q = ( 1 - LEARNING_RATE ) * current_q + LEARNING_RATE * ( reward + DISCOUNT * max_future_q ) q_table [ discrete_state + ( action ,)] = new_q discrete_state = new_discrete_state if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value if episode % SHOW_EVERY == 0 : np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) np . save ( f \"cart_e { episode } -qtable.npy\" , q_table )","title":"Python skripta"},{"location":"carSB3Contrib/","text":"Uporaba drugih algoritmov za u\u010denje Stable-Baselines3 vsebuje veliko implementacij algoritmov RL Algorithms Dodatni algoritmi so v SB3 Contrib SB3 Contrib Hiperparametri za posamezna Gym okolja in RL algoritme so v RL Baselines3 Zoo RL Baselines3 Zoo hiperparametri Python skripta za u\u010denje agenta z QRDQN algoritmom carQRDQN.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import gym from sb3_contrib import QRDQN import os models_dir = \"models/QRDQN\" if not os . path . exists ( models_dir ): os . makedirs ( models_dir ) logdir = \"logs\" if not os . path . exists ( logdir ): os . makedirs ( logdir ) env = gym . make ( \"MountainCar-v0\" ) policy_kwargs = dict ( net_arch = [ 256 , 256 ], n_quantiles = 25 ) model = QRDQN ( 'MlpPolicy' , env = env , tensorboard_log = logdir , verbose = 1 , learning_rate = 4e-3 , batch_size = 128 , buffer_size = 10000 , learning_starts = 1000 , gamma = 0.98 , target_update_interval = 600 , train_freq = 16 , gradient_steps = 8 , exploration_fraction = 0.2 , exploration_final_eps = 0.07 , policy_kwargs = policy_kwargs ) TIMESTEPS = 2000 iters = 0 while True : iters += 1 model . learn ( total_timesteps = TIMESTEPS , reset_num_timesteps = False , tb_log_name = \"QRDQN\" ) model . save ( f \" { models_dir } / { TIMESTEPS * iters } \" ) PPO algoritem Primer 1 Primer 2","title":"Uporaba drugih algoritmov za u\u010denje"},{"location":"carSB3Contrib/#uporaba-drugih-algoritmov-za-ucenje","text":"Stable-Baselines3 vsebuje veliko implementacij algoritmov RL Algorithms Dodatni algoritmi so v SB3 Contrib SB3 Contrib Hiperparametri za posamezna Gym okolja in RL algoritme so v RL Baselines3 Zoo RL Baselines3 Zoo hiperparametri","title":"Uporaba drugih algoritmov za u\u010denje"},{"location":"carSB3Contrib/#python-skripta-za-ucenje-agenta-z-qrdqn-algoritmom","text":"carQRDQN.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import gym from sb3_contrib import QRDQN import os models_dir = \"models/QRDQN\" if not os . path . exists ( models_dir ): os . makedirs ( models_dir ) logdir = \"logs\" if not os . path . exists ( logdir ): os . makedirs ( logdir ) env = gym . make ( \"MountainCar-v0\" ) policy_kwargs = dict ( net_arch = [ 256 , 256 ], n_quantiles = 25 ) model = QRDQN ( 'MlpPolicy' , env = env , tensorboard_log = logdir , verbose = 1 , learning_rate = 4e-3 , batch_size = 128 , buffer_size = 10000 , learning_starts = 1000 , gamma = 0.98 , target_update_interval = 600 , train_freq = 16 , gradient_steps = 8 , exploration_fraction = 0.2 , exploration_final_eps = 0.07 , policy_kwargs = policy_kwargs ) TIMESTEPS = 2000 iters = 0 while True : iters += 1 model . learn ( total_timesteps = TIMESTEPS , reset_num_timesteps = False , tb_log_name = \"QRDQN\" ) model . save ( f \" { models_dir } / { TIMESTEPS * iters } \" )","title":"Python skripta za u\u010denje agenta z QRDQN algoritmom"},{"location":"carSB3Contrib/#ppo-algoritem","text":"Primer 1 Primer 2","title":"PPO algoritem"},{"location":"car_DQN/","text":"U\u010denje z nevronskimi mre\u017eami z metodo DQN Globoko spodbujevalno u\u010denje https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html DQN Q-u\u010denje je algoritma za spodbujevalno u\u010denje, ki uporablja tabelo za shranjevanje vrednosti Q za vsak par stanje-akcija. Ker pa prostor stanj postaja vse ve\u010dji in kompleksnej\u0161i, postane shranjevanje vseh vrednosti Q v tabeli neprakti\u010dno. Tu pride na vrsto DQN. DQN je vrsta algoritma za Q-u\u010denje, ki za izra\u010dun (pribli\u017eka) vrednosti Q uporablja nevronsko mre\u017eo. Nevronska mre\u017ea sprejme stanje kot vhod in kot izhod izra\u010duna vrednosti Q za vsako mo\u017eno akcijo. Mre\u017ea je nau\u010dena s kombinacijo ponovitve izku\u0161enj in \u00bbtarget\u00ab nevronske mre\u017ee za izbolj\u0161anje stabilnosti in u\u010dinkovitosti. Tehnika ponovitve izku\u0161enj se uporablja za izbolj\u0161anje u\u010dinkovitosti u\u010denja. Namesto da bi nevronsko mre\u017eo u\u010dili takoj na vsaki izku\u0161nji, se izku\u0161nje shranijo v spomin in se iz njega naklju\u010dno izbere serija izku\u0161enj za u\u010denje mre\u017ee. To ima ve\u010d prednosti. Prvi\u010d, pomaga prekiniti korelacijo med zaporednimi izku\u0161njami, kar lahko zmanj\u0161a verjetnost, da bo mre\u017ea obti\u010dalo v lokalnem optimumu. Drugi\u010d, nevronski mre\u017ei omogo\u010da, da se u\u010di iz \u0161ir\u0161ega razpona izku\u0161enj, kar lahko privede do bolj\u0161e posplo\u0161itve in bolj\u0161ega delovanja. \u00bbTarget\u00ab nevronska mre\u017ea je lo\u010dena kopija glavnega nevronske mre\u017ee, ki se med u\u010denjem uporablja za izra\u010dun ciljnih (\u00bbtarget\u00ab) vrednosti Q. V DQN uporabljamo \u00bbtarget\u00ab nevronsko mre\u017eo za re\u0161evanje problema, znanega kot \"problem premikajo\u010de se tar\u010de\". Ta te\u017eava nastane, ko uporabimo isto omre\u017eje za izra\u010dunavanje napovedi vrednosti Q in ciljnih vrednosti Q ter po vsaki napovedi posodobimo ute\u017ei nevronske mre\u017ee. To lahko povzro\u010di hitro in nestanovitno spreminjanje ciljnih vrednosti Q, kar lahko privede do nestabilnosti v procesu u\u010denja. Za re\u0161itev te te\u017eave uporabimo \u00bbtarget\u00ab nevronsko mre\u017eo, ki je kopija glavne nevronske mre\u017ee, vendar z zamrznjenimi parametri. \u00bbTarget\u00ab nevronska mre\u017ea se uporablja za izra\u010dun ciljnih vrednosti Q, ute\u017ei glavnega omre\u017eja pa se redno posodabljajo le z uporabo ciljnih vrednosti Q, ki jih izra\u010duna ciljno omre\u017eje. To stabilizira postopek u\u010denja in omre\u017eju omogo\u010da u\u010dinkovitej\u0161e u\u010denje. Najprej za\u010dnemo z vhodnim stanjem kor vhodom v nevronsko mre\u017eo. Mre\u017ea nato vrne vrednosti Q za vsako mo\u017eno akcijo. Podobno kot pri Q-u\u010denju ne izberemo akcije z najvi\u0161jo vrednostjo Q. Namesto tega uporabimo tehniko, imenovano \"epsilon-greedy exploration\", kjer z verjetnostjo epsilon izberemo naklju\u010dno akcijo in z verjetnostjo 1-epsilon izberemo akcijo z najvi\u0161jo vrednostjo Q. To algoritmu omogo\u010da, da raziskuje nove akcije in potencialno najde bolj\u0161e re\u0161itve. Ko izberemo akcijo, jo izvedemo in opazujemo dobljeno nagrado in novo stanje. Ta izku\u0161nja se shrani v spomin za ponovitev, ki ga bomo pozneje uporabili za u\u010denje mre\u017ee. Nato naklju\u010dno izberemo serijo izku\u0161enj iz spomina in jih uporabimo za u\u010denje mre\u017ee. Vendar mre\u017ee ne \u017eelimo vedno znova u\u010diti na istih izku\u0161njah, saj to lahko privede do pojava \u00bboverfitting\u00ab. Za re\u0161itev te te\u017eave uporabimo \u00bbtarget\u00ab nevronsko mre\u017eo, ki je kopija glavnega nevronske mre\u017ee, vendar z zamrznjenimi parametri. To \u00bbtarget\u00ab nevronsko mre\u017eo uporabimo za izra\u010dun kon\u010dnih vrednosti Q za vsako izku\u0161njo, nato pa posodobimo parametre glavne mre\u017ee, da \u010dim bolj zmanj\u0161amo razliko med napovedanimi vrednostmi Q in kon\u010dnimi vrednostmi Q. Ta postopek se ponavlja iterativno, pri \u010demer nevronska mre\u017ea nenehno izbolj\u0161uje svoje ocene vrednosti Q, ko se u\u010di iz izku\u0161enj. \u010ce povzamemo, je DQN vrsta algoritma za Q-u\u010denje, ki za oceno vrednosti Q uporablja nevronsko mre\u017eo. Zdru\u017euje ponovitev izku\u0161enj in \u00bbtarget\u00ab nevronsko mre\u017eo za izbolj\u0161anje stabilnosti in u\u010dinkovitosti. Z uporabo te metode lahko obravnavamo ve\u010dje in bolj zapletene prostore stanj ter na koncu dose\u017eemo bolj\u0161e rezultate kot z Q-metodo. PPO DQN (Deep Q-Network) in PPO (Proximal Policy Optimization) sta priljubljeni metodi za spodbujevalno u\u010denje, ki se uporabljata za u\u010denje agentov za izvajanje dolo\u010denih nalog. Vendar se razlikujeta glede na pristop in cilje. DQN je na Q vrednosti temelje\u010da metoda spodbujevalnega u\u010denja, ki u\u010di vrednosti Q (pri\u010dakovane prihodnje nagrade) za vsak par stanje-akcija. Cilj DQN je nau\u010diti se optimalno funkcijo Q, ki povezuje stanja z akcijami. To dose\u017eemo z u\u010denjem globoke nevronske mre\u017ee za oceno vrednosti Q z uporabo kombinacije ponovitve izku\u0161enj in \u00bbtarget\u00ab nevronske mre\u017ee. PPO pa je metoda spodbujevalnega u\u010denja na podlagi strategije, ki se neposredno u\u010di strategija za preslikavo iz stanj v akcije. Cilj PPO je maksimizirati pri\u010dakovano kumulativno nagrado agenta z iskanjem optimalne strategije. To se dose\u017ee z optimizacijo nadomestne objektivne funkcije, ki zagotavlja, da se nova strategija ne razlikuje preve\u010d od stare. \u010ceprav se lahko tako DQN kot PPO uporabljata za diskretne in zvezne prostore akcij, se razlikujeta po u\u010dinkovitosti vzor\u010denja, stabilnosti in enostavnosti izvajanja. DQN je na splo\u0161no manj vzor\u010dno u\u010dinkovit kot PPO, saj za u\u010denje natan\u010dne funkcije Q potrebuje veliko \u0161tevilo u\u010dnih vzorcev. Vendar je DQN znana po svoji stabilnosti in se uspe\u0161no uporablja v \u0161tevilnih aplikacijah, tudi v robotiki. Po drugi strani pa je PPO znana po svoji u\u010dinkovitosti vzor\u010denja in je dokazano dosegla vrhunske rezultate pri \u0161tevilnih nalogah zveznega vodenja. PPO je tudi razmeroma enostavna za izvajanje, saj ne potrebuje ponovitve izku\u0161enj ali lo\u010dene \u00bbtarget\u00ab nevronske mre\u017ee. \u010ceprav sta DQN in PPO zmogljivi metodi u\u010denja za spodbujevalno u\u010denje, se razlikujeta po pristopu in ciljih. DQN je metoda, ki temelji na vrednosti in se u\u010di optimalne funkcije Q, PPO pa je metoda, ki temelji na strategiji in se neposredno u\u010di optimalne strategije. A2C A2C pa je metoda spodbujevalnega u\u010denja, ki temelji na strategiji in se neposredno u\u010di strategije (preslikava med stanji in akcijami). Cilj A2C je maksimizirati pri\u010dakovano kumulativno nagrado agenta z iskanjem optimalne strategije, podobno kot PPO. To se dose\u017ee s hkratnim u\u010denjem dveh nevronskih mre\u017e: \u00bbactor\u00ab nevronska mre\u017ea, ki aproksimira strategijo, in \u00bbcritic\u00ab nevronske mre\u017ee, ki ocenjuje funkcijo vrednosti. Klju\u010dna razlika med DQN in A2C je, da je DQN metoda \u00bboff-policy\u201d, kar pomeni, da se u\u010di optimalne vrednosti Q z uporabo lo\u010dene strategije, medtem ko je A2C metoda \u00bbon-policy\u201d, kar pomeni, da se u\u010di strategije in funkcije vrednosti neposredno iz trenutne strategije. Druga razlika je, da se DQN lahko uporablja tako za diskretne kot za zvezne prostore akcij, medtem ko se A2C obi\u010dajno uporablja za diskretne prostore akcij. Poleg tega je A2C na splo\u0161no bolj vzor\u010dno u\u010dinkovita kot DQN, saj se lahko u\u010di iz delnih trajektorij, medtem ko DQN za posodobitev svojih vrednosti Q potrebuje celotne trajektorije. Trajektorija $\\tau$ je sekvenca parov $stanje_0$->$akcija_0$->$stanje_1$->$akcija_1$->... Klju\u010dna razlika med PPO in A2C je v tem, da PPO uporablja \u00bbclipped\u00ab objektivno funkcijo, da prepre\u010di prevelike spremembe strategije ob vsaki posodobitvi, medtem ko A2C uporablja \u00bbadvantage\u00ab funkcijo za oceno kakovosti vsake akcije glede na trenutno strategijo. Druga razlika je, da je PPO na splo\u0161no bolj vzor\u010dno u\u010dinkovit kot A2C, saj uporablja tehniko, imenovano vzor\u010denje po pomembnosti, za popravke posodobitev gradienta. Vzor\u010denje po pomembnosti omogo\u010da, da se PPO u\u010di iz stare strategije, kar lahko izbolj\u0161a u\u010dinkovitost vzor\u010denja. \u010ceprav sta tako PPO kot A2C u\u010dinkoviti metodi za spodbujevalno u\u010denje, se razlikujeta po pristopu in ciljih. PPO je metoda, ki temelji na strategiji in se u\u010di optimalne strategije z uporabo optimizacije in \u00bbclipped\u00ab objektivne funkcije, A2C pa je prav tako metoda, ki temelji na strategiji in se u\u010di optimalne strategije in funkcije vrednosti z uporabo \u00bbadvantage\u00ab funkcije prednosti. \u201cAdvantage\u201d funkcija je temeljni koncept spodbujevalnega u\u010denja, ki se uporablja za oceno kakovosti vsake akcije agenta glede na trenutno strategijo. Meri, koliko bolj\u0161a je akcija v primerjavi s povpre\u010dno akcijo izvedeno na podlagi trenutne strategije, in se uporablja za posodabljanje strategije in funkcije vrednosti. Bolj formalno je funkcija prednosti A(s, a) opredeljena kot razlika med vrednostjo Q (pri\u010dakovana kumulativna nagrada) za akcijo a v stanju s in funkcijo vrednosti (pri\u010dakovana kumulativna nagrada po trenutni strategiji) stanja s: A(s, a) = Q(s, a) - V(s) \u201cAdvantage\u201d funkcija je pomemben gradnik \u0161tevilnih algoritmov spodbujevalnega u\u010denja, kot sta A2C in PPO. Agentu omogo\u010da razlikovanje med dobrimi in slabimi akcijami z izra\u010dunom pri\u010dakovanega izbolj\u0161anja dolgoro\u010dne nagrade za dolo\u010deno akcijo v primerjavi s povpre\u010dno akcijo izvedeno na podlagi trenutne strategije. Ta informacija se uporabi za posodobitev strategije na na\u010din, ki agenta spodbuja k sprejemanju akcij, ki so bolj\u0161a od povpre\u010dne akcije. Eden od na\u010dinov za oceno funkcije prednosti je uporaba napake TD (napake \u010dasovne razlike) med ocenjeno funkcijo vrednosti in opazovano nagrado. To lahko storimo z metodo, imenovano enostopenjsko u\u010denje TD (temporal difference), ki v vsakem \u010dasovnem koraku posodobi funkcijo vrednosti in \u201cadvantage\u201d funkcijo. Drug pristop je uporaba natan\u010dnej\u0161e ocene \u201cadvantage\u201d funkcije, kot je posplo\u0161ena ocena prednosti (GAE), ki zdru\u017euje ve\u010d \u010dasovnih korakov, da zagotovi stabilnej\u0161o oceno \u201cadvantage\u201d funkcije. \u201cAdvantage\u201d funkcija je klju\u010den gradnik algoritmov spodbujevalnega u\u010denja, ki se uporablja za oceno kakovosti vsake akcije agenta. Agentu omogo\u010da, da se nau\u010di, katere akcije so bolj\u0161e od drugih, ter ustrezno posodobi strategijo in funkcijo vrednosti. Primer u\u010denja z DQN in PPO Prakti\u010dna uporaba U\u010denje z nevronskimi mre\u017eami z metodo DQN Uporabimo python paket Stable Baselines3 (SB3) Stable-Baselines3 Docs Stable Baselines3 omogo\u010da celo vrsto drugih algoritmov A2C PPO Raz\u0161iritev SB3 Contrib dodatni sodobnej\u0161i algoritmi SB3 Contrib dokumentacije Github repozitorij Q tabela Aproksimacija Q tabele s funkcijami Diskretne akcije Zvezne akcije Aproksimacija Q tabele z nevronsko mre\u017eo Python skripta za u\u010denje agenta Nova python skripta Incializacija okolja in u\u010denja import gym from stable_baselines3 import DQN env = gym . make ( \"MountainCar-v0\" ) 3. Preverimo prostor akcij in opazovanja print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) Inicializiramo u\u010denje agenta Podatki za inicializacijo so na spletni strani ter na rl-baselines3-zoo policy_kwargs = dict ( net_arch = [ 256 , 256 ]) model = DQN ( 'MlpPolicy' , env = env , learning_rate = 4e-3 , batch_size = 128 , buffer_size = 10000 , learning_starts = 1000 , gamma = 0.99 , target_update_interval = 600 , train_freq = 16 , gradient_steps = 8 , exploration_fraction = 0.2 , exploration_final_eps = 0.07 , policy_kwargs = policy_kwargs , seed = 2 , verbose = 1 ) U\u010denje agenta model . learn ( total_timesteps = 1.2e5 ) Shranimo model model . save ( \"dqn_car\" ) Python skripta za testiranje agenta Nova python skripta Incializacija okolja in u\u010denja import gym from stable_baselines3 import DQN from stable_baselines3.common.evaluation import evaluate_policy env = gym . make ( \"MountainCar-v0\" ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) Nalo\u017eimo in incializiramo agenta model = DQN . load ( \"dqn_car\" , env = env ) Za\u017eenemo in testiramo agenta mean_reward , std_reward = evaluate_policy ( model , model . get_env (), n_eval_episodes = 10 ) print ( f 'Mean reward: { mean_reward } , Std reward: { std_reward } ' ) obs = env . reset () while True : action , _state = model . predict ( obs , deterministic = True ) obs , reward , done , info = env . step ( action ) env . render () if done : obs = env . reset ()","title":"DQN"},{"location":"car_DQN/#ucenje-z-nevronskimi-mrezami-z-metodo-dqn","text":"","title":"U\u010denje z nevronskimi mre\u017eami z metodo DQN"},{"location":"car_DQN/#globoko-spodbujevalno-ucenje","text":"https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html","title":"Globoko spodbujevalno u\u010denje"},{"location":"car_DQN/#dqn","text":"Q-u\u010denje je algoritma za spodbujevalno u\u010denje, ki uporablja tabelo za shranjevanje vrednosti Q za vsak par stanje-akcija. Ker pa prostor stanj postaja vse ve\u010dji in kompleksnej\u0161i, postane shranjevanje vseh vrednosti Q v tabeli neprakti\u010dno. Tu pride na vrsto DQN. DQN je vrsta algoritma za Q-u\u010denje, ki za izra\u010dun (pribli\u017eka) vrednosti Q uporablja nevronsko mre\u017eo. Nevronska mre\u017ea sprejme stanje kot vhod in kot izhod izra\u010duna vrednosti Q za vsako mo\u017eno akcijo. Mre\u017ea je nau\u010dena s kombinacijo ponovitve izku\u0161enj in \u00bbtarget\u00ab nevronske mre\u017ee za izbolj\u0161anje stabilnosti in u\u010dinkovitosti. Tehnika ponovitve izku\u0161enj se uporablja za izbolj\u0161anje u\u010dinkovitosti u\u010denja. Namesto da bi nevronsko mre\u017eo u\u010dili takoj na vsaki izku\u0161nji, se izku\u0161nje shranijo v spomin in se iz njega naklju\u010dno izbere serija izku\u0161enj za u\u010denje mre\u017ee. To ima ve\u010d prednosti. Prvi\u010d, pomaga prekiniti korelacijo med zaporednimi izku\u0161njami, kar lahko zmanj\u0161a verjetnost, da bo mre\u017ea obti\u010dalo v lokalnem optimumu. Drugi\u010d, nevronski mre\u017ei omogo\u010da, da se u\u010di iz \u0161ir\u0161ega razpona izku\u0161enj, kar lahko privede do bolj\u0161e posplo\u0161itve in bolj\u0161ega delovanja. \u00bbTarget\u00ab nevronska mre\u017ea je lo\u010dena kopija glavnega nevronske mre\u017ee, ki se med u\u010denjem uporablja za izra\u010dun ciljnih (\u00bbtarget\u00ab) vrednosti Q. V DQN uporabljamo \u00bbtarget\u00ab nevronsko mre\u017eo za re\u0161evanje problema, znanega kot \"problem premikajo\u010de se tar\u010de\". Ta te\u017eava nastane, ko uporabimo isto omre\u017eje za izra\u010dunavanje napovedi vrednosti Q in ciljnih vrednosti Q ter po vsaki napovedi posodobimo ute\u017ei nevronske mre\u017ee. To lahko povzro\u010di hitro in nestanovitno spreminjanje ciljnih vrednosti Q, kar lahko privede do nestabilnosti v procesu u\u010denja. Za re\u0161itev te te\u017eave uporabimo \u00bbtarget\u00ab nevronsko mre\u017eo, ki je kopija glavne nevronske mre\u017ee, vendar z zamrznjenimi parametri. \u00bbTarget\u00ab nevronska mre\u017ea se uporablja za izra\u010dun ciljnih vrednosti Q, ute\u017ei glavnega omre\u017eja pa se redno posodabljajo le z uporabo ciljnih vrednosti Q, ki jih izra\u010duna ciljno omre\u017eje. To stabilizira postopek u\u010denja in omre\u017eju omogo\u010da u\u010dinkovitej\u0161e u\u010denje. Najprej za\u010dnemo z vhodnim stanjem kor vhodom v nevronsko mre\u017eo. Mre\u017ea nato vrne vrednosti Q za vsako mo\u017eno akcijo. Podobno kot pri Q-u\u010denju ne izberemo akcije z najvi\u0161jo vrednostjo Q. Namesto tega uporabimo tehniko, imenovano \"epsilon-greedy exploration\", kjer z verjetnostjo epsilon izberemo naklju\u010dno akcijo in z verjetnostjo 1-epsilon izberemo akcijo z najvi\u0161jo vrednostjo Q. To algoritmu omogo\u010da, da raziskuje nove akcije in potencialno najde bolj\u0161e re\u0161itve. Ko izberemo akcijo, jo izvedemo in opazujemo dobljeno nagrado in novo stanje. Ta izku\u0161nja se shrani v spomin za ponovitev, ki ga bomo pozneje uporabili za u\u010denje mre\u017ee. Nato naklju\u010dno izberemo serijo izku\u0161enj iz spomina in jih uporabimo za u\u010denje mre\u017ee. Vendar mre\u017ee ne \u017eelimo vedno znova u\u010diti na istih izku\u0161njah, saj to lahko privede do pojava \u00bboverfitting\u00ab. Za re\u0161itev te te\u017eave uporabimo \u00bbtarget\u00ab nevronsko mre\u017eo, ki je kopija glavnega nevronske mre\u017ee, vendar z zamrznjenimi parametri. To \u00bbtarget\u00ab nevronsko mre\u017eo uporabimo za izra\u010dun kon\u010dnih vrednosti Q za vsako izku\u0161njo, nato pa posodobimo parametre glavne mre\u017ee, da \u010dim bolj zmanj\u0161amo razliko med napovedanimi vrednostmi Q in kon\u010dnimi vrednostmi Q. Ta postopek se ponavlja iterativno, pri \u010demer nevronska mre\u017ea nenehno izbolj\u0161uje svoje ocene vrednosti Q, ko se u\u010di iz izku\u0161enj. \u010ce povzamemo, je DQN vrsta algoritma za Q-u\u010denje, ki za oceno vrednosti Q uporablja nevronsko mre\u017eo. Zdru\u017euje ponovitev izku\u0161enj in \u00bbtarget\u00ab nevronsko mre\u017eo za izbolj\u0161anje stabilnosti in u\u010dinkovitosti. Z uporabo te metode lahko obravnavamo ve\u010dje in bolj zapletene prostore stanj ter na koncu dose\u017eemo bolj\u0161e rezultate kot z Q-metodo.","title":"DQN"},{"location":"car_DQN/#ppo","text":"DQN (Deep Q-Network) in PPO (Proximal Policy Optimization) sta priljubljeni metodi za spodbujevalno u\u010denje, ki se uporabljata za u\u010denje agentov za izvajanje dolo\u010denih nalog. Vendar se razlikujeta glede na pristop in cilje. DQN je na Q vrednosti temelje\u010da metoda spodbujevalnega u\u010denja, ki u\u010di vrednosti Q (pri\u010dakovane prihodnje nagrade) za vsak par stanje-akcija. Cilj DQN je nau\u010diti se optimalno funkcijo Q, ki povezuje stanja z akcijami. To dose\u017eemo z u\u010denjem globoke nevronske mre\u017ee za oceno vrednosti Q z uporabo kombinacije ponovitve izku\u0161enj in \u00bbtarget\u00ab nevronske mre\u017ee. PPO pa je metoda spodbujevalnega u\u010denja na podlagi strategije, ki se neposredno u\u010di strategija za preslikavo iz stanj v akcije. Cilj PPO je maksimizirati pri\u010dakovano kumulativno nagrado agenta z iskanjem optimalne strategije. To se dose\u017ee z optimizacijo nadomestne objektivne funkcije, ki zagotavlja, da se nova strategija ne razlikuje preve\u010d od stare. \u010ceprav se lahko tako DQN kot PPO uporabljata za diskretne in zvezne prostore akcij, se razlikujeta po u\u010dinkovitosti vzor\u010denja, stabilnosti in enostavnosti izvajanja. DQN je na splo\u0161no manj vzor\u010dno u\u010dinkovit kot PPO, saj za u\u010denje natan\u010dne funkcije Q potrebuje veliko \u0161tevilo u\u010dnih vzorcev. Vendar je DQN znana po svoji stabilnosti in se uspe\u0161no uporablja v \u0161tevilnih aplikacijah, tudi v robotiki. Po drugi strani pa je PPO znana po svoji u\u010dinkovitosti vzor\u010denja in je dokazano dosegla vrhunske rezultate pri \u0161tevilnih nalogah zveznega vodenja. PPO je tudi razmeroma enostavna za izvajanje, saj ne potrebuje ponovitve izku\u0161enj ali lo\u010dene \u00bbtarget\u00ab nevronske mre\u017ee. \u010ceprav sta DQN in PPO zmogljivi metodi u\u010denja za spodbujevalno u\u010denje, se razlikujeta po pristopu in ciljih. DQN je metoda, ki temelji na vrednosti in se u\u010di optimalne funkcije Q, PPO pa je metoda, ki temelji na strategiji in se neposredno u\u010di optimalne strategije.","title":"PPO"},{"location":"car_DQN/#a2c","text":"A2C pa je metoda spodbujevalnega u\u010denja, ki temelji na strategiji in se neposredno u\u010di strategije (preslikava med stanji in akcijami). Cilj A2C je maksimizirati pri\u010dakovano kumulativno nagrado agenta z iskanjem optimalne strategije, podobno kot PPO. To se dose\u017ee s hkratnim u\u010denjem dveh nevronskih mre\u017e: \u00bbactor\u00ab nevronska mre\u017ea, ki aproksimira strategijo, in \u00bbcritic\u00ab nevronske mre\u017ee, ki ocenjuje funkcijo vrednosti. Klju\u010dna razlika med DQN in A2C je, da je DQN metoda \u00bboff-policy\u201d, kar pomeni, da se u\u010di optimalne vrednosti Q z uporabo lo\u010dene strategije, medtem ko je A2C metoda \u00bbon-policy\u201d, kar pomeni, da se u\u010di strategije in funkcije vrednosti neposredno iz trenutne strategije. Druga razlika je, da se DQN lahko uporablja tako za diskretne kot za zvezne prostore akcij, medtem ko se A2C obi\u010dajno uporablja za diskretne prostore akcij. Poleg tega je A2C na splo\u0161no bolj vzor\u010dno u\u010dinkovita kot DQN, saj se lahko u\u010di iz delnih trajektorij, medtem ko DQN za posodobitev svojih vrednosti Q potrebuje celotne trajektorije. Trajektorija $\\tau$ je sekvenca parov $stanje_0$->$akcija_0$->$stanje_1$->$akcija_1$->... Klju\u010dna razlika med PPO in A2C je v tem, da PPO uporablja \u00bbclipped\u00ab objektivno funkcijo, da prepre\u010di prevelike spremembe strategije ob vsaki posodobitvi, medtem ko A2C uporablja \u00bbadvantage\u00ab funkcijo za oceno kakovosti vsake akcije glede na trenutno strategijo. Druga razlika je, da je PPO na splo\u0161no bolj vzor\u010dno u\u010dinkovit kot A2C, saj uporablja tehniko, imenovano vzor\u010denje po pomembnosti, za popravke posodobitev gradienta. Vzor\u010denje po pomembnosti omogo\u010da, da se PPO u\u010di iz stare strategije, kar lahko izbolj\u0161a u\u010dinkovitost vzor\u010denja. \u010ceprav sta tako PPO kot A2C u\u010dinkoviti metodi za spodbujevalno u\u010denje, se razlikujeta po pristopu in ciljih. PPO je metoda, ki temelji na strategiji in se u\u010di optimalne strategije z uporabo optimizacije in \u00bbclipped\u00ab objektivne funkcije, A2C pa je prav tako metoda, ki temelji na strategiji in se u\u010di optimalne strategije in funkcije vrednosti z uporabo \u00bbadvantage\u00ab funkcije prednosti. \u201cAdvantage\u201d funkcija je temeljni koncept spodbujevalnega u\u010denja, ki se uporablja za oceno kakovosti vsake akcije agenta glede na trenutno strategijo. Meri, koliko bolj\u0161a je akcija v primerjavi s povpre\u010dno akcijo izvedeno na podlagi trenutne strategije, in se uporablja za posodabljanje strategije in funkcije vrednosti. Bolj formalno je funkcija prednosti A(s, a) opredeljena kot razlika med vrednostjo Q (pri\u010dakovana kumulativna nagrada) za akcijo a v stanju s in funkcijo vrednosti (pri\u010dakovana kumulativna nagrada po trenutni strategiji) stanja s: A(s, a) = Q(s, a) - V(s) \u201cAdvantage\u201d funkcija je pomemben gradnik \u0161tevilnih algoritmov spodbujevalnega u\u010denja, kot sta A2C in PPO. Agentu omogo\u010da razlikovanje med dobrimi in slabimi akcijami z izra\u010dunom pri\u010dakovanega izbolj\u0161anja dolgoro\u010dne nagrade za dolo\u010deno akcijo v primerjavi s povpre\u010dno akcijo izvedeno na podlagi trenutne strategije. Ta informacija se uporabi za posodobitev strategije na na\u010din, ki agenta spodbuja k sprejemanju akcij, ki so bolj\u0161a od povpre\u010dne akcije. Eden od na\u010dinov za oceno funkcije prednosti je uporaba napake TD (napake \u010dasovne razlike) med ocenjeno funkcijo vrednosti in opazovano nagrado. To lahko storimo z metodo, imenovano enostopenjsko u\u010denje TD (temporal difference), ki v vsakem \u010dasovnem koraku posodobi funkcijo vrednosti in \u201cadvantage\u201d funkcijo. Drug pristop je uporaba natan\u010dnej\u0161e ocene \u201cadvantage\u201d funkcije, kot je posplo\u0161ena ocena prednosti (GAE), ki zdru\u017euje ve\u010d \u010dasovnih korakov, da zagotovi stabilnej\u0161o oceno \u201cadvantage\u201d funkcije. \u201cAdvantage\u201d funkcija je klju\u010den gradnik algoritmov spodbujevalnega u\u010denja, ki se uporablja za oceno kakovosti vsake akcije agenta. Agentu omogo\u010da, da se nau\u010di, katere akcije so bolj\u0161e od drugih, ter ustrezno posodobi strategijo in funkcijo vrednosti. Primer u\u010denja z DQN in PPO","title":"A2C"},{"location":"car_DQN/#prakticna-uporaba","text":"U\u010denje z nevronskimi mre\u017eami z metodo DQN Uporabimo python paket Stable Baselines3 (SB3) Stable-Baselines3 Docs Stable Baselines3 omogo\u010da celo vrsto drugih algoritmov A2C PPO Raz\u0161iritev SB3 Contrib dodatni sodobnej\u0161i algoritmi SB3 Contrib dokumentacije Github repozitorij Q tabela Aproksimacija Q tabele s funkcijami Diskretne akcije Zvezne akcije Aproksimacija Q tabele z nevronsko mre\u017eo","title":"Prakti\u010dna uporaba"},{"location":"car_DQN/#python-skripta-za-ucenje-agenta","text":"Nova python skripta Incializacija okolja in u\u010denja import gym from stable_baselines3 import DQN env = gym . make ( \"MountainCar-v0\" ) 3. Preverimo prostor akcij in opazovanja print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) Inicializiramo u\u010denje agenta Podatki za inicializacijo so na spletni strani ter na rl-baselines3-zoo policy_kwargs = dict ( net_arch = [ 256 , 256 ]) model = DQN ( 'MlpPolicy' , env = env , learning_rate = 4e-3 , batch_size = 128 , buffer_size = 10000 , learning_starts = 1000 , gamma = 0.99 , target_update_interval = 600 , train_freq = 16 , gradient_steps = 8 , exploration_fraction = 0.2 , exploration_final_eps = 0.07 , policy_kwargs = policy_kwargs , seed = 2 , verbose = 1 ) U\u010denje agenta model . learn ( total_timesteps = 1.2e5 ) Shranimo model model . save ( \"dqn_car\" )","title":"Python skripta za u\u010denje agenta"},{"location":"car_DQN/#python-skripta-za-testiranje-agenta","text":"Nova python skripta Incializacija okolja in u\u010denja import gym from stable_baselines3 import DQN from stable_baselines3.common.evaluation import evaluate_policy env = gym . make ( \"MountainCar-v0\" ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) Nalo\u017eimo in incializiramo agenta model = DQN . load ( \"dqn_car\" , env = env ) Za\u017eenemo in testiramo agenta mean_reward , std_reward = evaluate_policy ( model , model . get_env (), n_eval_episodes = 10 ) print ( f 'Mean reward: { mean_reward } , Std reward: { std_reward } ' ) obs = env . reset () while True : action , _state = model . predict ( obs , deterministic = True ) obs , reward , done , info = env . step ( action ) env . render () if done : obs = env . reset ()","title":"Python skripta za testiranje agenta"},{"location":"car_action/","text":"Prikaz akcij glede na Q tabelo load_car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 import gym import numpy as np from time import sleep import sys ### import matplotlib.pyplot as plt ### import matplotlib.patches as mpatches ### env = gym . make ( \"MountainCar-v0\" ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) q_table = np . load ( f \"cart_e14900-qtable.npy\" ) print ( \"Q table size = \" , q_table . shape ) state = env . reset () discrete_state = get_discrete_state ( state ) env . render () done = False while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) env . render () #sleep(0.5) env . close () def get_actions ( dataset ): stolpec = 0 #print(type(data)) actions = np . ndarray ([ GRID_SIZE , GRID_SIZE ]) for stolpec in range ( GRID_SIZE ): vrstica = 0 for vrstica in range ( GRID_SIZE ): if dataset [ stolpec , vrstica , 0 ] == dataset [ stolpec , vrstica , 1 ] == dataset [ stolpec , vrstica , 2 ] == 0 : actions [ stolpec , vrstica ] = - 1 else : actions [ stolpec , vrstica ] = np . argmax ( dataset [ stolpec , vrstica ]) return actions def plot_graphs ( ep_list ): ep = 0 fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 15 )) fig . suptitle ( \"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\" , fontsize = 16 ) print ( enumerate ( axs . flat )) for i , ax in enumerate ( axs . flat ): data = np . load ( 'cart_e' + str ( ep_list [ ep ]) + '-qtable.npy' ) np . set_printoptions ( threshold = sys . maxsize ) positions = np . arange ( - 1.2 , 0.6 + discrete_os_win_size [ 0 ], discrete_os_win_size [ 0 ]) velocities = np . arange ( - 0.07 , 0.07 + discrete_os_win_size [ 1 ], discrete_os_win_size [ 1 ]) #ax = fig.add_subplot(2,2) labels = [ \"Neobiskana stanja\" , \"Premik levo\" , \"Ne naredimo ni\u010desar\" , \"Premik desno\" ] cmap = plt . colormaps . get_cmap ( 'Blues' ) #matplotlib.colormaps ax = plt . subplot ( 2 , 2 , i + 1 ) actions = get_actions ( data ) ax . pcolor ( velocities , positions , actions , cmap = cmap ) ax . set_ylabel ( \"Pozicija\" , fontsize = 14 ) ax . set_xlabel ( \"Hitrost\" , fontsize = 14 ) #ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r') #ax.plot(velocities_0[0],positions_0[0],'ro') ax . set_title ( 'Epizoda ' + str ( ep_list [ ep ] + 1 ), fontsize = 13 ) ep += 1 bound = np . linspace ( 0 , 1 , 5 ) print ( bound ) fig . legend ([ mpatches . Patch ( color = cmap ( b )) for b in bound [: - 1 ]], [ labels [ i ] for i in range ( 4 )], loc = 'upper right' ) plt . subplots_adjust ( wspace = 0.4 , hspace = 0.4 ) fig . savefig ( 'Qtable.jpg' ) ### #plt.show() plot_graphs ([ 0 , 5000 , 10000 , 14999 ])","title":"Prikaz akcij glede na Q tabelo"},{"location":"car_action/#prikaz-akcij-glede-na-q-tabelo","text":"load_car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 import gym import numpy as np from time import sleep import sys ### import matplotlib.pyplot as plt ### import matplotlib.patches as mpatches ### env = gym . make ( \"MountainCar-v0\" ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) q_table = np . load ( f \"cart_e14900-qtable.npy\" ) print ( \"Q table size = \" , q_table . shape ) state = env . reset () discrete_state = get_discrete_state ( state ) env . render () done = False while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) env . render () #sleep(0.5) env . close () def get_actions ( dataset ): stolpec = 0 #print(type(data)) actions = np . ndarray ([ GRID_SIZE , GRID_SIZE ]) for stolpec in range ( GRID_SIZE ): vrstica = 0 for vrstica in range ( GRID_SIZE ): if dataset [ stolpec , vrstica , 0 ] == dataset [ stolpec , vrstica , 1 ] == dataset [ stolpec , vrstica , 2 ] == 0 : actions [ stolpec , vrstica ] = - 1 else : actions [ stolpec , vrstica ] = np . argmax ( dataset [ stolpec , vrstica ]) return actions def plot_graphs ( ep_list ): ep = 0 fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 15 )) fig . suptitle ( \"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\" , fontsize = 16 ) print ( enumerate ( axs . flat )) for i , ax in enumerate ( axs . flat ): data = np . load ( 'cart_e' + str ( ep_list [ ep ]) + '-qtable.npy' ) np . set_printoptions ( threshold = sys . maxsize ) positions = np . arange ( - 1.2 , 0.6 + discrete_os_win_size [ 0 ], discrete_os_win_size [ 0 ]) velocities = np . arange ( - 0.07 , 0.07 + discrete_os_win_size [ 1 ], discrete_os_win_size [ 1 ]) #ax = fig.add_subplot(2,2) labels = [ \"Neobiskana stanja\" , \"Premik levo\" , \"Ne naredimo ni\u010desar\" , \"Premik desno\" ] cmap = plt . colormaps . get_cmap ( 'Blues' ) #matplotlib.colormaps ax = plt . subplot ( 2 , 2 , i + 1 ) actions = get_actions ( data ) ax . pcolor ( velocities , positions , actions , cmap = cmap ) ax . set_ylabel ( \"Pozicija\" , fontsize = 14 ) ax . set_xlabel ( \"Hitrost\" , fontsize = 14 ) #ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r') #ax.plot(velocities_0[0],positions_0[0],'ro') ax . set_title ( 'Epizoda ' + str ( ep_list [ ep ] + 1 ), fontsize = 13 ) ep += 1 bound = np . linspace ( 0 , 1 , 5 ) print ( bound ) fig . legend ([ mpatches . Patch ( color = cmap ( b )) for b in bound [: - 1 ]], [ labels [ i ] for i in range ( 4 )], loc = 'upper right' ) plt . subplots_adjust ( wspace = 0.4 , hspace = 0.4 ) fig . savefig ( 'Qtable.jpg' ) ### #plt.show() plot_graphs ([ 0 , 5000 , 10000 , 14999 ])","title":"Prikaz akcij glede na Q tabelo"},{"location":"car_reward/","text":"Izpis nagrade agenta med u\u010denjem car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 import gym import numpy as np import matplotlib.pyplot as plt ### env = gym . make ( \"MountainCar-v0\" ) #env = gym.make(\"Acrobot-v1\") #env = gym.make(\"CartPole-v1\") LEARNING_RATE = 0.1 DISCOUNT = 0.95 EPISODES = 25000 SHOW_EVERY = 100 epsilon = 1.0 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = EPISODES // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) #q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n])) q_table = np . zeros ( DISCRETE_OS_SIZE + [ env . action_space . n ]) print ( \"Q table size = \" , q_table . shape ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) reward_list = [] ### ave_reward_list = [] ### for episode in range ( EPISODES ): discrete_state = get_discrete_state ( env . reset ()) done = False tot_reward , reward = 0 , 0 ### if episode % SHOW_EVERY == 0 : render = True print ( episode ) else : render = False while not done : if np . random . random () > epsilon : action = np . argmax ( q_table [ discrete_state ]) else : action = np . random . randint ( 0 , env . action_space . n ) new_state , reward , done , _ = env . step ( action ) new_discrete_state = get_discrete_state ( new_state ) if episode % SHOW_EVERY == 0 : env . render () if not done : max_future_q = np . max ( q_table [ new_discrete_state ]) current_q = q_table [ discrete_state + ( action ,)] new_q = ( 1 - LEARNING_RATE ) * current_q + LEARNING_RATE * ( reward + DISCOUNT * max_future_q ) q_table [ discrete_state + ( action ,)] = new_q discrete_state = new_discrete_state tot_reward += reward ### if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value if episode % SHOW_EVERY == 0 : np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) reward_list . append ( tot_reward ) ### if episode % SHOW_EVERY == 0 : ### ave_reward = np . mean ( reward_list ) ### ave_reward_list . append ( ave_reward ) ### reward_list = [] ### print ( 'Episode {} Average Reward: {} ' . format ( episode , ave_reward )) ### np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) # Plot Rewards fig , ax = plt . subplots () ax . plot ( SHOW_EVERY * ( np . arange ( len ( ave_reward_list )) + 1 ), ave_reward_list ) ### ax . set_xlabel ( 'Episodes' ) ### ax . set_ylabel ( 'Average Reward' ) ### ax . set_title ( 'Average Reward vs Episodes' ) ### fig . savefig ( 'rewards.jpg' ) ### plt . show () ### Izpis nagrade za ve\u010d ponovitev load_car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 import gym import numpy as np import matplotlib.pyplot as plt ### env = gym . make ( \"MountainCar-v0\" ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) q_table = np . load ( f \"cart_e24999-qtable.npy\" ) print ( \"Q table size = \" , q_table . shape ) reward_list = [] ### for episode in range ( 50 ): ### state = env . reset () discrete_state = get_discrete_state ( state ) done = False print ( \"EPISODE \" , episode ) tot_reward , reward = 0 , 0 ### while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) tot_reward += reward ### reward_list . append ( tot_reward ) ### env . close () f = plt . figure () plt . plot ( reward_list ) plt . xticks ( range ( 0 , len ( reward_list ), 2 ), range ( 1 , len ( reward_list ) + 1 , 2 )) plt . axhline ( y = - 200 , color = 'k' , linestyle = '--' ) plt . axhline ( y = np . average ( reward_list ), color = 'blue' , linestyle = '--' , label = 'povpre\u010dna nagrada' ) plt . annotate ( str ( np . average ( reward_list )), xy = ( - 2 , np . average ( reward_list ) + 0.7 ), color = 'blue' , fontsize = 13 , weight = 'bold' ) ax = plt . gca () ax . tick_params ( axis = \"both\" , labelsize = 12 ) f . legend ( loc = 'right' , fontsize = 13 ) plt . xlabel ( 'Epizoda' , fontsize = 14 ) plt . ylabel ( 'Nagrada' , fontsize = 14 ) plt . title ( 'Vrednost nagrade v posamezni epizodi' , fontsize = 16 ) plt . show ()","title":"Izpis nagrade"},{"location":"car_reward/#izpis-nagrade-agenta-med-ucenjem","text":"car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 import gym import numpy as np import matplotlib.pyplot as plt ### env = gym . make ( \"MountainCar-v0\" ) #env = gym.make(\"Acrobot-v1\") #env = gym.make(\"CartPole-v1\") LEARNING_RATE = 0.1 DISCOUNT = 0.95 EPISODES = 25000 SHOW_EVERY = 100 epsilon = 1.0 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = EPISODES // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) #q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n])) q_table = np . zeros ( DISCRETE_OS_SIZE + [ env . action_space . n ]) print ( \"Q table size = \" , q_table . shape ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) reward_list = [] ### ave_reward_list = [] ### for episode in range ( EPISODES ): discrete_state = get_discrete_state ( env . reset ()) done = False tot_reward , reward = 0 , 0 ### if episode % SHOW_EVERY == 0 : render = True print ( episode ) else : render = False while not done : if np . random . random () > epsilon : action = np . argmax ( q_table [ discrete_state ]) else : action = np . random . randint ( 0 , env . action_space . n ) new_state , reward , done , _ = env . step ( action ) new_discrete_state = get_discrete_state ( new_state ) if episode % SHOW_EVERY == 0 : env . render () if not done : max_future_q = np . max ( q_table [ new_discrete_state ]) current_q = q_table [ discrete_state + ( action ,)] new_q = ( 1 - LEARNING_RATE ) * current_q + LEARNING_RATE * ( reward + DISCOUNT * max_future_q ) q_table [ discrete_state + ( action ,)] = new_q discrete_state = new_discrete_state tot_reward += reward ### if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value if episode % SHOW_EVERY == 0 : np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) reward_list . append ( tot_reward ) ### if episode % SHOW_EVERY == 0 : ### ave_reward = np . mean ( reward_list ) ### ave_reward_list . append ( ave_reward ) ### reward_list = [] ### print ( 'Episode {} Average Reward: {} ' . format ( episode , ave_reward )) ### np . save ( f \"cart_e { episode } -qtable.npy\" , q_table ) # Plot Rewards fig , ax = plt . subplots () ax . plot ( SHOW_EVERY * ( np . arange ( len ( ave_reward_list )) + 1 ), ave_reward_list ) ### ax . set_xlabel ( 'Episodes' ) ### ax . set_ylabel ( 'Average Reward' ) ### ax . set_title ( 'Average Reward vs Episodes' ) ### fig . savefig ( 'rewards.jpg' ) ### plt . show () ###","title":"Izpis nagrade agenta med u\u010denjem"},{"location":"car_reward/#izpis-nagrade-za-vec-ponovitev","text":"load_car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 import gym import numpy as np import matplotlib.pyplot as plt ### env = gym . make ( \"MountainCar-v0\" ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) q_table = np . load ( f \"cart_e24999-qtable.npy\" ) print ( \"Q table size = \" , q_table . shape ) reward_list = [] ### for episode in range ( 50 ): ### state = env . reset () discrete_state = get_discrete_state ( state ) done = False print ( \"EPISODE \" , episode ) tot_reward , reward = 0 , 0 ### while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) tot_reward += reward ### reward_list . append ( tot_reward ) ### env . close () f = plt . figure () plt . plot ( reward_list ) plt . xticks ( range ( 0 , len ( reward_list ), 2 ), range ( 1 , len ( reward_list ) + 1 , 2 )) plt . axhline ( y = - 200 , color = 'k' , linestyle = '--' ) plt . axhline ( y = np . average ( reward_list ), color = 'blue' , linestyle = '--' , label = 'povpre\u010dna nagrada' ) plt . annotate ( str ( np . average ( reward_list )), xy = ( - 2 , np . average ( reward_list ) + 0.7 ), color = 'blue' , fontsize = 13 , weight = 'bold' ) ax = plt . gca () ax . tick_params ( axis = \"both\" , labelsize = 12 ) f . legend ( loc = 'right' , fontsize = 13 ) plt . xlabel ( 'Epizoda' , fontsize = 14 ) plt . ylabel ( 'Nagrada' , fontsize = 14 ) plt . title ( 'Vrednost nagrade v posamezni epizodi' , fontsize = 16 ) plt . show ()","title":"Izpis nagrade za ve\u010d ponovitev"},{"location":"cart/","text":"Cartpole primer github povezava na py skripto za okolje Cart Pole Diskretne akcije (2 akciji) Zvezna opazovanja (4 spremenljivke) Nujna diskretizacija za uporabo Q u\u010denja Nekatera opazovanje imajo meje od -\u221e do +\u221e -> dolo\u010ditev mej na roke Nagrada: +1 za vsak korak -Optimizacija, da vodenje \u010dim dlje obdr\u017ei nihalo in vozi\u010dek pokonci in znotraj okolja Q u\u010denje Namigi obs_high = np . array ([ 2.4 , 3 , 0.21 , 3 ]) obs_low = - obs_high Predvideni rezultati U\u010denje Qtabela load_cartpole.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 import gym import numpy as np import sys ### import matplotlib.pyplot as plt ### import matplotlib.patches as mpatches ### env = gym . make ( \"CartPole-v1\" ) #### #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = np . array ([ 2.4 , 3 , 0.21 , 3 ]) #### obs_low = - obs_high discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) def get_actions ( dataset ): stolpec = 0 #print(type(data)) actions = np . ndarray ([ GRID_SIZE , GRID_SIZE ]) for stolpec in range ( GRID_SIZE ): vrstica = 0 for vrstica in range ( GRID_SIZE ): if dataset [ stolpec , vrstica , GRID_SIZE // 2 , GRID_SIZE // 2 , 0 ] == dataset [ stolpec , vrstica , GRID_SIZE // 2 , GRID_SIZE // 2 , 1 ] == 0 : ### actions [ stolpec , vrstica ] = - 1 else : actions [ stolpec , vrstica ] = np . argmax ( dataset [ stolpec , vrstica , GRID_SIZE // 2 , GRID_SIZE // 2 ]) ### return actions def plot_graphs ( ep_list ): ep = 0 fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 15 )) fig . suptitle ( \"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\" , fontsize = 16 ) print ( enumerate ( axs . flat )) for i , ax in enumerate ( axs . flat ): data = np . load ( 'cartpole_e' + str ( ep_list [ ep ]) + '-qtable.npy' ) np . set_printoptions ( threshold = sys . maxsize ) positions = np . arange ( - 2.4 , 2.4 + discrete_os_win_size [ 0 ], discrete_os_win_size [ 0 ]) ### velocities = np . arange ( - 3 , 3 + discrete_os_win_size [ 1 ], discrete_os_win_size [ 1 ]) ### #ax = fig.add_subplot(2,2) labels = [ \"Neobiskana stanja\" , \"Premik levo\" , \"Premik desno\" ] cmap = plt . colormaps . get_cmap ( 'Blues' ) #matplotlib.colormaps ax = plt . subplot ( 2 , 2 , i + 1 ) actions = get_actions ( data ) ax . pcolor ( velocities , positions , actions , cmap = cmap ) ax . set_ylabel ( \"Pozicija\" , fontsize = 14 ) ax . set_xlabel ( \"Hitrost\" , fontsize = 14 ) #ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r') #ax.plot(velocities_0[0],positions_0[0],'ro') ax . set_title ( 'Epizoda ' + str ( ep_list [ ep ] + 1 ), fontsize = 13 ) ep += 1 bound = np . linspace ( 0 , 1 , 4 ) print ( bound ) fig . legend ([ mpatches . Patch ( color = cmap ( b )) for b in bound [: - 1 ]], [ labels [ i ] for i in range ( 3 )], loc = 'upper right' ) plt . subplots_adjust ( wspace = 0.4 , hspace = 0.4 ) fig . savefig ( 'Qtable.jpg' ) ### plt . show () plot_graphs ([ 0 , 10000 , 20000 , 50000 ])","title":"Cartpole primer"},{"location":"cart/#cartpole-primer","text":"github povezava na py skripto za okolje Cart Pole Diskretne akcije (2 akciji) Zvezna opazovanja (4 spremenljivke) Nujna diskretizacija za uporabo Q u\u010denja Nekatera opazovanje imajo meje od -\u221e do +\u221e -> dolo\u010ditev mej na roke Nagrada: +1 za vsak korak -Optimizacija, da vodenje \u010dim dlje obdr\u017ei nihalo in vozi\u010dek pokonci in znotraj okolja","title":"Cartpole primer"},{"location":"cart/#q-ucenje","text":"","title":"Q u\u010denje"},{"location":"cart/#namigi","text":"obs_high = np . array ([ 2.4 , 3 , 0.21 , 3 ]) obs_low = - obs_high","title":"Namigi"},{"location":"cart/#predvideni-rezultati","text":"","title":"Predvideni rezultati"},{"location":"cart/#ucenje","text":"","title":"U\u010denje"},{"location":"cart/#qtabela","text":"load_cartpole.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 import gym import numpy as np import sys ### import matplotlib.pyplot as plt ### import matplotlib.patches as mpatches ### env = gym . make ( \"CartPole-v1\" ) #### #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = np . array ([ 2.4 , 3 , 0.21 , 3 ]) #### obs_low = - obs_high discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) def get_actions ( dataset ): stolpec = 0 #print(type(data)) actions = np . ndarray ([ GRID_SIZE , GRID_SIZE ]) for stolpec in range ( GRID_SIZE ): vrstica = 0 for vrstica in range ( GRID_SIZE ): if dataset [ stolpec , vrstica , GRID_SIZE // 2 , GRID_SIZE // 2 , 0 ] == dataset [ stolpec , vrstica , GRID_SIZE // 2 , GRID_SIZE // 2 , 1 ] == 0 : ### actions [ stolpec , vrstica ] = - 1 else : actions [ stolpec , vrstica ] = np . argmax ( dataset [ stolpec , vrstica , GRID_SIZE // 2 , GRID_SIZE // 2 ]) ### return actions def plot_graphs ( ep_list ): ep = 0 fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 15 )) fig . suptitle ( \"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\" , fontsize = 16 ) print ( enumerate ( axs . flat )) for i , ax in enumerate ( axs . flat ): data = np . load ( 'cartpole_e' + str ( ep_list [ ep ]) + '-qtable.npy' ) np . set_printoptions ( threshold = sys . maxsize ) positions = np . arange ( - 2.4 , 2.4 + discrete_os_win_size [ 0 ], discrete_os_win_size [ 0 ]) ### velocities = np . arange ( - 3 , 3 + discrete_os_win_size [ 1 ], discrete_os_win_size [ 1 ]) ### #ax = fig.add_subplot(2,2) labels = [ \"Neobiskana stanja\" , \"Premik levo\" , \"Premik desno\" ] cmap = plt . colormaps . get_cmap ( 'Blues' ) #matplotlib.colormaps ax = plt . subplot ( 2 , 2 , i + 1 ) actions = get_actions ( data ) ax . pcolor ( velocities , positions , actions , cmap = cmap ) ax . set_ylabel ( \"Pozicija\" , fontsize = 14 ) ax . set_xlabel ( \"Hitrost\" , fontsize = 14 ) #ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r') #ax.plot(velocities_0[0],positions_0[0],'ro') ax . set_title ( 'Epizoda ' + str ( ep_list [ ep ] + 1 ), fontsize = 13 ) ep += 1 bound = np . linspace ( 0 , 1 , 4 ) print ( bound ) fig . legend ([ mpatches . Patch ( color = cmap ( b )) for b in bound [: - 1 ]], [ labels [ i ] for i in range ( 3 )], loc = 'upper right' ) plt . subplots_adjust ( wspace = 0.4 , hspace = 0.4 ) fig . savefig ( 'Qtable.jpg' ) ### plt . show () plot_graphs ([ 0 , 10000 , 20000 , 50000 ])","title":"Qtabela"},{"location":"cartpole1/","text":"cartpole.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 import gym import numpy as np from time import sleep #env = gym.make(\"MountainCar-v0\") #env = gym.make(\"Acrobot-v1\") env = gym . make ( \"CartPole-v1\" ) ########## LEARNING_RATE = 0.1 DISCOUNT = 0.95 EPISODES = 15000 SHOW_EVERY = 1000 epsilon = 1.0 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = EPISODES // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = np . array ([ 2.4 , 3 , 0.21 , 3 ]) #### obs_low = - obs_high #### discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) #q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n])) q_table = np . zeros ( DISCRETE_OS_SIZE + [ env . action_space . n ]) print ( \"Q table size = \" , q_table . shape ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) for episode in range ( EPISODES ): discrete_state = get_discrete_state ( env . reset ()) done = False if episode % SHOW_EVERY == 0 : render = True print ( episode ) else : render = False while not done : if np . random . random () > epsilon : action = np . argmax ( q_table [ discrete_state ]) else : action = np . random . randint ( 0 , env . action_space . n ) new_state , reward , done , _ = env . step ( action ) new_discrete_state = get_discrete_state ( new_state ) if episode % SHOW_EVERY == 0 : env . render () if not done : max_future_q = np . max ( q_table [ new_discrete_state ]) current_q = q_table [ discrete_state + ( action ,)] new_q = ( 1 - LEARNING_RATE ) * current_q + LEARNING_RATE * ( reward + DISCOUNT * max_future_q ) q_table [ discrete_state + ( action ,)] = new_q discrete_state = new_discrete_state if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value if episode % SHOW_EVERY == 0 : np . save ( f \"cartpole_e { episode } -qtable.npy\" , q_table ) ### np . save ( f \"cartpole_e { episode } -qtable.npy\" , q_table ) ### load_cartpole.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import gym import numpy as np from time import sleep env = gym . make ( \"CartPole-v1\" ) #### #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = np . array ([ 2.4 , 3 , 0.21 , 3 ]) #### obs_low = - obs_high discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) q_table = np . load ( f \"cartpole_e14999-qtable.npy\" ) #### print ( \"Q table size = \" , q_table . shape ) state = env . reset () discrete_state = get_discrete_state ( state ) env . render () done = False while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) env . render () env . close ()","title":"Cartpole1"},{"location":"circ_env/","text":"Okolje Circle Envrionment This Python code defines a custom environment for the OpenAI Gym toolkit. The environment is a rectangular, and an agent in the form of a circle can move within it. The agent is required to avoid colliding with the boundary of the world. The agent's movement and collision with the environment are simulated using the Pybox2D and Pygame libraries. The environment is defined in a Python class named CircleEnvironment that inherits from the gym.Env class. The class defines several functions, including the __init__() , reset() , step() , render() , and close() functions, that are part of the OpenAI Gym environment API. Libraries Used The code imports the following libraries: gym : an open-source toolkit for developing and comparing reinforcement learning algorithms. OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It provides a set of environments for testing and training reinforcement learning agents, as well as standardized interfaces for interacting with those environments. numpy : a library for numerical computing with Python. math : a library for mathematical functions. pygame : a library for game development with Python. Pygame is a cross-platform set of Python modules designed for writing video games. It includes functionality to handle graphics, sound, input devices, and networking. Pygame can be used to create simple 2D games or more complex games with physics and AI. Box2D : a 2D physics engine for game development. Box2D is a 2D physics engine for simulations of physical systems. It can be used for simulations in games, robotics, computer vision, and other fields. It provides realistic simulation of collisions, forces, friction, and other physical interactions between objects in a 2D space. Variables Used The environment uses the following variables: PPM : the number of pixels per meter used for rendering the environment. TARGET_FPS : the target frame rate used for rendering the environment. TIME_STEP : the length of each simulation time step. WORLD_WIDTH : the width of the circular world in simulation units [ m ]. WORLD_HEIGHT : the height of the circular world in simulation units [ m ]. metadata : a dictionary that contains metadata about the environment, including the available render modes and the target render frame rate. contact_listener : an instance of a custom contact listener class that is used to detect collisions between the agent and the environment. Standard gym functions Used The environment uses the following functions: __init__() : the initialization function for the environment. It sets up the environment parameters, initializes the Pybox2D world, and sets the observation and action spaces. _get_obs() : a function that returns the current observation of the agent, which is its position within the world. reset() : a function that resets the environment to its initial state and returns the initial observation. step() : a function that takes an action as input, simulates the environment for one time step, and returns the new observation, reward, done flag, and optional information about the simulation. render() : a function that renders the environment using the specified render mode. _render_frame() : a function that renders a single frame of the environment. close() : a function that destroys the Pybox2D world and the agent, and closes the Pygame window. Custom functions _is_collision(object, goal) : Checks if there is a collision between the body object and the body goal. It takes in two b2Body objects, object and goal, and returns a boolean indicating whether there is a collision. It can take any two Box2D b2Body objects to check collison between them and not just body object and the body goal. create_agent(radius_px, friction) : Creates a new b2Body object representing the agent. It takes in the radius of the agent in pixels, radius_px, and a friction value, friction, for the agent's fixtures. create_puck(radius_px, friction, type) : Creates a new b2Body object representing the puck. It takes in the radius of the puck in pixels, radius_px, a friction value, friction, for the puck's fixtures, and a string type that indicates whether the puck should be a dynamic object ('d') or a kinematic object ('k'). create_goal(dim_px, position) : Creates a new b2Body object representing the goal. It takes in a tuple dim_px representing the dimensions of the goal in pixels, and a tuple position representing the position of the goal in the simulation. create_circ_target(radius_px) : Creates a new circular target. It takes in the radius of the target in pixels, radius_px. create_random_object(radius_px) : Creates a new random object with a random shape. It takes in the radius of the object in pixels, radius_px.. create_border() : Creates the boundary of the simulation. get_agent_position() : Returns the current position of the agent as a numpy array. get_puck_position() : Returns the current position of the puck as a numpy array. get_puck_velocity() : Returns the current velocity of the puck as a numpy array. get_target_position() : Returns the current position of the target as a numpy array. reset_agent(position) : Resets the position and velocity of the agent to a specified position. reset_puck(position) : Resets the position and velocity of the puck to a specified position. reset_target(position) : Resets the position of the target to a specified position. reset_random_object(position) : Resets the position and velocity of the random object to a specified position. set_agent_velocity(vel) : This function sets the linear and angular velocity of the agent in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the agent. set_puck_velocity(vel) : This function sets the linear and angular velocity of the puck in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the puck. draw_agent(color) : This function draws the agent as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the agent is converted to Pygame coordinates before drawing. draw_puck(color) : This function draws the puck as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the puck is converted to Pygame coordinates before drawing. draw_border(color) : This function draws the border of the game as a rectangle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. draw_target(color) : This function draws the target as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the target is converted to Pygame coordinates before drawing. draw_goal(color) : This function draws the goal as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. draw_random_object(color) : This function draws a random object as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position, angle and vertices of the object are converted to Pygame coordinates before drawing. is_agent_outside_screen() : This function checks if the agent is outside the screen or not. It returns True if the agent is outside the screen, otherwise it returns False. calc_distance(pos1, pos2) : This function calculates the Euclidean distance between two points. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. unit_vector(pos1, pos2) : This function calculates the unit vector pointing from pos1 to pos2. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. The function returns the unit vector as a numpy array. calculate_scaled_component(pos_agent, pos_target, vel) : This function calculates the scaled component of the velocity vector in the direction of the unit vector pointing from pos_agent to pos_target. The input parameters pos_agent and pos_target are tuples containing the x and y coordinates of the points. The input parameter vel is a tuple containing the x and y components of the velocity vector. The function returns the scaled component as a float value. Koda Premikanje agenta s tipkovnico Ustvarimo agenta, dolo\u010dimo observation_space in action_space . __init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 ]), high = np . array ([ self . width , self . height ]), dtype = np . float32 ) self . action_space = spaces . Box ( low =- 2.0 , high = 2.0 , shape = ( 2 ,), dtype = np . float32 ) self . create_agent ( agent_radius_px , 0.1 ) Potrebujemo vsaj en podatek za observacijo. _get_obs() agent_pos = self . get_agent_position () return agent_pos Ponastavimo agenta v reset() funkciji. reset() self . reset_agent (( np . random . uniform ( self . agent_radius * 1.5 , self . width - self . agent_radius * 1.5 ), np . random . uniform ( self . agent_radius * 1.5 , self . height - self . agent_radius * 1.5 ))) Naredimo korak v step() funkciji. step() Premik agenta za action self . set_agent_velocity ( action ) Dolo\u010dimo obs spremenljivko. obs = self . _get_obs () Dolo\u010dimo, reward in done spremenljivki. Za za\u010detek inicializacija: reward = 0.0 done = False Nato lahko dolo\u010dimo osnovne pogoje: if self . current_step >= self . time_steps : #reward = -1.0 done = True izri\u0161emo agenta _render_frame() self . draw_agent ( green ) Na koncu izbri\u0161emo agenta v close() funkciji close() self . world . DestroyBody ( self . agent ) Za\u017eenemo okolje Za\u017eeni skripto playCirc.py python3 playCirc.py V skripti pove\u010dajte hitrost na 5 . playCirc.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 while True : if any ([ event . type == pygame . QUIT for event in pygame . event . get ()]): break #player controls keys = pygame . key . get_pressed () if keys [ pygame . K_LEFT ]: x = - 1 elif keys [ pygame . K_RIGHT ]: x = 1 else : x = 0 if keys [ pygame . K_UP ]: y = - 1 elif keys [ pygame . K_DOWN ]: y = 1 else : y = 0 action = np . array ([ x , y ], dtype = np . float32 ) #action = env.action_space.sample() #print(action) obs , reward , done , _ = env . step ( action ) # Render the environment env . render () # Check if the episode is finished if done : obs = env . reset () # Close the environment env . close () Dodajmo osnovno mejo __init__() self . create_border () - _render_frame() self . draw_border ( black ) Zaklju\u010dimo episodo, ko se agent dotakne meje. step() if self . _is_collision ( self . agent , self . border ): reward = - 1.0 done = True","title":"Okolje Circle Envrionment"},{"location":"circ_env/#okolje-circle-envrionment","text":"This Python code defines a custom environment for the OpenAI Gym toolkit. The environment is a rectangular, and an agent in the form of a circle can move within it. The agent is required to avoid colliding with the boundary of the world. The agent's movement and collision with the environment are simulated using the Pybox2D and Pygame libraries. The environment is defined in a Python class named CircleEnvironment that inherits from the gym.Env class. The class defines several functions, including the __init__() , reset() , step() , render() , and close() functions, that are part of the OpenAI Gym environment API.","title":"Okolje Circle Envrionment"},{"location":"circ_env/#libraries-used","text":"The code imports the following libraries: gym : an open-source toolkit for developing and comparing reinforcement learning algorithms. OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It provides a set of environments for testing and training reinforcement learning agents, as well as standardized interfaces for interacting with those environments. numpy : a library for numerical computing with Python. math : a library for mathematical functions. pygame : a library for game development with Python. Pygame is a cross-platform set of Python modules designed for writing video games. It includes functionality to handle graphics, sound, input devices, and networking. Pygame can be used to create simple 2D games or more complex games with physics and AI. Box2D : a 2D physics engine for game development. Box2D is a 2D physics engine for simulations of physical systems. It can be used for simulations in games, robotics, computer vision, and other fields. It provides realistic simulation of collisions, forces, friction, and other physical interactions between objects in a 2D space.","title":"Libraries Used"},{"location":"circ_env/#variables-used","text":"The environment uses the following variables: PPM : the number of pixels per meter used for rendering the environment. TARGET_FPS : the target frame rate used for rendering the environment. TIME_STEP : the length of each simulation time step. WORLD_WIDTH : the width of the circular world in simulation units [ m ]. WORLD_HEIGHT : the height of the circular world in simulation units [ m ]. metadata : a dictionary that contains metadata about the environment, including the available render modes and the target render frame rate. contact_listener : an instance of a custom contact listener class that is used to detect collisions between the agent and the environment.","title":"Variables Used"},{"location":"circ_env/#standard-gym-functions-used","text":"The environment uses the following functions: __init__() : the initialization function for the environment. It sets up the environment parameters, initializes the Pybox2D world, and sets the observation and action spaces. _get_obs() : a function that returns the current observation of the agent, which is its position within the world. reset() : a function that resets the environment to its initial state and returns the initial observation. step() : a function that takes an action as input, simulates the environment for one time step, and returns the new observation, reward, done flag, and optional information about the simulation. render() : a function that renders the environment using the specified render mode. _render_frame() : a function that renders a single frame of the environment. close() : a function that destroys the Pybox2D world and the agent, and closes the Pygame window.","title":"Standard gym functions Used"},{"location":"circ_env/#custom-functions","text":"_is_collision(object, goal) : Checks if there is a collision between the body object and the body goal. It takes in two b2Body objects, object and goal, and returns a boolean indicating whether there is a collision. It can take any two Box2D b2Body objects to check collison between them and not just body object and the body goal. create_agent(radius_px, friction) : Creates a new b2Body object representing the agent. It takes in the radius of the agent in pixels, radius_px, and a friction value, friction, for the agent's fixtures. create_puck(radius_px, friction, type) : Creates a new b2Body object representing the puck. It takes in the radius of the puck in pixels, radius_px, a friction value, friction, for the puck's fixtures, and a string type that indicates whether the puck should be a dynamic object ('d') or a kinematic object ('k'). create_goal(dim_px, position) : Creates a new b2Body object representing the goal. It takes in a tuple dim_px representing the dimensions of the goal in pixels, and a tuple position representing the position of the goal in the simulation. create_circ_target(radius_px) : Creates a new circular target. It takes in the radius of the target in pixels, radius_px. create_random_object(radius_px) : Creates a new random object with a random shape. It takes in the radius of the object in pixels, radius_px.. create_border() : Creates the boundary of the simulation. get_agent_position() : Returns the current position of the agent as a numpy array. get_puck_position() : Returns the current position of the puck as a numpy array. get_puck_velocity() : Returns the current velocity of the puck as a numpy array. get_target_position() : Returns the current position of the target as a numpy array. reset_agent(position) : Resets the position and velocity of the agent to a specified position. reset_puck(position) : Resets the position and velocity of the puck to a specified position. reset_target(position) : Resets the position of the target to a specified position. reset_random_object(position) : Resets the position and velocity of the random object to a specified position. set_agent_velocity(vel) : This function sets the linear and angular velocity of the agent in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the agent. set_puck_velocity(vel) : This function sets the linear and angular velocity of the puck in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the puck. draw_agent(color) : This function draws the agent as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the agent is converted to Pygame coordinates before drawing. draw_puck(color) : This function draws the puck as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the puck is converted to Pygame coordinates before drawing. draw_border(color) : This function draws the border of the game as a rectangle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. draw_target(color) : This function draws the target as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the target is converted to Pygame coordinates before drawing. draw_goal(color) : This function draws the goal as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. draw_random_object(color) : This function draws a random object as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position, angle and vertices of the object are converted to Pygame coordinates before drawing. is_agent_outside_screen() : This function checks if the agent is outside the screen or not. It returns True if the agent is outside the screen, otherwise it returns False. calc_distance(pos1, pos2) : This function calculates the Euclidean distance between two points. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. unit_vector(pos1, pos2) : This function calculates the unit vector pointing from pos1 to pos2. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. The function returns the unit vector as a numpy array. calculate_scaled_component(pos_agent, pos_target, vel) : This function calculates the scaled component of the velocity vector in the direction of the unit vector pointing from pos_agent to pos_target. The input parameters pos_agent and pos_target are tuples containing the x and y coordinates of the points. The input parameter vel is a tuple containing the x and y components of the velocity vector. The function returns the scaled component as a float value.","title":"Custom functions"},{"location":"circ_env/#koda","text":"","title":"Koda"},{"location":"circ_env/#premikanje-agenta-s-tipkovnico","text":"","title":"Premikanje agenta s tipkovnico"},{"location":"circ_env/#ustvarimo-agenta-dolocimo-observation_space-in-action_space","text":"__init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 ]), high = np . array ([ self . width , self . height ]), dtype = np . float32 ) self . action_space = spaces . Box ( low =- 2.0 , high = 2.0 , shape = ( 2 ,), dtype = np . float32 ) self . create_agent ( agent_radius_px , 0.1 )","title":"Ustvarimo agenta, dolo\u010dimo observation_space in action_space."},{"location":"circ_env/#potrebujemo-vsaj-en-podatek-za-observacijo","text":"_get_obs() agent_pos = self . get_agent_position () return agent_pos","title":"Potrebujemo vsaj en podatek za observacijo."},{"location":"circ_env/#ponastavimo-agenta-v-reset-funkciji","text":"reset() self . reset_agent (( np . random . uniform ( self . agent_radius * 1.5 , self . width - self . agent_radius * 1.5 ), np . random . uniform ( self . agent_radius * 1.5 , self . height - self . agent_radius * 1.5 )))","title":"Ponastavimo agenta v reset() funkciji."},{"location":"circ_env/#naredimo-korak-v-step-funkciji","text":"step() Premik agenta za action self . set_agent_velocity ( action ) Dolo\u010dimo obs spremenljivko. obs = self . _get_obs () Dolo\u010dimo, reward in done spremenljivki. Za za\u010detek inicializacija: reward = 0.0 done = False Nato lahko dolo\u010dimo osnovne pogoje: if self . current_step >= self . time_steps : #reward = -1.0 done = True","title":"Naredimo korak v step() funkciji."},{"location":"circ_env/#izrisemo-agenta","text":"_render_frame() self . draw_agent ( green )","title":"izri\u0161emo agenta"},{"location":"circ_env/#na-koncu-izbrisemo-agenta-v-close-funkciji","text":"close() self . world . DestroyBody ( self . agent )","title":"Na koncu izbri\u0161emo agenta v close() funkciji"},{"location":"circ_env/#zazenemo-okolje","text":"Za\u017eeni skripto playCirc.py python3 playCirc.py V skripti pove\u010dajte hitrost na 5 . playCirc.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 while True : if any ([ event . type == pygame . QUIT for event in pygame . event . get ()]): break #player controls keys = pygame . key . get_pressed () if keys [ pygame . K_LEFT ]: x = - 1 elif keys [ pygame . K_RIGHT ]: x = 1 else : x = 0 if keys [ pygame . K_UP ]: y = - 1 elif keys [ pygame . K_DOWN ]: y = 1 else : y = 0 action = np . array ([ x , y ], dtype = np . float32 ) #action = env.action_space.sample() #print(action) obs , reward , done , _ = env . step ( action ) # Render the environment env . render () # Check if the episode is finished if done : obs = env . reset () # Close the environment env . close ()","title":"Za\u017eenemo okolje"},{"location":"circ_env/#dodajmo-osnovno-mejo","text":"__init__() self . create_border () - _render_frame() self . draw_border ( black ) Zaklju\u010dimo episodo, ko se agent dotakne meje. step() if self . _is_collision ( self . agent , self . border ): reward = - 1.0 done = True","title":"Dodajmo osnovno mejo"},{"location":"circ_env_obj_gol/","text":"__init__() ############ TUKAJ SPREMINJATE agent_radius_px = 30 object_radius_px = 30 self . max_puck_vel = 5.0 self . max_agent_vel = 2.0 self . time_steps = 500 self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height ]), dtype = np . float32 ) self . action_space = spaces . Box ( low =- self . max_agent_vel , high = self . max_agent_vel , shape = ( 2 ,), dtype = np . float32 ) self . create_agent ( agent_radius_px , 0.1 ) self . create_border () #self.create_circ_target(25) #self.create_puck(object_radius_px, 0.5, 'k') self . create_random_object ( object_radius_px ) self . create_goal (( 400 , 10 ), ( self . width / 2 , 10 / self . PPM )) ############ DO TUKAJ SPREMINJATE _get_obs() agent_pos = self . get_agent_position () #target_pos = self.get_target_position() pak_pos = self . get_puck_position () #pak_vel = self.get_puck_velocity() return np . concatenate (( agent_pos , pak_pos )) reset() ############ TUKAJ SPREMINJATE self . reset_agent (( np . random . uniform ( self . agent_radius * 1.5 , self . width - self . agent_radius * 1.5 ), np . random . uniform ( self . agent_radius * 1.5 , self . height - self . agent_radius * 1.5 ))) #self.reset_target((np.random.uniform(self.agent_radius*2, self.width - self.agent_radius*2), np.random.uniform(self.agent_radius*2, self.height - self.agent_radius*2))) #self.reset_puck((np.random.uniform(self.object_radius+self.agent_radius*3, self.width - self.object_radius-self.agent_radius*3), np.random.uniform(self.object_radius+self.agent_radius*3, self.height - self.object_radius-self.agent_radius*3))) self . reset_random_object (( np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . width - self . object_radius - self . agent_radius * 3 ), np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . height - self . object_radius - self . agent_radius * 3 ))) ############ DO TUKAJ SPREMINJATE step() ############ TUKAJ SPREMINJATE self . set_agent_velocity ( action ) obs = self . _get_obs () self . object . ApplyAngularImpulse ( - 0.25 * self . object . inertia * self . object . angularVelocity , True ) self . object . ApplyForce ( - 1 * self . object . linearVelocity , self . object . worldCenter , True ) reward = 0.0 done = False #if self.calc_distance(self.get_puck_position(), self.get_target_position()) < self.target_radius: # reward = 1.0 # done = True if self . _is_collision ( self . object , self . goal ): reward = 1.0 done = True if self . _is_collision ( self . agent , self . object ): reward = 0.01 if self . _is_collision ( self . object , self . border ): reward = - 0.01 if self . current_step >= self . time_steps : reward = - 1.0 done = True if self . _is_collision ( self . agent , self . border ): reward = - 1.0 done = True ############ DO TUKAJ SPREMINJATE _render_frame() ############ TUKAJ SPREMINJATE self . draw_agent ( green ) self . draw_border ( black ) #self.draw_target(blue) self . draw_goal ( blue ) #self.draw_puck(yellow) self . draw_random_object ( yellow ) ############ DO TUKAJ SPREMINJATE","title":"Pomik naklju\u010dnega objekta v gol"},{"location":"circ_env_pak/","text":"Dotik agenta s pakom Koda posameznih funkcij v circle_world.py __init__() self . create_puck ( object_radius_px , 'k' ) _get_obs() pak_pos = self . get_puck_position () return np . concatenate (( agent_pos , pak_pos )) reset() self . reset_puck (( np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . width - self . object_radius - self . agent_radius * 3 ), np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . height - self . object_radius - self . agent_radius * 3 ))) step() if self . _is_collision ( self . agent , self . object ): reward = 1.0 done = True _render_frame() self . draw_puck ( yellow ) Potek u\u010denja","title":"Dotik s pakom"},{"location":"circ_env_pak/#dotik-agenta-s-pakom","text":"","title":"Dotik agenta s pakom"},{"location":"circ_env_pak/#koda-posameznih-funkcij-v-circle_worldpy","text":"__init__() self . create_puck ( object_radius_px , 'k' ) _get_obs() pak_pos = self . get_puck_position () return np . concatenate (( agent_pos , pak_pos )) reset() self . reset_puck (( np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . width - self . object_radius - self . agent_radius * 3 ), np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . height - self . object_radius - self . agent_radius * 3 ))) step() if self . _is_collision ( self . agent , self . object ): reward = 1.0 done = True _render_frame() self . draw_puck ( yellow ) Potek u\u010denja","title":"Koda posameznih funkcij v circle_world.py"},{"location":"circ_env_pak_gol/","text":"__init__() agent_radius_px = 30 object_radius_px = 30 self . max_agent_vel = 2.0 self . max_puck_vel = 5.0 self . time_steps = 500 self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height ]), dtype = np . float32 ) self . action_space = spaces . Box ( low =- self . max_agent_vel , high = self . max_agent_vel , shape = ( 2 ,), dtype = np . float32 ) self . create_agent ( agent_radius_px , 'd' ) self . create_puck ( object_radius_px , 'd' ) self . create_goal (( 400 , 10 ), ( self . width / 2 , 10 / self . PPM )) self . create_border () step() self . set_agent_velocity ( action ) self . limit_puck_velocity ( self . max_puck_vel ) reward = - 1.0 / self . time_steps done = False if self . current_step >= self . time_steps : done = True if self . _is_collision ( self . object , self . goal ): reward += 1.0 done = True if self . _is_collision ( self . agent , self . object ): goal_pos = np . array ([ self . goal . position . x , self . goal . position . y ]) coll_normal = self . get_puck_position () - self . get_agent_position () coll_normal = coll_normal / np . linalg . norm ( coll_normal ) F_comp = self . calculate_component ( self . get_puck_position (), goal_pos , coll_normal ) reward += 0.05 * F_comp _render_frame() self . draw_goal ( blue ) self . draw_border ( black ) Nagrada skozi \u010das Primerjava u\u010denja z u\u010denjem potiskanja do roba Vijol\u010dna je u\u010denje potiska do gola.","title":"Pomik paka v gol 2"},{"location":"circ_env_pak_gol2/","text":"Drugi primeri potiskanje Manj\u0161i gol, razli\u010dne pozicije levo-desno Primer okolja v katerem je gol manj\u0161i in ima razli\u010dno pozicijo levo-desno. Osnovne funkcije __init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height , self . width , self . height ]), dtype = np . float32 ) self . create_goal (( 100 , 10 ), ( self . width / 2 + np . random . uniform ( - self . width / 2 + 50 , self . width / 2 - 50 ) / self . PPM , 10 / self . PPM )) _get_obs() goal_pos = self . get_goal_position () return np . concatenate (( agent_pos , pak_pos , goal_pos )) reset() self . reset_goal (( self . width / 2 + np . random . uniform ( - self . width / 2 + 50 / self . PPM , self . width / 2 - 50 / self . PPM ), 10 / self . PPM )) step() reward = - 1.0 / self . time_steps done = False if self . current_step >= self . time_steps : done = True if self . _is_collision ( self . object , self . goal ): reward += 1.0 done = True if self . _is_collision ( self . agent , self . object ): coll_normal = self . get_puck_position () - self . get_agent_position () coll_normal = coll_normal / np . linalg . norm ( coll_normal ) F_comp = self . calculate_component ( self . get_puck_position (), self . get_goal_position (), coll_normal ) reward += 0.05 * F_comp if self . _is_collision ( self . agent , self . border ): reward += - 1.0 done = True if self . _is_collision ( self . object , self . border ): reward += - 0.007 Dodatne funkcije def get_goal_position ( self ): return np . array ([ self . goal . position . x , self . goal . position . y ]) def reset_goal ( self , position ): self . goal . position = position self . goal . linearVelocity = ( 0 , 0 ) self . goal . angularVelocity = 0.0 Primerjava u\u010denja za velik in majhen gol Temno modra je u\u010denje potiska do velikega gola. Svetlo modra je u\u010denje potiska do malega gola. Kvadraten gol, razli\u010dne pozicije levo-desno, gor-dol Primer okolja v katerem je gol v obliki kvadrata in ima razli\u010dno pozicijo levo-desno ter gor-dol. Osnovne funkcije __init__() self . goal_dim = 100 self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height , self . width , self . height ]), dtype = np . float32 ) self . create_goal (( self . goal_dim , self . goal_dim ), ( self . width / 2 + np . random . uniform ( - self . width / 2 + self . goal_dim / 2 , self . width / 2 - self . goal_dim / 2 ) / self . PPM , np . random . uniform ( self . goal_dim / self . PPM / 2 , 2 * self . goal_dim / self . PPM ))) reset() self . reset_goal (( self . width / 2 + np . random . uniform ( - self . width / 2 + self . goal_dim / 2 , self . width / 2 - self . goal_dim / 2 ) / self . PPM , np . random . uniform ( self . goal_dim / self . PPM / 2 , 2 * self . goal_dim / self . PPM ))) Potek u\u010denja Dodaten primer Potek u\u010denja","title":"Pomik paka v gol 3"},{"location":"circ_env_pak_gol2/#drugi-primeri-potiskanje","text":"","title":"Drugi primeri potiskanje"},{"location":"circ_env_pak_gol2/#manjsi-gol-razlicne-pozicije-levo-desno","text":"Primer okolja v katerem je gol manj\u0161i in ima razli\u010dno pozicijo levo-desno.","title":"Manj\u0161i gol, razli\u010dne pozicije levo-desno"},{"location":"circ_env_pak_gol2/#osnovne-funkcije","text":"__init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height , self . width , self . height ]), dtype = np . float32 ) self . create_goal (( 100 , 10 ), ( self . width / 2 + np . random . uniform ( - self . width / 2 + 50 , self . width / 2 - 50 ) / self . PPM , 10 / self . PPM )) _get_obs() goal_pos = self . get_goal_position () return np . concatenate (( agent_pos , pak_pos , goal_pos )) reset() self . reset_goal (( self . width / 2 + np . random . uniform ( - self . width / 2 + 50 / self . PPM , self . width / 2 - 50 / self . PPM ), 10 / self . PPM )) step() reward = - 1.0 / self . time_steps done = False if self . current_step >= self . time_steps : done = True if self . _is_collision ( self . object , self . goal ): reward += 1.0 done = True if self . _is_collision ( self . agent , self . object ): coll_normal = self . get_puck_position () - self . get_agent_position () coll_normal = coll_normal / np . linalg . norm ( coll_normal ) F_comp = self . calculate_component ( self . get_puck_position (), self . get_goal_position (), coll_normal ) reward += 0.05 * F_comp if self . _is_collision ( self . agent , self . border ): reward += - 1.0 done = True if self . _is_collision ( self . object , self . border ): reward += - 0.007","title":"Osnovne funkcije"},{"location":"circ_env_pak_gol2/#dodatne-funkcije","text":"def get_goal_position ( self ): return np . array ([ self . goal . position . x , self . goal . position . y ]) def reset_goal ( self , position ): self . goal . position = position self . goal . linearVelocity = ( 0 , 0 ) self . goal . angularVelocity = 0.0 Primerjava u\u010denja za velik in majhen gol Temno modra je u\u010denje potiska do velikega gola. Svetlo modra je u\u010denje potiska do malega gola.","title":"Dodatne funkcije"},{"location":"circ_env_pak_gol2/#kvadraten-gol-razlicne-pozicije-levo-desno-gor-dol","text":"Primer okolja v katerem je gol v obliki kvadrata in ima razli\u010dno pozicijo levo-desno ter gor-dol.","title":"Kvadraten gol, razli\u010dne pozicije levo-desno, gor-dol"},{"location":"circ_env_pak_gol2/#osnovne-funkcije_1","text":"__init__() self . goal_dim = 100 self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height , self . width , self . height ]), dtype = np . float32 ) self . create_goal (( self . goal_dim , self . goal_dim ), ( self . width / 2 + np . random . uniform ( - self . width / 2 + self . goal_dim / 2 , self . width / 2 - self . goal_dim / 2 ) / self . PPM , np . random . uniform ( self . goal_dim / self . PPM / 2 , 2 * self . goal_dim / self . PPM ))) reset() self . reset_goal (( self . width / 2 + np . random . uniform ( - self . width / 2 + self . goal_dim / 2 , self . width / 2 - self . goal_dim / 2 ) / self . PPM , np . random . uniform ( self . goal_dim / self . PPM / 2 , 2 * self . goal_dim / self . PPM ))) Potek u\u010denja","title":"Osnovne funkcije"},{"location":"circ_env_pak_gol2/#dodaten-primer","text":"Potek u\u010denja","title":"Dodaten primer"},{"location":"circ_env_pak_rob/","text":"Dinamika agenta s pakom Popravki funkcij __init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height ]), dtype = np . float32 ) _get_obs() return np . concatenate (( agent_pos , pak_pos )) reset() self . reset_agent (( np . random . uniform ( self . agent_radius * 1.5 , self . width - self . agent_radius * 1.5 ), np . random . uniform ( self . agent_radius * 1.5 , self . height - self . agent_radius * 1.5 ))) self . reset_puck (( np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . width - self . object_radius - self . agent_radius * 3 ), np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . height - self . object_radius - self . agent_radius * 3 ))) step() Po spodnjem algoritmu napi\u0161ite kodo za izra\u010dun hitrosti paka. Razlaga algoritma: Nastavite K na 2000 in B na 0.5 Nastavite m na 1 Pridobite polo\u017eaj agenta in ga shranite v pos_a Pridobite polo\u017eaj paka in ga shranite v pos_p Izra\u010dunajte razdaljo med agentom in pakom kot (agent_radius+object_radius) - calc_distance(pos_a, pos_p) in jo shranite v dist Ustvari spremenljivko FK z ni\u010dlami z enakimi dimenzijami kot pos_p \u010ce je dist > 0, potem nastavite smer kot enotski vektor razlike med pos_a in pos_p in nastavite FK kot K krat dist krat smer Pridobite hitrost plo\u0161\u010dka in jo shranite v vel_p Nastavite FB kot -B krat vel_p Ustvari spremenljivko F z ni\u010del z enakimi dimenzijami kot pos_p Ustvari spremenljivko acc z ni\u010dlami, ki ima enake dimenzije kot pos_p Nastavite F kot vsoto FK in FB Nastavite acc kot F, deljeno z m Posodobite hitrost plo\u0161\u010dka tako, da jo nastavite na vel_p plus acc krat TIME_STEP Izra\u010dun trka med krogoma step() Kon\u010danje epizode, ko pote\u010de \u010das: reward = - 1.0 / self . time_steps done = False if self . current_step >= self . time_steps : done = True Dotik s pakom step() V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za pozitivno nagrado reward += 1.0 in kon\u010danje epizode. Psevdokoda : if is_contact_between_agent_and_pack ... reward += 1.0 done = True Odbijanje paka izven okolja step() V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za majhno pozitivno nagrado reward = 0.1 . if is_contact_between_agent_and_pack ... reward += 0.05 #done = True Definirajte novo funkcijo is_puck_outside_screen in jo dodajte za funkcijo is_agent_outside_screen . def is_puck_outside_screen ( self ): outside = False if ( self . object . position . x < self . object_radius ) or ( self . object . position . x > ( self . width - self . object_radius )): outside = True if ( self . object . position . y < self . object_radius ) or ( self . object . position . y > ( self . height - self . object_radius )): outside = True return outside V step() dodate pogoj za kon\u010danje epizode z pozitivno nagrado: if self . is_puck_outside_screen (): reward = 1.0 done = True Nagrada skozi \u010das s PPO metodo za odboj paka iz okolja Streljanje na zgornji rob Za omejitev okolja uporabite funkcijo self.wall_contact_force(object, object_radius, K) , ki izra\u010duna silo kontakta med objektom in stenami. Silo pri\u0161tejte k silami, ki delujejo na pak. def wall_contact_force ( self , object , object_radius , K ): wall_positions = np . array ([[ 0 , 0 ], [ self . width , 0 ], [ 0 , 0 ], [ 0 , self . height ]]) wall_normals = np . array ([[ 1 , 0 ], [ - 1 , 0 ], [ 0 , 1 ], [ 0 , - 1 ]]) x_pos , y_pos = object . position . x , object . position . y distances = [ np . dot ( wall_normals [ i ], [ x_pos , y_pos ]) - np . dot ( wall_normals [ i ], wall_positions [ i ]) for i in range ( len ( wall_positions ))] # Find the closest wall closest_wall_index = np . argmin ( distances ) closest_wall_normal = wall_normals [ closest_wall_index ] # Calculate the distance between the circle and the closest wall distance_to_wall = distances [ closest_wall_index ] - object_radius force = np . array ([ 0 , 0 ]) # Calculate the interaction force between the circle and the closest wall if distance_to_wall <= 0 : force = - K * distance_to_wall * closest_wall_normal return force Primer, ki ka\u017ee kako je mogo\u010de omejiti gibanje agenta FWa = self . wall_contact_force ( self . agent , self . agent_radius , K ) agent_vel = action + FWa / m * TIME_STEP self . set_agent_velocity ( agent_vel ) Nagrada za uspe\u0161no kon\u010danje epizode: if FW [ 1 ] > 0.0 : #Sila, ki ka\u017ee v -y smeri, se zgodi le ob trku z zgornjo steno reward += 1.0 done = True Za nagrado lahko tudi izra\u010dunate parameter dele\u017e smeri sile proti sredi\u0161\u010du zgornjega roba ter oblikujete nagrado, ki bo pozitivno nagradila trke agenta s pakom proti zgornjemu robu in z negativno nagrado kaznovala trke v napa\u010dno smer. FK_mag = np . linalg . norm ( FK ) F_comp = self . calculate_component ( self . get_puck_position (), np . array ([ self . width / 2 , 0.0 ]), FK / FK_mag ) reward += 0.05 * F_comp Lahko nagradite \u0161e z kaznijo, da se agent izogiba stenam in strelom z odboji paka od sten. if np . linalg . norm ( FWa ) > 0 : reward += - 1.0 / self . time_steps * 10 # kazen za dotike agenta s steno if FW [ 1 ] > 0.0 : reward += 1.0 done = True elif np . linalg . norm ( FW ) > 0.0 : reward += - 1.0 / self . time_steps * 10 # kazen za dotike paka s stenami Za u\u010denje uporabite metodo TQC . from sb3_contrib import TQC policy_kwargs = dict ( n_critics = 2 , n_quantiles = 25 ) model = TQC ( \"MlpPolicy\" , env = env , tensorboard_log = logdir , verbose = 1 , policy_kwargs = policy_kwargs ) Primerjava u\u010denja treh metod : PPO , TRPO in TQC Nagrada skozi \u010das za TQC metodo","title":"Pomik paka v gol"},{"location":"circ_env_pak_rob/#dinamika-agenta-s-pakom","text":"","title":"Dinamika agenta s pakom"},{"location":"circ_env_pak_rob/#popravki-funkcij","text":"__init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height ]), dtype = np . float32 ) _get_obs() return np . concatenate (( agent_pos , pak_pos )) reset() self . reset_agent (( np . random . uniform ( self . agent_radius * 1.5 , self . width - self . agent_radius * 1.5 ), np . random . uniform ( self . agent_radius * 1.5 , self . height - self . agent_radius * 1.5 ))) self . reset_puck (( np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . width - self . object_radius - self . agent_radius * 3 ), np . random . uniform ( self . object_radius + self . agent_radius * 3 , self . height - self . object_radius - self . agent_radius * 3 ))) step() Po spodnjem algoritmu napi\u0161ite kodo za izra\u010dun hitrosti paka. Razlaga algoritma: Nastavite K na 2000 in B na 0.5 Nastavite m na 1 Pridobite polo\u017eaj agenta in ga shranite v pos_a Pridobite polo\u017eaj paka in ga shranite v pos_p Izra\u010dunajte razdaljo med agentom in pakom kot (agent_radius+object_radius) - calc_distance(pos_a, pos_p) in jo shranite v dist Ustvari spremenljivko FK z ni\u010dlami z enakimi dimenzijami kot pos_p \u010ce je dist > 0, potem nastavite smer kot enotski vektor razlike med pos_a in pos_p in nastavite FK kot K krat dist krat smer Pridobite hitrost plo\u0161\u010dka in jo shranite v vel_p Nastavite FB kot -B krat vel_p Ustvari spremenljivko F z ni\u010del z enakimi dimenzijami kot pos_p Ustvari spremenljivko acc z ni\u010dlami, ki ima enake dimenzije kot pos_p Nastavite F kot vsoto FK in FB Nastavite acc kot F, deljeno z m Posodobite hitrost plo\u0161\u010dka tako, da jo nastavite na vel_p plus acc krat TIME_STEP Izra\u010dun trka med krogoma step() Kon\u010danje epizode, ko pote\u010de \u010das: reward = - 1.0 / self . time_steps done = False if self . current_step >= self . time_steps : done = True","title":"Popravki funkcij"},{"location":"circ_env_pak_rob/#dotik-s-pakom","text":"step() V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za pozitivno nagrado reward += 1.0 in kon\u010danje epizode. Psevdokoda : if is_contact_between_agent_and_pack ... reward += 1.0 done = True","title":"Dotik s pakom"},{"location":"circ_env_pak_rob/#odbijanje-paka-izven-okolja","text":"step() V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za majhno pozitivno nagrado reward = 0.1 . if is_contact_between_agent_and_pack ... reward += 0.05 #done = True Definirajte novo funkcijo is_puck_outside_screen in jo dodajte za funkcijo is_agent_outside_screen . def is_puck_outside_screen ( self ): outside = False if ( self . object . position . x < self . object_radius ) or ( self . object . position . x > ( self . width - self . object_radius )): outside = True if ( self . object . position . y < self . object_radius ) or ( self . object . position . y > ( self . height - self . object_radius )): outside = True return outside V step() dodate pogoj za kon\u010danje epizode z pozitivno nagrado: if self . is_puck_outside_screen (): reward = 1.0 done = True Nagrada skozi \u010das s PPO metodo za odboj paka iz okolja","title":"Odbijanje paka izven okolja"},{"location":"circ_env_pak_rob/#streljanje-na-zgornji-rob","text":"Za omejitev okolja uporabite funkcijo self.wall_contact_force(object, object_radius, K) , ki izra\u010duna silo kontakta med objektom in stenami. Silo pri\u0161tejte k silami, ki delujejo na pak. def wall_contact_force ( self , object , object_radius , K ): wall_positions = np . array ([[ 0 , 0 ], [ self . width , 0 ], [ 0 , 0 ], [ 0 , self . height ]]) wall_normals = np . array ([[ 1 , 0 ], [ - 1 , 0 ], [ 0 , 1 ], [ 0 , - 1 ]]) x_pos , y_pos = object . position . x , object . position . y distances = [ np . dot ( wall_normals [ i ], [ x_pos , y_pos ]) - np . dot ( wall_normals [ i ], wall_positions [ i ]) for i in range ( len ( wall_positions ))] # Find the closest wall closest_wall_index = np . argmin ( distances ) closest_wall_normal = wall_normals [ closest_wall_index ] # Calculate the distance between the circle and the closest wall distance_to_wall = distances [ closest_wall_index ] - object_radius force = np . array ([ 0 , 0 ]) # Calculate the interaction force between the circle and the closest wall if distance_to_wall <= 0 : force = - K * distance_to_wall * closest_wall_normal return force Primer, ki ka\u017ee kako je mogo\u010de omejiti gibanje agenta FWa = self . wall_contact_force ( self . agent , self . agent_radius , K ) agent_vel = action + FWa / m * TIME_STEP self . set_agent_velocity ( agent_vel ) Nagrada za uspe\u0161no kon\u010danje epizode: if FW [ 1 ] > 0.0 : #Sila, ki ka\u017ee v -y smeri, se zgodi le ob trku z zgornjo steno reward += 1.0 done = True Za nagrado lahko tudi izra\u010dunate parameter dele\u017e smeri sile proti sredi\u0161\u010du zgornjega roba ter oblikujete nagrado, ki bo pozitivno nagradila trke agenta s pakom proti zgornjemu robu in z negativno nagrado kaznovala trke v napa\u010dno smer. FK_mag = np . linalg . norm ( FK ) F_comp = self . calculate_component ( self . get_puck_position (), np . array ([ self . width / 2 , 0.0 ]), FK / FK_mag ) reward += 0.05 * F_comp Lahko nagradite \u0161e z kaznijo, da se agent izogiba stenam in strelom z odboji paka od sten. if np . linalg . norm ( FWa ) > 0 : reward += - 1.0 / self . time_steps * 10 # kazen za dotike agenta s steno if FW [ 1 ] > 0.0 : reward += 1.0 done = True elif np . linalg . norm ( FW ) > 0.0 : reward += - 1.0 / self . time_steps * 10 # kazen za dotike paka s stenami Za u\u010denje uporabite metodo TQC . from sb3_contrib import TQC policy_kwargs = dict ( n_critics = 2 , n_quantiles = 25 ) model = TQC ( \"MlpPolicy\" , env = env , tensorboard_log = logdir , verbose = 1 , policy_kwargs = policy_kwargs ) Primerjava u\u010denja treh metod : PPO , TRPO in TQC Nagrada skozi \u010das za TQC metodo","title":"Streljanje na zgornji rob"},{"location":"circ_env_pak_tocka/","text":"Dinamika agenta s pakom Popravki funkcij __init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height , self . width , self . height ]), dtype = np . float32 ) _get_obs() return np . concatenate (( agent_pos , target_pos , pak_pos )) step() Po spodnjem algoritmu napi\u0161ite kodo za izra\u010dun hitrosti paka. Razlaga algoritma: Nastavite K na 2000 in B na 0.5 Nastavite m na 1 Pridobite polo\u017eaj agenta in ga shranite v pos_a Pridobite polo\u017eaj paka in ga shranite v pos_p Izra\u010dunajte razdaljo med agentom in pakom kot (agent_radius+object_radius) - calc_distance(pos_a, pos_p) in jo shranite v dist Ustvari spremenljivko FK z ni\u010dlami z enakimi dimenzijami kot pos_p \u010ce je dist > 0, potem nastavite smer kot enotski vektor razlike med pos_a in pos_p in nastavite FK kot K krat dist krat smer Pridobite hitrost plo\u0161\u010dka in jo shranite v vel_p Nastavite FB kot -B krat vel_p Ustvari spremenljivko F z ni\u010del z enakimi dimenzijami kot pos_p Ustvari spremenljivko acc z ni\u010dlami, ki ima enake dimenzije kot pos_p Nastavite F kot vsoto FK in FB Nastavite acc kot F, deljeno z m Posodobite hitrost plo\u0161\u010dka tako, da jo nastavite na vel_p plus acc krat TIME_STEP Izra\u010dun trka med krogoma Koda za nagrado in kon\u010danje epizode: if self . calc_distance ( self . get_puck_position (), self . get_target_position ()) < self . target_radius : reward = 1.0 done = True #if self._is_collision(self.agent, self.object): # reward = 1.0 # done = True Dotik s pakom step() V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za pozitivno nagrado reward = 1.0 in kon\u010danje epizode. if is_contact_between_agent_and_pack ... reward = 1.0 done = True Odbijanje paka izven okolja step() V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za majhno pozitivno nagrado reward = 0.1 . if is_contact_between_agent_and_pack ... reward = 0.1 #done = True if self . is_puck_outside_screen (): reward = 1.0 done = True","title":"Dinamika agenta s pakom"},{"location":"circ_env_pak_tocka/#dinamika-agenta-s-pakom","text":"","title":"Dinamika agenta s pakom"},{"location":"circ_env_pak_tocka/#popravki-funkcij","text":"__init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height , self . width , self . height ]), dtype = np . float32 ) _get_obs() return np . concatenate (( agent_pos , target_pos , pak_pos )) step() Po spodnjem algoritmu napi\u0161ite kodo za izra\u010dun hitrosti paka. Razlaga algoritma: Nastavite K na 2000 in B na 0.5 Nastavite m na 1 Pridobite polo\u017eaj agenta in ga shranite v pos_a Pridobite polo\u017eaj paka in ga shranite v pos_p Izra\u010dunajte razdaljo med agentom in pakom kot (agent_radius+object_radius) - calc_distance(pos_a, pos_p) in jo shranite v dist Ustvari spremenljivko FK z ni\u010dlami z enakimi dimenzijami kot pos_p \u010ce je dist > 0, potem nastavite smer kot enotski vektor razlike med pos_a in pos_p in nastavite FK kot K krat dist krat smer Pridobite hitrost plo\u0161\u010dka in jo shranite v vel_p Nastavite FB kot -B krat vel_p Ustvari spremenljivko F z ni\u010del z enakimi dimenzijami kot pos_p Ustvari spremenljivko acc z ni\u010dlami, ki ima enake dimenzije kot pos_p Nastavite F kot vsoto FK in FB Nastavite acc kot F, deljeno z m Posodobite hitrost plo\u0161\u010dka tako, da jo nastavite na vel_p plus acc krat TIME_STEP Izra\u010dun trka med krogoma Koda za nagrado in kon\u010danje epizode: if self . calc_distance ( self . get_puck_position (), self . get_target_position ()) < self . target_radius : reward = 1.0 done = True #if self._is_collision(self.agent, self.object): # reward = 1.0 # done = True","title":"Popravki funkcij"},{"location":"circ_env_pak_tocka/#dotik-s-pakom","text":"step() V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za pozitivno nagrado reward = 1.0 in kon\u010danje epizode. if is_contact_between_agent_and_pack ... reward = 1.0 done = True","title":"Dotik s pakom"},{"location":"circ_env_pak_tocka/#odbijanje-paka-izven-okolja","text":"step() V pogoj za izra\u010dun sile kontakta med agentom in pakom dodaj kodo za majhno pozitivno nagrado reward = 0.1 . if is_contact_between_agent_and_pack ... reward = 0.1 #done = True if self . is_puck_outside_screen (): reward = 1.0 done = True","title":"Odbijanje paka izven okolja"},{"location":"circ_env_robot/","text":"Povezovanje z robotom Povezovanje z robotom poteka preko UDP komunikacije. Za prikaz spodbujevalnega u\u010denja na dejanskem robotu boste uporabili robota Franka Emika Panda, zaslon, ki je name\u0161\u010den v delovnem prostoru robota, in IR senzorje za zaznavanje predmetov na zaslonu. Zaslon je prekrit s prozorno povr\u0161ino (pleksi steklo) z robovi in deluje kot delovni prostor za igralno okolje. Predmeti so postavljeni na povr\u0161ino iz pleksi stekla, IR-senzorji pa so name\u0161\u010deni ob robu povr\u0161ine iz pleksi stekla. Franka Emika Panda je sodelujo\u010d robotski manipulator s sedmimi stopnjami prostosti, senzorjem navora v vsakem sklepu, dosegom 855 mm in nosilnostjo 3 kg. Na vrhu ima prijemalo, s katerim lahko zgrabi predmete ali orodja. V na\u0161em primeru bomo uporabili 3D-natisnjeno orodje, ki predstavlja agenta. Robota se upravlja s shemo Simulink, ki po\u0161ilja podatke krmilniku robota prek protokola UDP (User Datagram Protocol). Za vizualizacijo okolja se uporablja simulacijski zaslon v programu Unity, pravtako se za po\u0161iljanje podatkov iz sheme Simulink v program Unity uporablja protokol UDP. Zaslon slu\u017ei kot senzor za zaznavanje polo\u017eaja predmetov na njem s pomo\u010djo IR-senzorjev na robovih. Ko predmet prekine \u017earek, lahko sistem zazna in izmeri njegov polo\u017eaj. Arhitektura celotnega sistema je prikazana na spodnji sliki. Agent se izvaja na lo\u010denem ra\u010dunalniku in po\u0161ilja polo\u017eaj agenta (pu\u0161\u010dica \u0161tevilka 1 na sliki 2) na ra\u010dunalnik, na katerem te\u010de model Simulink in vizualizacija Unity. Model Simulink po\u0161ilja podatke za vodenje robota krmilniku robota (pu\u0161\u010dica \u0161tevilka 2). Na zaslonu se prika\u017ee vizualizacija Unity. IR-senzorji zaznajo polo\u017eaj predmeta v delovnem prostoru okolja in ga po\u0161ljejo v ra\u010dunalnik z modelom Simulink in vizualizacijo Unity (pu\u0161\u010dica \u0161tevilka 3). Stanje okolja se nazadnje po\u0161lje nazaj v ra\u010dunalnik, ki izra\u010duna dejanja agenta (pu\u0161\u010dica \u0161tevilka 4). Slika z robotom in zaslonom Orodje Primer premikanja v to\u010dko Primer potiskanja \u017eogice Primer potiskanja naklju\u010dnega objekta Predelana koda za komunikacijo z robotom load_circ_modelUDP.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 import gym import numpy as np from sb3_contrib import TQC #from stable_baselines3 import PPO import os import circ_env import socket import sys import struct env = gym . make ( 'circ_env/Circle-v0' , render_mode = \"human\" ) client_ip = \"192.168.65.194\" port_send = 1026 send_socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) port_rec = 1027 rec_socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) rec_socket . bind (( \"0.0.0.0\" , port_rec )) #rec_socket.settimeout(1.0) # Set timeout to 1 second model = TQC . load ( \"./models/TQC/11111\" , env = env ) print ( \"Model loaded.\" ) # Reset the environment obs = env . reset () EPISODES = 1000 for episode in range ( EPISODES ): print ( \"Episode: \" , episode ) obs = env . reset () done = False total_reward = 0 while not done : #lokacija pucka na plo\u0161\u010di try : # Attempt to receive up to 1024 bytes of data print ( \"Try to receive data.\" ) data , addr = rec_socket . recvfrom ( 8 ) puck_values = struct . unpack ( '2f' , data ) except socket . timeout : # Handle timeout exception print ( \"Timeout occurred while receiving data.\" ) except socket . error : # If no data is received, you get here, but it's not an error # Ignore and continue print ( \"noData\" ) pass #print(puck_values) puck_pos = [ puck_values [ 0 ], puck_values [ 1 ]] print ( \"puck_values\" , puck_values ) #center simulink [0,0] puck_pos [ 0 ] += env . width / 2 puck_pos [ 1 ] += env . height / 2 #pixel to m puck_pos [ 0 ] /= env . PPM puck_pos [ 1 ] /= env . PPM print ( \"puck_pos\" , puck_pos ) env . set_puck_position ([ puck_pos [ 0 ], puck_pos [ 1 ]]) action , _state = model . predict ( obs , deterministic = True ) obs , reward , done , _ = env . step ( action ) sys . stdout . flush () # agent pos., target pos. bin_vals = struct . pack ( '4f' ,( obs [ 0 ] - env . width / 2 ) * env . PPM , ( obs [ 1 ] - env . height / 2 ) * env . PPM , ( obs [ 2 ] - env . width / 2 ) * env . PPM , ( obs [ 3 ] - env . height / 2 ) * env . PPM ) send_socket . sendto ( bin_vals , ( client_ip , port_send )) total_reward = total_reward + reward env . render () print ( \"total reward\" , total_reward ) # Close the environment env . close ()","title":"Povezovanje z robotom"},{"location":"circ_env_robot/#povezovanje-z-robotom","text":"","title":"Povezovanje z robotom"},{"location":"circ_env_robot/#povezovanje-z-robotom-poteka-preko-udp-komunikacije","text":"Za prikaz spodbujevalnega u\u010denja na dejanskem robotu boste uporabili robota Franka Emika Panda, zaslon, ki je name\u0161\u010den v delovnem prostoru robota, in IR senzorje za zaznavanje predmetov na zaslonu. Zaslon je prekrit s prozorno povr\u0161ino (pleksi steklo) z robovi in deluje kot delovni prostor za igralno okolje. Predmeti so postavljeni na povr\u0161ino iz pleksi stekla, IR-senzorji pa so name\u0161\u010deni ob robu povr\u0161ine iz pleksi stekla. Franka Emika Panda je sodelujo\u010d robotski manipulator s sedmimi stopnjami prostosti, senzorjem navora v vsakem sklepu, dosegom 855 mm in nosilnostjo 3 kg. Na vrhu ima prijemalo, s katerim lahko zgrabi predmete ali orodja. V na\u0161em primeru bomo uporabili 3D-natisnjeno orodje, ki predstavlja agenta. Robota se upravlja s shemo Simulink, ki po\u0161ilja podatke krmilniku robota prek protokola UDP (User Datagram Protocol). Za vizualizacijo okolja se uporablja simulacijski zaslon v programu Unity, pravtako se za po\u0161iljanje podatkov iz sheme Simulink v program Unity uporablja protokol UDP. Zaslon slu\u017ei kot senzor za zaznavanje polo\u017eaja predmetov na njem s pomo\u010djo IR-senzorjev na robovih. Ko predmet prekine \u017earek, lahko sistem zazna in izmeri njegov polo\u017eaj. Arhitektura celotnega sistema je prikazana na spodnji sliki. Agent se izvaja na lo\u010denem ra\u010dunalniku in po\u0161ilja polo\u017eaj agenta (pu\u0161\u010dica \u0161tevilka 1 na sliki 2) na ra\u010dunalnik, na katerem te\u010de model Simulink in vizualizacija Unity. Model Simulink po\u0161ilja podatke za vodenje robota krmilniku robota (pu\u0161\u010dica \u0161tevilka 2). Na zaslonu se prika\u017ee vizualizacija Unity. IR-senzorji zaznajo polo\u017eaj predmeta v delovnem prostoru okolja in ga po\u0161ljejo v ra\u010dunalnik z modelom Simulink in vizualizacijo Unity (pu\u0161\u010dica \u0161tevilka 3). Stanje okolja se nazadnje po\u0161lje nazaj v ra\u010dunalnik, ki izra\u010duna dejanja agenta (pu\u0161\u010dica \u0161tevilka 4). Slika z robotom in zaslonom Orodje Primer premikanja v to\u010dko Primer potiskanja \u017eogice Primer potiskanja naklju\u010dnega objekta","title":"Povezovanje z robotom poteka preko UDP komunikacije."},{"location":"circ_env_robot/#predelana-koda-za-komunikacijo-z-robotom","text":"load_circ_modelUDP.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 import gym import numpy as np from sb3_contrib import TQC #from stable_baselines3 import PPO import os import circ_env import socket import sys import struct env = gym . make ( 'circ_env/Circle-v0' , render_mode = \"human\" ) client_ip = \"192.168.65.194\" port_send = 1026 send_socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) port_rec = 1027 rec_socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) rec_socket . bind (( \"0.0.0.0\" , port_rec )) #rec_socket.settimeout(1.0) # Set timeout to 1 second model = TQC . load ( \"./models/TQC/11111\" , env = env ) print ( \"Model loaded.\" ) # Reset the environment obs = env . reset () EPISODES = 1000 for episode in range ( EPISODES ): print ( \"Episode: \" , episode ) obs = env . reset () done = False total_reward = 0 while not done : #lokacija pucka na plo\u0161\u010di try : # Attempt to receive up to 1024 bytes of data print ( \"Try to receive data.\" ) data , addr = rec_socket . recvfrom ( 8 ) puck_values = struct . unpack ( '2f' , data ) except socket . timeout : # Handle timeout exception print ( \"Timeout occurred while receiving data.\" ) except socket . error : # If no data is received, you get here, but it's not an error # Ignore and continue print ( \"noData\" ) pass #print(puck_values) puck_pos = [ puck_values [ 0 ], puck_values [ 1 ]] print ( \"puck_values\" , puck_values ) #center simulink [0,0] puck_pos [ 0 ] += env . width / 2 puck_pos [ 1 ] += env . height / 2 #pixel to m puck_pos [ 0 ] /= env . PPM puck_pos [ 1 ] /= env . PPM print ( \"puck_pos\" , puck_pos ) env . set_puck_position ([ puck_pos [ 0 ], puck_pos [ 1 ]]) action , _state = model . predict ( obs , deterministic = True ) obs , reward , done , _ = env . step ( action ) sys . stdout . flush () # agent pos., target pos. bin_vals = struct . pack ( '4f' ,( obs [ 0 ] - env . width / 2 ) * env . PPM , ( obs [ 1 ] - env . height / 2 ) * env . PPM , ( obs [ 2 ] - env . width / 2 ) * env . PPM , ( obs [ 3 ] - env . height / 2 ) * env . PPM ) send_socket . sendto ( bin_vals , ( client_ip , port_send )) total_reward = total_reward + reward env . render () print ( \"total reward\" , total_reward ) # Close the environment env . close ()","title":"Predelana koda za komunikacijo z robotom"},{"location":"circ_env_slo/","text":"Okolje Circle Envrionment Slede\u010da Python koda definira novo okolje za orodje OpenAI Gym. Okolje je pravokotno, agent v obliki kroga pa se lahko giblje v njem. Agent se mora izogibati trkom s steno oziroma mejo okolja. Gibanje agenta in trki v okolju so simulirani z uporabo knji\u017enic Pybox2D in prikazani s knji\u017enjico Pygame. Okolje je definirano kot Python razred CircleEnvironment , ki deduje iz razreda gym.Env . Razred definira ve\u010d funkcij, vklju\u010dno s funkcijami __init__() , reset() , step() , render() in close() , ki so del API okolja OpenAI Gym. Uporabljene funkcije The code imports the following libraries: gym : odprtokodno orodje za razvoj in primerjavo algoritmov za spodbujevalno u\u010denje. OpenAI Gym je zbirka orodij za razvoj in testiranje algoritmov za spodbujevalno u\u010denje. Zagotavlja nabor okolij za testiranje in u\u010denje agentov za spodbujevalno u\u010denje ter standardizirane vmesnike za interakcijo z okolji. numpy : Python knji\u017enica za numeri\u010dne izra\u010dune. math : knji\u017enica z matemati\u010dnimi funkcijami. pygame : knji\u017enica za razvoj iger v Pythonu. Pygame je med-platformni nabor modulov Python, zasnovan za pisanje iger. Vklju\u010duje funkcije za upravljanje grafike, zvoka, vhodnih naprav in omre\u017eja. Pygame lahko uporabite za ustvarjanje preprostih 2D iger ali bolj zapletenih iger s fiziko in umetno inteligenco. Box2D : 2D fizikalni pogon za razvoj iger. Box2D je 2D fizikalni pogon za simulacije fizikalnih sistemov. Uporablja se lahko za simulacije v igrah, robotiki, ra\u010dunalni\u0161kem vidu in na drugih podro\u010djih. Zagotavlja realisti\u010dno simulacijo trkov, sil, trenja in drugih fizikalnih interakcij med predmeti v 2D-prostoru. Spremenljivke Okolje uporablja naslednje spremenljivke: PPM : \u0161tevilo slikovnih pik na meter, uporabljenih za prikaz okolja. TARGET_FPS : hitrost osve\u017eevanja slike, ki se uporablja za prikaz okolja. TIME_STEP : dol\u017eina trajanja posameznega \u010dasovnega koraka simulacije. WORLD_WIDTH : \u0161irina okolja v Box2D simulacijskih enotah [ m ]. WORLD_HEIGHT : vi\u0161ina okolja v Box2D simulacijskih enotah [ m ]. metadata : slovar ( ang. dictionary ), ki vsebuje metapodatke o okolju, vklju\u010dno s razpolo\u017eljivimi na\u010dini prikazovanje okolja in hitrostjo osve\u017eevanja slike. contact_listener : an instance of a custom contact listener class that is used to detect collisions between the agent and the environment. Uporabljene standardne gym funkcije Okolje vsebuje naslednje funkcije: __init__() : funkcija inicializacije okolja. Nastavi parametre okolja, inicializira svet ( ang. world ) Pybox2D in nastavi dimenzije za stanja ( observation space ) in akcije ( action space ). _get_obs() : funkcija, ki vrne trenutno stanje agenta oziroma okolja, tj. njegovo pozicijo v svetu. reset() : funkcija, ki ponastavi okolje na njegovo za\u010detno stanje in vrne za\u010detno stanje. step() : funkcija, ki kot vhod sprejme akcijo, simulira okolje za en korak \u010dasa in vrne novo stanje, nagrado, zastavico done in dodatne informacije o simulaciji. render() : funkcija, ki prika\u017ee okolje z uporabo izbranega na\u010dina prikaza. _render_frame() : funkcija, ki prika\u017ee posamezno sliko okolja. close() : funkcija, ki pobri\u0161e svet Pybox2D in agenta ter zapre okno Pygame. Dodatne funkcije _is_collision(object, goal) : Checks if there is a collision between the body object and the body goal. It takes in two b2Body objects, object and goal, and returns a boolean indicating whether there is a collision. It can take any two Box2D b2Body objects to check collison between them and not just body object and the body goal. create_agent(radius_px) : Creates a new b2Body object representing the agent. It takes in the radius of the agent in pixels, radius_px . create_puck(radius_px, type) : Creates a new b2Body object representing the puck. It takes in the radius of the puck in pixels, radius_px , and a string type that indicates whether the puck should be a dynamic object ('d') or a kinematic object ('k'). create_goal(dim_px, position) : Creates a new b2Body object representing the goal. It takes in a tuple dim_px representing the dimensions of the goal in pixels, and a tuple position representing the position of the goal in the simulation. create_circ_target(radius_px) : Creates a new circular target. It takes in the radius of the target in pixels, radius_px. create_random_object(radius_px) : Creates a new random object with a random shape. It takes in the radius of the object in pixels, radius_px.. create_border() : Creates the boundary of the simulation. get_agent_position() : Returns the current position of the agent as a numpy array. get_puck_position() : Returns the current position of the puck as a numpy array. get_puck_velocity() : Returns the current velocity of the puck as a numpy array. get_target_position() : Returns the current position of the target as a numpy array. reset_agent(position) : Resets the position and velocity of the agent to a specified position. reset_puck(position) : Resets the position and velocity of the puck to a specified position. reset_target(position) : Resets the position of the target to a specified position. reset_random_object(position) : Resets the position and velocity of the random object to a specified position. set_agent_velocity(vel) : This function sets the linear and angular velocity of the agent in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the agent. set_puck_velocity(vel) : This function sets the linear and angular velocity of the puck in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the puck. limit_puck_velocity(maximal_velocity) : This function limits the velocity of a puck object to the maximum velocity specified by maximal_velocity . The function takes a single input parameter, maximal_velocity , which is a float value representing the maximum velocity allowed. The function does not return any values. draw_agent(color) : This function draws the agent as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the agent is converted to Pygame coordinates before drawing. draw_puck(color) : This function draws the puck as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the puck is converted to Pygame coordinates before drawing. draw_border(color) : This function draws the border of the game as a rectangle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. draw_target(color) : This function draws the target as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the target is converted to Pygame coordinates before drawing. draw_goal(color) : This function draws the goal as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. draw_random_object(color) : This function draws a random object as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position, angle and vertices of the object are converted to Pygame coordinates before drawing. is_agent_outside_screen() : This function checks if the agent is outside the screen or not. It returns True if the agent is outside the screen, otherwise it returns False. calc_distance(pos1, pos2) : This function calculates the Euclidean distance between two points. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. unit_vector(pos1, pos2) : This function calculates the unit vector pointing from pos1 to pos2. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. The function returns the unit vector as a numpy array. calculate_component(pos_agent, pos_target, vel) : This function calculates the component of the velocity vel in the direction of the unit vector pointing from pos_agent to pos_target . The input parameters pos_agent and pos_target are tuples containing the x and y coordinates of the points, while vel is a numpy array representing the velocity vector. The function returns the component of vel as a float value. move_agent_mouse() : This function calculates the movement action of the agent based on the position of the mouse relative to the agent's position. The function returns the action as a numpy array. Koda Hiter pregled kode Ustvarimo agenta, dolo\u010dimo observation_space in action_space . __init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 ]), high = np . array ([ self . width , self . height ]), dtype = np . float32 ) self . action_space = spaces . Box ( low =- 2.0 , high = 2.0 , shape = ( 2 ,), dtype = np . float32 ) self . create_agent ( agent_radius_px ) Potrebujemo vsaj en podatek za observacijo. _get_obs() agent_pos = self . get_agent_position () return agent_pos Ponastavimo agenta v reset() funkciji. reset() self . reset_agent (( np . random . uniform ( self . agent_radius * 1.5 , self . width - self . agent_radius * 1.5 ), np . random . uniform ( self . agent_radius * 1.5 , self . height - self . agent_radius * 1.5 ))) Naredimo korak v step() funkciji. step() Premik agenta za action self . set_agent_velocity ( action ) Dolo\u010dimo obs spremenljivko. obs = self . _get_obs () Dolo\u010dimo, reward in done spremenljivki. Za za\u010detek inicializacija: reward = 0.0 done = False Nato lahko dolo\u010dimo osnovne pogoje: if self . current_step >= self . time_steps : #reward = -1.0 done = True izri\u0161emo agenta _render_frame() self . draw_agent ( green ) Na koncu izbri\u0161emo agenta v close() funkciji close() self . world . DestroyBody ( self . agent ) Za\u017eenemo okolje Za\u017eeni skripto playCirc.py python3 playCirc.py Premikanje agenta s tipkovnico V skripti pove\u010dajte hitrost na maksimalno hitrost 2 . playCirc.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 while True : if any ([ event . type == pygame . QUIT for event in pygame . event . get ()]): break #player controls keys = pygame . key . get_pressed () if keys [ pygame . K_LEFT ]: x = - 1 elif keys [ pygame . K_RIGHT ]: x = 1 else : x = 0 if keys [ pygame . K_UP ]: y = - 1 elif keys [ pygame . K_DOWN ]: y = 1 else : y = 0 action = np . array ([ x , y ], dtype = np . float32 ) #action = env.action_space.sample() #print(action) obs , reward , done , _ = env . step ( action ) # Render the environment env . render () # Check if the episode is finished if done : obs = env . reset () # Close the environment env . close () Dodajmo osnovno mejo __init__() self . create_border () - _render_frame() self . draw_border ( black ) Zaklju\u010dimo episodo, ko se agent dotakne meje. step() if self . _is_collision ( self . agent , self . border ): reward = - 1.0 done = True","title":"Okolje Circle Envrionment"},{"location":"circ_env_slo/#okolje-circle-envrionment","text":"Slede\u010da Python koda definira novo okolje za orodje OpenAI Gym. Okolje je pravokotno, agent v obliki kroga pa se lahko giblje v njem. Agent se mora izogibati trkom s steno oziroma mejo okolja. Gibanje agenta in trki v okolju so simulirani z uporabo knji\u017enic Pybox2D in prikazani s knji\u017enjico Pygame. Okolje je definirano kot Python razred CircleEnvironment , ki deduje iz razreda gym.Env . Razred definira ve\u010d funkcij, vklju\u010dno s funkcijami __init__() , reset() , step() , render() in close() , ki so del API okolja OpenAI Gym.","title":"Okolje Circle Envrionment"},{"location":"circ_env_slo/#uporabljene-funkcije","text":"The code imports the following libraries: gym : odprtokodno orodje za razvoj in primerjavo algoritmov za spodbujevalno u\u010denje. OpenAI Gym je zbirka orodij za razvoj in testiranje algoritmov za spodbujevalno u\u010denje. Zagotavlja nabor okolij za testiranje in u\u010denje agentov za spodbujevalno u\u010denje ter standardizirane vmesnike za interakcijo z okolji. numpy : Python knji\u017enica za numeri\u010dne izra\u010dune. math : knji\u017enica z matemati\u010dnimi funkcijami. pygame : knji\u017enica za razvoj iger v Pythonu. Pygame je med-platformni nabor modulov Python, zasnovan za pisanje iger. Vklju\u010duje funkcije za upravljanje grafike, zvoka, vhodnih naprav in omre\u017eja. Pygame lahko uporabite za ustvarjanje preprostih 2D iger ali bolj zapletenih iger s fiziko in umetno inteligenco. Box2D : 2D fizikalni pogon za razvoj iger. Box2D je 2D fizikalni pogon za simulacije fizikalnih sistemov. Uporablja se lahko za simulacije v igrah, robotiki, ra\u010dunalni\u0161kem vidu in na drugih podro\u010djih. Zagotavlja realisti\u010dno simulacijo trkov, sil, trenja in drugih fizikalnih interakcij med predmeti v 2D-prostoru.","title":"Uporabljene funkcije"},{"location":"circ_env_slo/#spremenljivke","text":"Okolje uporablja naslednje spremenljivke: PPM : \u0161tevilo slikovnih pik na meter, uporabljenih za prikaz okolja. TARGET_FPS : hitrost osve\u017eevanja slike, ki se uporablja za prikaz okolja. TIME_STEP : dol\u017eina trajanja posameznega \u010dasovnega koraka simulacije. WORLD_WIDTH : \u0161irina okolja v Box2D simulacijskih enotah [ m ]. WORLD_HEIGHT : vi\u0161ina okolja v Box2D simulacijskih enotah [ m ]. metadata : slovar ( ang. dictionary ), ki vsebuje metapodatke o okolju, vklju\u010dno s razpolo\u017eljivimi na\u010dini prikazovanje okolja in hitrostjo osve\u017eevanja slike. contact_listener : an instance of a custom contact listener class that is used to detect collisions between the agent and the environment.","title":"Spremenljivke"},{"location":"circ_env_slo/#uporabljene-standardne-gym-funkcije","text":"Okolje vsebuje naslednje funkcije: __init__() : funkcija inicializacije okolja. Nastavi parametre okolja, inicializira svet ( ang. world ) Pybox2D in nastavi dimenzije za stanja ( observation space ) in akcije ( action space ). _get_obs() : funkcija, ki vrne trenutno stanje agenta oziroma okolja, tj. njegovo pozicijo v svetu. reset() : funkcija, ki ponastavi okolje na njegovo za\u010detno stanje in vrne za\u010detno stanje. step() : funkcija, ki kot vhod sprejme akcijo, simulira okolje za en korak \u010dasa in vrne novo stanje, nagrado, zastavico done in dodatne informacije o simulaciji. render() : funkcija, ki prika\u017ee okolje z uporabo izbranega na\u010dina prikaza. _render_frame() : funkcija, ki prika\u017ee posamezno sliko okolja. close() : funkcija, ki pobri\u0161e svet Pybox2D in agenta ter zapre okno Pygame.","title":"Uporabljene standardne gym funkcije"},{"location":"circ_env_slo/#dodatne-funkcije","text":"_is_collision(object, goal) : Checks if there is a collision between the body object and the body goal. It takes in two b2Body objects, object and goal, and returns a boolean indicating whether there is a collision. It can take any two Box2D b2Body objects to check collison between them and not just body object and the body goal. create_agent(radius_px) : Creates a new b2Body object representing the agent. It takes in the radius of the agent in pixels, radius_px . create_puck(radius_px, type) : Creates a new b2Body object representing the puck. It takes in the radius of the puck in pixels, radius_px , and a string type that indicates whether the puck should be a dynamic object ('d') or a kinematic object ('k'). create_goal(dim_px, position) : Creates a new b2Body object representing the goal. It takes in a tuple dim_px representing the dimensions of the goal in pixels, and a tuple position representing the position of the goal in the simulation. create_circ_target(radius_px) : Creates a new circular target. It takes in the radius of the target in pixels, radius_px. create_random_object(radius_px) : Creates a new random object with a random shape. It takes in the radius of the object in pixels, radius_px.. create_border() : Creates the boundary of the simulation. get_agent_position() : Returns the current position of the agent as a numpy array. get_puck_position() : Returns the current position of the puck as a numpy array. get_puck_velocity() : Returns the current velocity of the puck as a numpy array. get_target_position() : Returns the current position of the target as a numpy array. reset_agent(position) : Resets the position and velocity of the agent to a specified position. reset_puck(position) : Resets the position and velocity of the puck to a specified position. reset_target(position) : Resets the position of the target to a specified position. reset_random_object(position) : Resets the position and velocity of the random object to a specified position. set_agent_velocity(vel) : This function sets the linear and angular velocity of the agent in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the agent. set_puck_velocity(vel) : This function sets the linear and angular velocity of the puck in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the puck. limit_puck_velocity(maximal_velocity) : This function limits the velocity of a puck object to the maximum velocity specified by maximal_velocity . The function takes a single input parameter, maximal_velocity , which is a float value representing the maximum velocity allowed. The function does not return any values. draw_agent(color) : This function draws the agent as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the agent is converted to Pygame coordinates before drawing. draw_puck(color) : This function draws the puck as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the puck is converted to Pygame coordinates before drawing. draw_border(color) : This function draws the border of the game as a rectangle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. draw_target(color) : This function draws the target as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the target is converted to Pygame coordinates before drawing. draw_goal(color) : This function draws the goal as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. draw_random_object(color) : This function draws a random object as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position, angle and vertices of the object are converted to Pygame coordinates before drawing. is_agent_outside_screen() : This function checks if the agent is outside the screen or not. It returns True if the agent is outside the screen, otherwise it returns False. calc_distance(pos1, pos2) : This function calculates the Euclidean distance between two points. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. unit_vector(pos1, pos2) : This function calculates the unit vector pointing from pos1 to pos2. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. The function returns the unit vector as a numpy array. calculate_component(pos_agent, pos_target, vel) : This function calculates the component of the velocity vel in the direction of the unit vector pointing from pos_agent to pos_target . The input parameters pos_agent and pos_target are tuples containing the x and y coordinates of the points, while vel is a numpy array representing the velocity vector. The function returns the component of vel as a float value. move_agent_mouse() : This function calculates the movement action of the agent based on the position of the mouse relative to the agent's position. The function returns the action as a numpy array.","title":"Dodatne funkcije"},{"location":"circ_env_slo/#koda","text":"","title":"Koda"},{"location":"circ_env_slo/#hiter-pregled-kode","text":"","title":"Hiter pregled kode"},{"location":"circ_env_slo/#ustvarimo-agenta-dolocimo-observation_space-in-action_space","text":"__init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 ]), high = np . array ([ self . width , self . height ]), dtype = np . float32 ) self . action_space = spaces . Box ( low =- 2.0 , high = 2.0 , shape = ( 2 ,), dtype = np . float32 ) self . create_agent ( agent_radius_px )","title":"Ustvarimo agenta, dolo\u010dimo observation_space in action_space."},{"location":"circ_env_slo/#potrebujemo-vsaj-en-podatek-za-observacijo","text":"_get_obs() agent_pos = self . get_agent_position () return agent_pos","title":"Potrebujemo vsaj en podatek za observacijo."},{"location":"circ_env_slo/#ponastavimo-agenta-v-reset-funkciji","text":"reset() self . reset_agent (( np . random . uniform ( self . agent_radius * 1.5 , self . width - self . agent_radius * 1.5 ), np . random . uniform ( self . agent_radius * 1.5 , self . height - self . agent_radius * 1.5 )))","title":"Ponastavimo agenta v reset() funkciji."},{"location":"circ_env_slo/#naredimo-korak-v-step-funkciji","text":"step() Premik agenta za action self . set_agent_velocity ( action ) Dolo\u010dimo obs spremenljivko. obs = self . _get_obs () Dolo\u010dimo, reward in done spremenljivki. Za za\u010detek inicializacija: reward = 0.0 done = False Nato lahko dolo\u010dimo osnovne pogoje: if self . current_step >= self . time_steps : #reward = -1.0 done = True","title":"Naredimo korak v step() funkciji."},{"location":"circ_env_slo/#izrisemo-agenta","text":"_render_frame() self . draw_agent ( green )","title":"izri\u0161emo agenta"},{"location":"circ_env_slo/#na-koncu-izbrisemo-agenta-v-close-funkciji","text":"close() self . world . DestroyBody ( self . agent )","title":"Na koncu izbri\u0161emo agenta v close() funkciji"},{"location":"circ_env_slo/#zazenemo-okolje","text":"Za\u017eeni skripto playCirc.py python3 playCirc.py","title":"Za\u017eenemo okolje"},{"location":"circ_env_slo/#premikanje-agenta-s-tipkovnico","text":"V skripti pove\u010dajte hitrost na maksimalno hitrost 2 . playCirc.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 while True : if any ([ event . type == pygame . QUIT for event in pygame . event . get ()]): break #player controls keys = pygame . key . get_pressed () if keys [ pygame . K_LEFT ]: x = - 1 elif keys [ pygame . K_RIGHT ]: x = 1 else : x = 0 if keys [ pygame . K_UP ]: y = - 1 elif keys [ pygame . K_DOWN ]: y = 1 else : y = 0 action = np . array ([ x , y ], dtype = np . float32 ) #action = env.action_space.sample() #print(action) obs , reward , done , _ = env . step ( action ) # Render the environment env . render () # Check if the episode is finished if done : obs = env . reset () # Close the environment env . close ()","title":"Premikanje agenta s tipkovnico"},{"location":"circ_env_slo/#dodajmo-osnovno-mejo","text":"__init__() self . create_border () - _render_frame() self . draw_border ( black ) Zaklju\u010dimo episodo, ko se agent dotakne meje. step() if self . _is_collision ( self . agent , self . border ): reward = - 1.0 done = True","title":"Dodajmo osnovno mejo"},{"location":"circ_env_tocka/","text":"Premik agenta v ciljno to\u010dko Koda posameznih funkcij v circle_world.py __init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height ]), dtype = np . float32 ) target_radius_px = 30 self . create_circ_target ( target_radius_px ) _get_obs() agent_pos = self . get_agent_position () target_pos = self . get_target_position () return np . concatenate (( agent_pos , target_pos )) reset() self . reset_target (( np . random . uniform ( self . agent_radius * 2 , self . width - self . agent_radius * 2 ), np . random . uniform ( self . agent_radius * 2 , self . height - self . agent_radius * 2 ))) step() if self . calc_distance ( self . get_agent_position (), self . get_target_position ()) < self . target_radius : reward = 1.0 done = True if self . current_step >= self . time_steps : reward = - 1.0 done = True _render_frame() self . draw_target ( blue ) Nastavitve u\u010denja Izhodi\u0161\u010dne vrednosti za u\u010denje: model = PPO ( 'MlpPolicy' , env = env , tensorboard_log = logdir , verbose = 1 , n_steps = 512 , batch_size = 256 , gae_lambda = 0.9 , gamma = 0.99 , n_epochs = 5 , ent_coef = 0.0 , learning_rate = 2.5e-4 , clip_range = 0.3 , seed = 2 ) Primer izpisa uspe\u0161nega u\u010denja s PPO metodo: ----------------------------------------- | rollout/ | | | ep_len_mean | 54.9 | | ep_rew_mean | 1 | | time/ | | | fps | 2638 | | iterations | 8 | | time_elapsed | 1 | | total_timesteps | 577536 | | train/ | | | approx_kl | 0.012461845 | | clip_fraction | 0.052 | | clip_range | 0.3 | | entropy_loss | -2 | | explained_variance | 0.993 | | learning_rate | 0.00025 | | loss | -0.00576 | | n_updates | 5635 | | policy_gradient_loss | -0.00652 | | std | 0.66 | | value_loss | 0.000254 | ----------------------------------------- Potek u\u010denja za tar\u010do velikosti 30 px Potek u\u010denja za tar\u010do velikosti 15 px","title":"Premik v to\u010dko"},{"location":"circ_env_tocka/#premik-agenta-v-ciljno-tocko","text":"","title":"Premik agenta v ciljno to\u010dko"},{"location":"circ_env_tocka/#koda-posameznih-funkcij-v-circle_worldpy","text":"__init__() self . observation_space = spaces . Box ( low = np . array ([ 0.0 , 0.0 , 0.0 , 0.0 ]), high = np . array ([ self . width , self . height , self . width , self . height ]), dtype = np . float32 ) target_radius_px = 30 self . create_circ_target ( target_radius_px ) _get_obs() agent_pos = self . get_agent_position () target_pos = self . get_target_position () return np . concatenate (( agent_pos , target_pos )) reset() self . reset_target (( np . random . uniform ( self . agent_radius * 2 , self . width - self . agent_radius * 2 ), np . random . uniform ( self . agent_radius * 2 , self . height - self . agent_radius * 2 ))) step() if self . calc_distance ( self . get_agent_position (), self . get_target_position ()) < self . target_radius : reward = 1.0 done = True if self . current_step >= self . time_steps : reward = - 1.0 done = True _render_frame() self . draw_target ( blue )","title":"Koda posameznih funkcij v circle_world.py"},{"location":"circ_env_tocka/#nastavitve-ucenja","text":"Izhodi\u0161\u010dne vrednosti za u\u010denje: model = PPO ( 'MlpPolicy' , env = env , tensorboard_log = logdir , verbose = 1 , n_steps = 512 , batch_size = 256 , gae_lambda = 0.9 , gamma = 0.99 , n_epochs = 5 , ent_coef = 0.0 , learning_rate = 2.5e-4 , clip_range = 0.3 , seed = 2 ) Primer izpisa uspe\u0161nega u\u010denja s PPO metodo: ----------------------------------------- | rollout/ | | | ep_len_mean | 54.9 | | ep_rew_mean | 1 | | time/ | | | fps | 2638 | | iterations | 8 | | time_elapsed | 1 | | total_timesteps | 577536 | | train/ | | | approx_kl | 0.012461845 | | clip_fraction | 0.052 | | clip_range | 0.3 | | entropy_loss | -2 | | explained_variance | 0.993 | | learning_rate | 0.00025 | | loss | -0.00576 | | n_updates | 5635 | | policy_gradient_loss | -0.00652 | | std | 0.66 | | value_loss | 0.000254 | ----------------------------------------- Potek u\u010denja za tar\u010do velikosti 30 px Potek u\u010denja za tar\u010do velikosti 15 px","title":"Nastavitve u\u010denja"},{"location":"circ_env_train/","text":"U\u010denje s spodbujevalnim u\u010denjem Skripta za u\u010denje train_circ.py train_circ.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import gym import numpy as np from sb3_contrib import TQC #from stable_baselines3 import PPO import os import circ_env env = gym . make ( 'circ_env/Circle-v0' ) # tensorboard logiranje models_dir = \"models/TQC01\" if not os . path . exists ( models_dir ): os . makedirs ( models_dir ) logdir = \"logs\" if not os . path . exists ( logdir ): os . makedirs ( logdir ) # deklaracija modela policy_kwargs = dict ( n_critics = 2 , n_quantiles = 25 ) model = TQC ( 'MlpPolicy' , env = env , tensorboard_log = logdir , verbose = 1 , policy_kwargs = policy_kwargs ) # Reset the environment obs = env . reset () # iteracija skozi u\u010denje in shranjevanje modela TIMESTEPS = 10000 iters = 0 while True : iters += 1 model . learn ( total_timesteps = TIMESTEPS , reset_num_timesteps = False , tb_log_name = \"TQC_01\" ) model . save ( f \" { models_dir } / { TIMESTEPS * iters } \" ) Zagon u\u010denja in logiranja Terminal 1 python3 train_circ.py Terminal 2 tensorboard --logdir=\"logs\" Zagon nau\u010denega agenta load_circ_model.py load_circ_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import gym import numpy as np from sb3_contrib import TQC #from stable_baselines3 import PPO import os import circ_env env = gym . make ( 'circ_env/Circle-v0' , render_mode = \"human\" ) model = TQC . load ( \"./models/TQC07/2260000\" , env = env ) # Reset the environment obs = env . reset () EPISODES = 1000 for episode in range ( EPISODES ): obs = env . reset () done = False while not done : action , _state = model . predict ( obs , deterministic = True ) obs , reward , done , _ = env . step ( action ) env . render () # Close the environment env . close () Nadgrajena skripta za u\u010denje train_circ.py import gym import numpy as np from sb3_contrib import TQC from stable_baselines3 import PPO import os import shutil from stable_baselines3.common.evaluation import evaluate_policy import csv from torch import nn import circ_env # function to copy text between two strings from a file to another file def copy_text_between_strings ( env_file_path , string1 , string2 , info_file_path ): with open ( env_file_path , \"r\" ) as file : content = file . read () start_index = content . find ( string1 ) + len ( string1 ) end_index = content . find ( string2 ) if start_index == - 1 or end_index == - 1 : print ( \"Either string1 or string2 not found in the file.\" ) return extracted_text = content [ start_index : end_index ] with open ( info_file_path , \"w\" ) as reward_file : reward_file . write ( extracted_text ) print ( \"Text copied to successfully.\" ) # create the environment using gym env = gym . make ( 'circ_env/Circle-v0' ) # set up directories for saving models and logging results method_dir_name = \"TQC01\" parent_dir = \"models\" models_dir = os . path . join ( parent_dir , method_dir_name ) if not os . path . exists ( models_dir ): os . makedirs ( models_dir ) # set up logging using TensorBoard logdir = \"logs\" if not os . path . exists ( logdir ): os . makedirs ( logdir ) # copy training script and environment file to models directory filename = \"train_circ.py\" shutil . copy ( filename , os . path . join ( models_dir , os . path . basename ( filename ))) filename = \"/home/student/RL/rl_pet/CircEnv/circ_env/envs/circle_world.py\" shutil . copy ( filename , os . path . join ( models_dir , os . path . basename ( filename ))) # extract reward function from environment file and save to separate file copy_text_between_strings ( \"/home/student/RL/rl_pet/CircEnv/circ_env/envs/circle_world.py\" , \"# Reward function start\" , \"# Reward function stop\" , f \" { models_dir } /rewards.py\" ) # create CSV files to store training statistics with open ( f \" { models_dir } /rewards.csv\" , 'a' , newline = '' ) as csvfile : writer = csv . writer ( csvfile ) writer . writerow ([ 'step' , 'mean reward' , 'std reward' ]) with open ( f \" { models_dir } /best_rewards.csv\" , 'a' , newline = '' ) as csvfile : writer = csv . writer ( csvfile ) writer . writerow ([ 'step' , 'mean reward' , 'std reward' ]) # set up policy hyperparameters for TQC algorithm policy_kwargs = dict ( n_critics = 2 , n_quantiles = 25 , activation_fn = nn . Tanh , net_arch = [ 256 , 256 ] ) # initialize TQC algorithm with the defined hyperparameters model = TQC ( \"MlpPolicy\" , env = env , tensorboard_log = logdir , verbose = 1 , policy_kwargs = policy_kwargs ) # Reset the environment obs = env . reset () # train the model in a loop for a fixed number of timesteps best_reward = - 10000 TIMESTEPS = 5000 iters = 0 while True : iters += 1 model . learn ( total_timesteps = TIMESTEPS , reset_num_timesteps = False , tb_log_name = method_dir_name ) mean_reward , std_reward = evaluate_policy ( model , model . get_env (), n_eval_episodes = 20 ) # save mean and standard deviation of rewards to CSV file with open ( f \" { models_dir } /rewards.csv\" , 'a' , newline = '' ) as csvfile : writer = csv . writer ( csvfile ) writer . writerow ([ TIMESTEPS * iters , mean_reward , std_reward ]) if ( best_reward > 0 and mean_reward > best_reward * 0.7 ) or \\ ( best_reward <= 0 and mean_reward * 0.7 > best_reward ): model . save ( f \" { models_dir } / { TIMESTEPS * iters } \" ) with open ( f \" { models_dir } /best_rewards.csv\" , 'a' , newline = '' ) as csvfile : writer = csv . writer ( csvfile ) writer . writerow ([ TIMESTEPS * iters , mean_reward , std_reward ]) if mean_reward > best_reward : best_reward = mean_reward Funkcija za izris nagrade # Uvoz potrebnih knji\u017enic import pandas as pd import matplotlib.pyplot as plt import os # Ime mape za trenutno metodo u\u010denja method_dir_name = \"TQC01\" # Ime glavne mape za shranjevanje modelov parent_dir = \"models\" # Ime mape za trenutno metodo u\u010denja znotraj glavne mape models_dir = os . path . join ( parent_dir , method_dir_name ) # Ime CSV datoteke z rezultati csv_file = os . path . join ( models_dir , 'rewards.csv' ) # Nalaganje podatkov iz CSV datoteke v pandas DataFrame data = pd . read_csv ( csv_file ) # Ustvarjanje grafa srednjo vrednostjo nagrade in pasovi standardne deviacije za nagrado plt . plot ( data [ 'step' ], data [ 'mean reward' ], color = 'blue' ) plt . fill_between ( data [ 'step' ], data [ 'mean reward' ] - data [ 'std reward' ], data [ 'mean reward' ] + data [ 'std reward' ], alpha = 0.2 , color = 'blue' ) plt . xlabel ( 'Time Steps' ) plt . ylabel ( 'Mean Reward' ) plt . title ( 'Reward vs Time Steps' ) plt . show ()","title":"U\u010denje"},{"location":"circ_env_train/#ucenje-s-spodbujevalnim-ucenjem","text":"","title":"U\u010denje s spodbujevalnim u\u010denjem"},{"location":"circ_env_train/#skripta-za-ucenje-train_circpy","text":"train_circ.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import gym import numpy as np from sb3_contrib import TQC #from stable_baselines3 import PPO import os import circ_env env = gym . make ( 'circ_env/Circle-v0' ) # tensorboard logiranje models_dir = \"models/TQC01\" if not os . path . exists ( models_dir ): os . makedirs ( models_dir ) logdir = \"logs\" if not os . path . exists ( logdir ): os . makedirs ( logdir ) # deklaracija modela policy_kwargs = dict ( n_critics = 2 , n_quantiles = 25 ) model = TQC ( 'MlpPolicy' , env = env , tensorboard_log = logdir , verbose = 1 , policy_kwargs = policy_kwargs ) # Reset the environment obs = env . reset () # iteracija skozi u\u010denje in shranjevanje modela TIMESTEPS = 10000 iters = 0 while True : iters += 1 model . learn ( total_timesteps = TIMESTEPS , reset_num_timesteps = False , tb_log_name = \"TQC_01\" ) model . save ( f \" { models_dir } / { TIMESTEPS * iters } \" )","title":"Skripta za u\u010denje train_circ.py"},{"location":"circ_env_train/#zagon-ucenja-in-logiranja","text":"Terminal 1 python3 train_circ.py Terminal 2 tensorboard --logdir=\"logs\"","title":"Zagon u\u010denja in logiranja"},{"location":"circ_env_train/#zagon-naucenega-agenta-load_circ_modelpy","text":"load_circ_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import gym import numpy as np from sb3_contrib import TQC #from stable_baselines3 import PPO import os import circ_env env = gym . make ( 'circ_env/Circle-v0' , render_mode = \"human\" ) model = TQC . load ( \"./models/TQC07/2260000\" , env = env ) # Reset the environment obs = env . reset () EPISODES = 1000 for episode in range ( EPISODES ): obs = env . reset () done = False while not done : action , _state = model . predict ( obs , deterministic = True ) obs , reward , done , _ = env . step ( action ) env . render () # Close the environment env . close ()","title":"Zagon nau\u010denega agenta load_circ_model.py"},{"location":"circ_env_train/#nadgrajena-skripta-za-ucenje-train_circpy","text":"import gym import numpy as np from sb3_contrib import TQC from stable_baselines3 import PPO import os import shutil from stable_baselines3.common.evaluation import evaluate_policy import csv from torch import nn import circ_env # function to copy text between two strings from a file to another file def copy_text_between_strings ( env_file_path , string1 , string2 , info_file_path ): with open ( env_file_path , \"r\" ) as file : content = file . read () start_index = content . find ( string1 ) + len ( string1 ) end_index = content . find ( string2 ) if start_index == - 1 or end_index == - 1 : print ( \"Either string1 or string2 not found in the file.\" ) return extracted_text = content [ start_index : end_index ] with open ( info_file_path , \"w\" ) as reward_file : reward_file . write ( extracted_text ) print ( \"Text copied to successfully.\" ) # create the environment using gym env = gym . make ( 'circ_env/Circle-v0' ) # set up directories for saving models and logging results method_dir_name = \"TQC01\" parent_dir = \"models\" models_dir = os . path . join ( parent_dir , method_dir_name ) if not os . path . exists ( models_dir ): os . makedirs ( models_dir ) # set up logging using TensorBoard logdir = \"logs\" if not os . path . exists ( logdir ): os . makedirs ( logdir ) # copy training script and environment file to models directory filename = \"train_circ.py\" shutil . copy ( filename , os . path . join ( models_dir , os . path . basename ( filename ))) filename = \"/home/student/RL/rl_pet/CircEnv/circ_env/envs/circle_world.py\" shutil . copy ( filename , os . path . join ( models_dir , os . path . basename ( filename ))) # extract reward function from environment file and save to separate file copy_text_between_strings ( \"/home/student/RL/rl_pet/CircEnv/circ_env/envs/circle_world.py\" , \"# Reward function start\" , \"# Reward function stop\" , f \" { models_dir } /rewards.py\" ) # create CSV files to store training statistics with open ( f \" { models_dir } /rewards.csv\" , 'a' , newline = '' ) as csvfile : writer = csv . writer ( csvfile ) writer . writerow ([ 'step' , 'mean reward' , 'std reward' ]) with open ( f \" { models_dir } /best_rewards.csv\" , 'a' , newline = '' ) as csvfile : writer = csv . writer ( csvfile ) writer . writerow ([ 'step' , 'mean reward' , 'std reward' ]) # set up policy hyperparameters for TQC algorithm policy_kwargs = dict ( n_critics = 2 , n_quantiles = 25 , activation_fn = nn . Tanh , net_arch = [ 256 , 256 ] ) # initialize TQC algorithm with the defined hyperparameters model = TQC ( \"MlpPolicy\" , env = env , tensorboard_log = logdir , verbose = 1 , policy_kwargs = policy_kwargs ) # Reset the environment obs = env . reset () # train the model in a loop for a fixed number of timesteps best_reward = - 10000 TIMESTEPS = 5000 iters = 0 while True : iters += 1 model . learn ( total_timesteps = TIMESTEPS , reset_num_timesteps = False , tb_log_name = method_dir_name ) mean_reward , std_reward = evaluate_policy ( model , model . get_env (), n_eval_episodes = 20 ) # save mean and standard deviation of rewards to CSV file with open ( f \" { models_dir } /rewards.csv\" , 'a' , newline = '' ) as csvfile : writer = csv . writer ( csvfile ) writer . writerow ([ TIMESTEPS * iters , mean_reward , std_reward ]) if ( best_reward > 0 and mean_reward > best_reward * 0.7 ) or \\ ( best_reward <= 0 and mean_reward * 0.7 > best_reward ): model . save ( f \" { models_dir } / { TIMESTEPS * iters } \" ) with open ( f \" { models_dir } /best_rewards.csv\" , 'a' , newline = '' ) as csvfile : writer = csv . writer ( csvfile ) writer . writerow ([ TIMESTEPS * iters , mean_reward , std_reward ]) if mean_reward > best_reward : best_reward = mean_reward","title":"Nadgrajena skripta za u\u010denje train_circ.py"},{"location":"circ_env_train/#funkcija-za-izris-nagrade","text":"# Uvoz potrebnih knji\u017enic import pandas as pd import matplotlib.pyplot as plt import os # Ime mape za trenutno metodo u\u010denja method_dir_name = \"TQC01\" # Ime glavne mape za shranjevanje modelov parent_dir = \"models\" # Ime mape za trenutno metodo u\u010denja znotraj glavne mape models_dir = os . path . join ( parent_dir , method_dir_name ) # Ime CSV datoteke z rezultati csv_file = os . path . join ( models_dir , 'rewards.csv' ) # Nalaganje podatkov iz CSV datoteke v pandas DataFrame data = pd . read_csv ( csv_file ) # Ustvarjanje grafa srednjo vrednostjo nagrade in pasovi standardne deviacije za nagrado plt . plot ( data [ 'step' ], data [ 'mean reward' ], color = 'blue' ) plt . fill_between ( data [ 'step' ], data [ 'mean reward' ] - data [ 'std reward' ], data [ 'mean reward' ] + data [ 'std reward' ], alpha = 0.2 , color = 'blue' ) plt . xlabel ( 'Time Steps' ) plt . ylabel ( 'Mean Reward' ) plt . title ( 'Reward vs Time Steps' ) plt . show ()","title":"Funkcija za izris nagrade"},{"location":"cliff/","text":"Primer okolja Cliff Walking Open AI Cliff Walking Za\u010dnemo z novo python skripto Uvoz potrebnih paketov import gym import numpy as np import random from time import sleep Deklaracija okolja your_env = gym.make(\"YourEnv \", some_kwarg=your_vars) your_env = gym.make(\"YourEnv\") Seznam okolij https://gymnasium.farama.org/environments/toy_text/ env = gym . make ( 'CliffWalking-v0' ) Resetiramo in prika\u017eemo okolje env . reset () env . render () Prika\u017eemo nekaj parametrov print ( \"Observation space = \" , env . observation_space . n ) print ( \"Actions = \" , env . action_space . n ) Deklariramo Q tabelo q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) #q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n]) print ( \"Q table size = \" , q_table . shape ) Deklariramo parametre learning_rate = 0.1 discount_factor = 0.95 epochs = 60000 epsilon = 1 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = epochs // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) SHOW_EVERY = 1000 Za\u010dnemo z u\u010denjem z izbranim \u0161tevilom epoh for episode in range ( epochs ): state = env . reset () done = False trial_length = 0 Izvedemo posamezni \u201esprehod \u010dez okolje\u201c z izbiro akcij raziskovanje: naklju\u010dna akcija uporabo zbranega znanja: akcija z maximalno q vrednostjo while not done : if ( random . uniform ( 0 , 1 ) < epsilon ): # Exploration with random action action = env . action_space . sample () else : # Use the action with the highest q-value action = np . argmax ( q_table [ state ]) Izvedemo akcijo v okolju in preberemo vrednosti novega stanja in nagrade next_state , reward , done , info = env . step ( action ) Posodobimo stanje Q tabele curr_q = q_table [ state , action ] next_max_q = np . max ( q_table [ next_state ]) new_q = ( 1 - learning_rate ) * curr_q + learning_rate * ( reward + discount_factor * next_max_q ) q_table [ state , action ] = new_q Posodobimo stanje state = next_state Shranimo dol\u017eino trenutnega \u201esprehoda\u201c if episode % SHOW_EVERY == 0 : trial_length += 1 Celotna koda u\u010denja do sedaj for episode in range ( epochs ): state = env . reset () done = False trial_length = 0 while not done : if ( random . uniform ( 0 , 1 ) < epsilon ): # Exploration with random action action = env . action_space . sample () else : # Use the action with the highest q-value action = np . argmax ( q_table [ state ]) next_state , reward , done , info = env . step ( action ) curr_q = q_table [ state , action ] next_max_q = np . max ( q_table [ next_state ]) new_q = ( 1 - learning_rate ) * curr_q + learning_rate * ( reward + discount_factor * next_max_q ) q_table [ state , action ] = new_q state = next_state if episode % SHOW_EVERY == 0 : trial_length += 1 if episode % SHOW_EVERY == 0 : print ( f 'Episode: { episode : >5d } , episode length: { int ( trial_length ) : >5d } ' ) if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value Testiramo nau\u010deno Q tabelo oziroma agenta Resetiramo okolje in ga izri\u0161emo print ( q_table ) state = env . reset () env . render () done = False trial_length = 0 Izvedemo sprehod while not done : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) trial_length += 1 print ( \" Step \" + str ( trial_length )) env . render () sleep ( .2 ) Ve\u010d testov, da vidimo uspe\u0161nost lengths = [] for trialnum in range ( 1 , 11 ): state = env . reset () done = False trial_length = 0 while not done and trial_length < 25 : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) print ( \"Trial number \" + str ( trialnum ) + \" Step \" + str ( trial_length )) env . render () sleep ( .2 ) trial_length += 1 lengths . append ( trial_length ) sleep ( .2 ) avg_len = sum ( lengths ) / 10 print ( avg_len ) Celotna koda cliff.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 import gym import numpy as np import random from time import sleep env = gym . make ( 'CliffWalking-v0' ) env . reset () env . render () print ( \"Observation space = \" , env . observation_space . n ) print ( \"Actions = \" , env . action_space . n ) q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) #q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n]) print ( \"Q table size = \" , q_table . shape ) learning_rate = 0.5 discount_factor = 0.95 epochs = 45000 epsilon = 1 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = epochs // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) SHOW_EVERY = 1000 for episode in range ( epochs ): state = env . reset () done = False trial_length = 0 while not done : if ( random . uniform ( 0 , 1 ) < epsilon ): # Exploration with random action action = env . action_space . sample () else : # Use the action with the highest q-value action = np . argmax ( q_table [ state ]) next_state , reward , done , info = env . step ( action ) curr_q = q_table [ state , action ] next_max_q = np . max ( q_table [ next_state ]) new_q = ( 1 - learning_rate ) * curr_q + learning_rate * ( reward + discount_factor * next_max_q ) q_table [ state , action ] = new_q state = next_state if episode % SHOW_EVERY == 0 : trial_length += 1 if episode % SHOW_EVERY == 0 : print ( f 'Episode: { episode : >5d } , episode length: { int ( trial_length ) : >5d } ' ) if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value print ( q_table ) state = env . reset () env . render () done = False trial_length = 0 while not done : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) trial_length += 1 print ( \" Step \" + str ( trial_length )) env . render () sleep ( .2 ) Zagon skripte Ob zagonu skripte s python cliff.py se bo pojavila napaka /home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py:44: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar. self._cliff = np.zeros(self.shape, dtype=np.bool) Traceback (most recent call last): File \"/home/student/RL/rl_pet/cliff.py\", line 7, in <module> env = gym.make('CliffWalking-v0') File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make return registry.make(id, **kwargs) File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 129, in make env = spec.make(**kwargs) File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 90, in make env = cls(**_kwargs) File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py\", line 44, in __init__ self._cliff = np.zeros(self.shape, dtype=np.bool) File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/numpy/__init__.py\", line 305, in __getattr__ raise AttributeError(__former_attrs__[attr]) AttributeError: module 'numpy' has no attribute 'bool'. `np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here. The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations Najbolj informativen del so vrstice File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py\", line 44, in __init__ self._cliff = np.zeros(self.shape, dtype=np.bool) in AttributeError: module 'numpy' has no attribute 'bool'. `np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here. The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations Napaka nastane, ker je v cliffwalking.py napaka v vrstici 44, ker se uporablja star zapis za spremenljivko bool in sicer zapis dtype=np.bool . Tega je potrebno spremeniti v samo bool . Potrebno je odpreti datoteko, ki se nahaja npr. v /home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py . Polna pot je odvisna od ime va\u0161ega python okolja, npr. /rl_pet . Odprite python datoteko in spremenite kodo. Stara koda cliffwalking.py 43 44 45 # Cliff Location self . _cliff = np . zeros ( self . shape , dtype = np . bool ) self . _cliff [ 3 , 1 : - 1 ] = True Nova koda cliffwalking.py 43 44 45 # Cliff Location self . _cliff = np . zeros ( self . shape , bool ) self . _cliff [ 3 , 1 : - 1 ] = True Celotna koda verzija 2 V tej verziji kode je dodana pred\u010dasna zaustevative u\u010denja za posamezno epoho , \u010de u\u010denje prese\u017ee 99 korakov u\u010denja v posamezni epohi . cliff.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 import gym import numpy as np import random from time import sleep env = gym . make ( 'CliffWalking-v0' ) env . reset () env . render () print ( \"Observation space = \" , env . observation_space . n ) print ( \"Actions = \" , env . action_space . n ) q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) #q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n]) print ( \"Q table size = \" , q_table . shape ) learning_rate = 0.5 discount_factor = 0.95 epochs = 45000 max_steps = 99 epsilon = 1 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = epochs // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) SHOW_EVERY = 1000 for episode in range ( epochs ): state = env . reset () done = False trial_length = 0 #while not done: for step in range ( max_steps ): if ( random . uniform ( 0 , 1 ) < epsilon ): # Exploration with random action action = env . action_space . sample () else : # Use the action with the highest q-value action = np . argmax ( q_table [ state ]) next_state , reward , done , info = env . step ( action ) curr_q = q_table [ state , action ] next_max_q = np . max ( q_table [ next_state ]) new_q = ( 1 - learning_rate ) * curr_q + learning_rate * ( reward + discount_factor * next_max_q ) q_table [ state , action ] = new_q state = next_state if episode % SHOW_EVERY == 0 : trial_length += 1 if done : break if episode % SHOW_EVERY == 0 : print ( f 'Episode: { episode : >5d } , episode length: { int ( trial_length ) : >5d } ' ) if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value print ( q_table ) state = env . reset () env . render () done = False trial_length = 0 while not done : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) trial_length += 1 print ( \" Step \" + str ( trial_length )) env . render () sleep ( .2 )","title":"Primer okolja *Cliff Walking*"},{"location":"cliff/#primer-okolja-cliff-walking","text":"Open AI Cliff Walking Za\u010dnemo z novo python skripto Uvoz potrebnih paketov import gym import numpy as np import random from time import sleep Deklaracija okolja your_env = gym.make(\"YourEnv \", some_kwarg=your_vars) your_env = gym.make(\"YourEnv\") Seznam okolij https://gymnasium.farama.org/environments/toy_text/ env = gym . make ( 'CliffWalking-v0' ) Resetiramo in prika\u017eemo okolje env . reset () env . render () Prika\u017eemo nekaj parametrov print ( \"Observation space = \" , env . observation_space . n ) print ( \"Actions = \" , env . action_space . n ) Deklariramo Q tabelo q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) #q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n]) print ( \"Q table size = \" , q_table . shape ) Deklariramo parametre learning_rate = 0.1 discount_factor = 0.95 epochs = 60000 epsilon = 1 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = epochs // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) SHOW_EVERY = 1000 Za\u010dnemo z u\u010denjem z izbranim \u0161tevilom epoh for episode in range ( epochs ): state = env . reset () done = False trial_length = 0 Izvedemo posamezni \u201esprehod \u010dez okolje\u201c z izbiro akcij raziskovanje: naklju\u010dna akcija uporabo zbranega znanja: akcija z maximalno q vrednostjo while not done : if ( random . uniform ( 0 , 1 ) < epsilon ): # Exploration with random action action = env . action_space . sample () else : # Use the action with the highest q-value action = np . argmax ( q_table [ state ]) Izvedemo akcijo v okolju in preberemo vrednosti novega stanja in nagrade next_state , reward , done , info = env . step ( action ) Posodobimo stanje Q tabele curr_q = q_table [ state , action ] next_max_q = np . max ( q_table [ next_state ]) new_q = ( 1 - learning_rate ) * curr_q + learning_rate * ( reward + discount_factor * next_max_q ) q_table [ state , action ] = new_q Posodobimo stanje state = next_state Shranimo dol\u017eino trenutnega \u201esprehoda\u201c if episode % SHOW_EVERY == 0 : trial_length += 1 Celotna koda u\u010denja do sedaj for episode in range ( epochs ): state = env . reset () done = False trial_length = 0 while not done : if ( random . uniform ( 0 , 1 ) < epsilon ): # Exploration with random action action = env . action_space . sample () else : # Use the action with the highest q-value action = np . argmax ( q_table [ state ]) next_state , reward , done , info = env . step ( action ) curr_q = q_table [ state , action ] next_max_q = np . max ( q_table [ next_state ]) new_q = ( 1 - learning_rate ) * curr_q + learning_rate * ( reward + discount_factor * next_max_q ) q_table [ state , action ] = new_q state = next_state if episode % SHOW_EVERY == 0 : trial_length += 1 if episode % SHOW_EVERY == 0 : print ( f 'Episode: { episode : >5d } , episode length: { int ( trial_length ) : >5d } ' ) if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value Testiramo nau\u010deno Q tabelo oziroma agenta Resetiramo okolje in ga izri\u0161emo print ( q_table ) state = env . reset () env . render () done = False trial_length = 0 Izvedemo sprehod while not done : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) trial_length += 1 print ( \" Step \" + str ( trial_length )) env . render () sleep ( .2 ) Ve\u010d testov, da vidimo uspe\u0161nost lengths = [] for trialnum in range ( 1 , 11 ): state = env . reset () done = False trial_length = 0 while not done and trial_length < 25 : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) print ( \"Trial number \" + str ( trialnum ) + \" Step \" + str ( trial_length )) env . render () sleep ( .2 ) trial_length += 1 lengths . append ( trial_length ) sleep ( .2 ) avg_len = sum ( lengths ) / 10 print ( avg_len ) Celotna koda cliff.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 import gym import numpy as np import random from time import sleep env = gym . make ( 'CliffWalking-v0' ) env . reset () env . render () print ( \"Observation space = \" , env . observation_space . n ) print ( \"Actions = \" , env . action_space . n ) q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) #q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n]) print ( \"Q table size = \" , q_table . shape ) learning_rate = 0.5 discount_factor = 0.95 epochs = 45000 epsilon = 1 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = epochs // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) SHOW_EVERY = 1000 for episode in range ( epochs ): state = env . reset () done = False trial_length = 0 while not done : if ( random . uniform ( 0 , 1 ) < epsilon ): # Exploration with random action action = env . action_space . sample () else : # Use the action with the highest q-value action = np . argmax ( q_table [ state ]) next_state , reward , done , info = env . step ( action ) curr_q = q_table [ state , action ] next_max_q = np . max ( q_table [ next_state ]) new_q = ( 1 - learning_rate ) * curr_q + learning_rate * ( reward + discount_factor * next_max_q ) q_table [ state , action ] = new_q state = next_state if episode % SHOW_EVERY == 0 : trial_length += 1 if episode % SHOW_EVERY == 0 : print ( f 'Episode: { episode : >5d } , episode length: { int ( trial_length ) : >5d } ' ) if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value print ( q_table ) state = env . reset () env . render () done = False trial_length = 0 while not done : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) trial_length += 1 print ( \" Step \" + str ( trial_length )) env . render () sleep ( .2 ) Zagon skripte Ob zagonu skripte s python cliff.py se bo pojavila napaka /home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py:44: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar. self._cliff = np.zeros(self.shape, dtype=np.bool) Traceback (most recent call last): File \"/home/student/RL/rl_pet/cliff.py\", line 7, in <module> env = gym.make('CliffWalking-v0') File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make return registry.make(id, **kwargs) File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 129, in make env = spec.make(**kwargs) File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 90, in make env = cls(**_kwargs) File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py\", line 44, in __init__ self._cliff = np.zeros(self.shape, dtype=np.bool) File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/numpy/__init__.py\", line 305, in __getattr__ raise AttributeError(__former_attrs__[attr]) AttributeError: module 'numpy' has no attribute 'bool'. `np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here. The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations Najbolj informativen del so vrstice File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py\", line 44, in __init__ self._cliff = np.zeros(self.shape, dtype=np.bool) in AttributeError: module 'numpy' has no attribute 'bool'. `np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here. The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations Napaka nastane, ker je v cliffwalking.py napaka v vrstici 44, ker se uporablja star zapis za spremenljivko bool in sicer zapis dtype=np.bool . Tega je potrebno spremeniti v samo bool . Potrebno je odpreti datoteko, ki se nahaja npr. v /home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py . Polna pot je odvisna od ime va\u0161ega python okolja, npr. /rl_pet . Odprite python datoteko in spremenite kodo. Stara koda cliffwalking.py 43 44 45 # Cliff Location self . _cliff = np . zeros ( self . shape , dtype = np . bool ) self . _cliff [ 3 , 1 : - 1 ] = True Nova koda cliffwalking.py 43 44 45 # Cliff Location self . _cliff = np . zeros ( self . shape , bool ) self . _cliff [ 3 , 1 : - 1 ] = True Celotna koda verzija 2 V tej verziji kode je dodana pred\u010dasna zaustevative u\u010denja za posamezno epoho , \u010de u\u010denje prese\u017ee 99 korakov u\u010denja v posamezni epohi . cliff.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 import gym import numpy as np import random from time import sleep env = gym . make ( 'CliffWalking-v0' ) env . reset () env . render () print ( \"Observation space = \" , env . observation_space . n ) print ( \"Actions = \" , env . action_space . n ) q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) #q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n]) print ( \"Q table size = \" , q_table . shape ) learning_rate = 0.5 discount_factor = 0.95 epochs = 45000 max_steps = 99 epsilon = 1 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = epochs // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) SHOW_EVERY = 1000 for episode in range ( epochs ): state = env . reset () done = False trial_length = 0 #while not done: for step in range ( max_steps ): if ( random . uniform ( 0 , 1 ) < epsilon ): # Exploration with random action action = env . action_space . sample () else : # Use the action with the highest q-value action = np . argmax ( q_table [ state ]) next_state , reward , done , info = env . step ( action ) curr_q = q_table [ state , action ] next_max_q = np . max ( q_table [ next_state ]) new_q = ( 1 - learning_rate ) * curr_q + learning_rate * ( reward + discount_factor * next_max_q ) q_table [ state , action ] = new_q state = next_state if episode % SHOW_EVERY == 0 : trial_length += 1 if done : break if episode % SHOW_EVERY == 0 : print ( f 'Episode: { episode : >5d } , episode length: { int ( trial_length ) : >5d } ' ) if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value print ( q_table ) state = env . reset () env . render () done = False trial_length = 0 while not done : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) trial_length += 1 print ( \" Step \" + str ( trial_length )) env . render () sleep ( .2 )","title":"Primer okolja Cliff Walking"},{"location":"cliff2/","text":"Prikaz Q tabele Prikaz okolja Tekstovni izpis okolja env . render () Grafi\u010dni prikaz okolja Okolje bomo prikazali grafi\u010dno z matplotlib knji\u017enico. import matplotlib.pyplot as plt Nato na konec datoteke dodamo kodo: fig1 , ax1 = plt . subplots () ax1 . axis ( 'off' ) ax1 . axis ( 'tight' ) okolje = [[ \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" ], [ \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" ], [ \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" ], [ \"S\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"G\" ]] okolje_colours = np . asarray ( okolje , dtype = 'U25' ) okolje_rows = len ( okolje [ 0 ][:]) okolje_columns = len ( okolje [:][ 0 ]) okolje_colours [ okolje_colours == \"x\" ] = \"firebrick\" okolje_colours [ okolje_colours == \"G\" ] = \"gold\" okolje_colours [ okolje_colours == \"S\" ] = \"limegreen\" okolje_colours [ okolje_colours == \"o\" ] = \"cornflowerblue\" okolje = [[ \"o,1\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o,12\" ], [ \"o,13\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o,24\" ], [ \"o,25\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o,36\" ], [ \"S,37\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"G,48\" ]] table_okolje = ax1 . table ( cellText = okolje , cellColours = okolje_colours , loc = 'center' , cellLoc = 'center' , rowLoc = 'center' , colLoc = 'center' ) table_okolje . scale ( 1 , 1.5 ) table_okolje . auto_set_font_size ( True ) table_okolje . set_fontsize ( 10 ) plt . show () Tekstovni izpis Q tabele print ( q_table ) Grafi\u010dni izpis Q tabele fig2 , ax2 = plt . subplots () fig2 . patch . set_visible ( False ) ax2 . axis ( 'off' ) columns = [ \"GOR\" , \"DESNO\" , \"DOL\" , \"LEVO\" ] rows = [ \"STANJE %d \" % ( i + 1 ) for i in range ( env . observation_space . n )] qtable = np . around ( qtable , 3 ) qtable_s = qtable [:][ 0 : 22 ] rows_s = rows [ 0 : 22 ] norm = plt . Normalize ( qtable_s . min (), qtable_s . max () + 0.1 ) colours = plt . cm . YlGn ( norm ( qtable_s )) table = ax2 . table ( cellText = qtable_s , rowLabels = rows_s , colLabels = columns , loc = 'center' , cellColours = colours , cellLoc = 'center' , rowLoc = 'center' , colLoc = 'center' , colWidths = [ 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ]) table . auto_set_font_size ( False ) table . set_fontsize ( 8 ) fig3 , ax3 = plt . subplots () fig3 . patch . set_visible ( False ) ax3 . axis ( 'off' ) qtable_s = qtable [:][ 23 : 48 ] rows_s = rows [ 23 : 48 ] colours = plt . cm . YlGn ( norm ( qtable_s )) table = ax3 . table ( cellText = qtable_s , rowLabels = rows_s , colLabels = columns , loc = 'center' , cellColours = colours , cellLoc = 'center' , rowLoc = 'center' , colLoc = 'center' , colWidths = [ 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ]) table . auto_set_font_size ( False ) table . set_fontsize ( 8 ) plt . show ()","title":"Prikaz Q tabele"},{"location":"cliff2/#prikaz-q-tabele","text":"","title":"Prikaz Q tabele"},{"location":"cliff2/#prikaz-okolja","text":"","title":"Prikaz okolja"},{"location":"cliff2/#tekstovni-izpis-okolja","text":"env . render ()","title":"Tekstovni izpis okolja"},{"location":"cliff2/#graficni-prikaz-okolja","text":"Okolje bomo prikazali grafi\u010dno z matplotlib knji\u017enico. import matplotlib.pyplot as plt Nato na konec datoteke dodamo kodo: fig1 , ax1 = plt . subplots () ax1 . axis ( 'off' ) ax1 . axis ( 'tight' ) okolje = [[ \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" ], [ \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" ], [ \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" ], [ \"S\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"G\" ]] okolje_colours = np . asarray ( okolje , dtype = 'U25' ) okolje_rows = len ( okolje [ 0 ][:]) okolje_columns = len ( okolje [:][ 0 ]) okolje_colours [ okolje_colours == \"x\" ] = \"firebrick\" okolje_colours [ okolje_colours == \"G\" ] = \"gold\" okolje_colours [ okolje_colours == \"S\" ] = \"limegreen\" okolje_colours [ okolje_colours == \"o\" ] = \"cornflowerblue\" okolje = [[ \"o,1\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o,12\" ], [ \"o,13\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o,24\" ], [ \"o,25\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o\" , \"o,36\" ], [ \"S,37\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"x\" , \"G,48\" ]] table_okolje = ax1 . table ( cellText = okolje , cellColours = okolje_colours , loc = 'center' , cellLoc = 'center' , rowLoc = 'center' , colLoc = 'center' ) table_okolje . scale ( 1 , 1.5 ) table_okolje . auto_set_font_size ( True ) table_okolje . set_fontsize ( 10 ) plt . show ()","title":"Grafi\u010dni prikaz okolja"},{"location":"cliff2/#tekstovni-izpis-q-tabele","text":"print ( q_table )","title":"Tekstovni izpis Q tabele"},{"location":"cliff2/#graficni-izpis-q-tabele","text":"fig2 , ax2 = plt . subplots () fig2 . patch . set_visible ( False ) ax2 . axis ( 'off' ) columns = [ \"GOR\" , \"DESNO\" , \"DOL\" , \"LEVO\" ] rows = [ \"STANJE %d \" % ( i + 1 ) for i in range ( env . observation_space . n )] qtable = np . around ( qtable , 3 ) qtable_s = qtable [:][ 0 : 22 ] rows_s = rows [ 0 : 22 ] norm = plt . Normalize ( qtable_s . min (), qtable_s . max () + 0.1 ) colours = plt . cm . YlGn ( norm ( qtable_s )) table = ax2 . table ( cellText = qtable_s , rowLabels = rows_s , colLabels = columns , loc = 'center' , cellColours = colours , cellLoc = 'center' , rowLoc = 'center' , colLoc = 'center' , colWidths = [ 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ]) table . auto_set_font_size ( False ) table . set_fontsize ( 8 ) fig3 , ax3 = plt . subplots () fig3 . patch . set_visible ( False ) ax3 . axis ( 'off' ) qtable_s = qtable [:][ 23 : 48 ] rows_s = rows [ 23 : 48 ] colours = plt . cm . YlGn ( norm ( qtable_s )) table = ax3 . table ( cellText = qtable_s , rowLabels = rows_s , colLabels = columns , loc = 'center' , cellColours = colours , cellLoc = 'center' , rowLoc = 'center' , colLoc = 'center' , colWidths = [ 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ]) table . auto_set_font_size ( False ) table . set_fontsize ( 8 ) plt . show ()","title":"Grafi\u010dni izpis Q tabele"},{"location":"cliff3/","text":"Eksploracija Vizualizacija funkcije za epsilon epsilon.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 import matplotlib.pyplot as plt from matplotlib.widgets import Slider , Button import numpy as np # Exploration parameters epsilon = 1.0 # Exploration rate max_epsilon = 1.0 # Exploration probability at start min_epsilon = 0.01 # Minimum exploration probability decay_rate = 0.001 # Exponential decay rate for exploration prob total_episodes = 10000 def plot_fcn ( min_epsilon_fcn , max_epsilon_fcn , decay_rate_fcn , total_episodes_fcn ): epsilon_values = [( min_epsilon_fcn + ( max_epsilon_fcn - min_epsilon_fcn ) * np . exp ( - decay_rate_fcn * episode )) for episode in range ( total_episodes_fcn )] ep_num = range ( total_episodes_fcn ) return epsilon_values #define inital values for sliders init_min_epsilon = min_epsilon init_max_epsilon = max_epsilon init_decay_rate = decay_rate init_total_episodes = total_episodes fig1 , ax1 = plt . subplots () line , = plt . plot ( plot_fcn ( init_min_epsilon , init_max_epsilon , init_decay_rate , init_total_episodes ), lw = 2 ) ax1 . set_xlabel ( \"Epizoda\" , fontsize = 15 ) ax1 . set_ylabel ( \"Epsilon vrednost\" , fontsize = 15 ) plt . subplots_adjust ( bottom = 0.3 ) plt . xticks ( fontsize = 14 ) plt . yticks ( fontsize = 14 ) plt . title ( \"Vrednost parametra epsilon v odvisnosti od \u0161tevila epizod (u\u010denje Q)\" , fontsize = 18 ) plt . gcf () . text ( 0.4 , 0.05 , r '$\\epsilon = \\epsilon_ {min} + (\\epsilon_ {max} - \\epsilon_ {min} )*e^{(\\lambda*N)}$' , fontsize = 18 ) #slider for decay rate ax_decay_rate = plt . axes ([ 0.25 , 0.1 , 0.65 , 0.03 ]) decay_rate_slider = Slider ( ax = ax_decay_rate , label = 'Lambda' , valmin = 0.0001 , valmax = 0.01 , valinit = init_decay_rate , ) #slider for max epsilon value ax_max_epsilon = plt . axes ([ 0.25 , 0.13 , 0.65 , 0.03 ]) max_epsilon_slider = Slider ( ax = ax_max_epsilon , label = 'Epsilon (max)' , valmin = 0.1 , valmax = 1 , valinit = init_max_epsilon , ) #slider for min epsilon value ax_min_epsilon = plt . axes ([ 0.25 , 0.16 , 0.65 , 0.03 ]) min_epsilon_slider = Slider ( ax = ax_min_epsilon , label = 'Epsilon (min)' , valmin = 0 , valmax = 0.5 , valinit = init_min_epsilon , ) #slider for max episode value ax_total_episodes = plt . axes ([ 0.25 , 0.19 , 0.65 , 0.03 ]) total_episodes_slider = Slider ( ax = ax_total_episodes , label = 'Skupno \u0161tevilo epizod - N' , valmin = 1 , valstep = 1 , valmax = 10000 , valinit = init_total_episodes , ) def update ( val ): line . set_data ( range ( total_episodes_slider . val ), plot_fcn ( min_epsilon_slider . val , max_epsilon_slider . val , decay_rate_slider . val , total_episodes_slider . val )) #ax3.set_xlim(0,total_episodes_slider.val) ax3 . autoscale_view ( True , True , True ) ax3 . relim () fig . canvas . draw_idle () decay_rate_slider . on_changed ( update ) max_epsilon_slider . on_changed ( update ) min_epsilon_slider . on_changed ( update ) total_episodes_slider . on_changed ( update ) decay_rate_slider . label . set_size ( 16 ) max_epsilon_slider . label . set_size ( 16 ) min_epsilon_slider . label . set_size ( 16 ) total_episodes_slider . label . set_size ( 16 ) resetax = plt . axes ([ 0.8 , 0.025 , 0.1 , 0.04 ]) button = Button ( resetax , 'Reset' , hovercolor = '0.975' ) button . label . set_size ( 16 ) def reset ( event ): decay_rate_slider . reset () max_epsilon_slider . reset () min_epsilon_slider . reset () total_episodes_slider . reset () ax3 . autoscale_view ( True , True , True ) ax3 . relim () button . on_clicked ( reset ) plt . show () Sprememba parametrov za izbol\u0161anje u\u010denja cliff.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import gym import numpy as np import random from time import sleep import matplotlib.pyplot as plt env = gym . make ( 'CliffWalking-v0' ) env . reset () env . render () print ( \"Observation space = \" , env . observation_space . n ) print ( \"Actions = \" , env . action_space . n ) q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) #q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n]) print ( \"Q table size = \" , q_table . shape ) learning_rate = 0.5 discount_factor = 0.95 epochs = 45000 max_steps = 99 epsilon = 1 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = epochs // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) Zamenjamo z # Exploration parameters epsilon = 1.0 # Exploration rate max_epsilon = 1.0 # Exploration probability at start min_epsilon = 0.01 # Minimum exploration probability decay_rate = 0.001 # Exponential decay rate for exploration prob Zamenjamo krivuljo za epsilon spremenljivko if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value s kodo epsilon = min_epsilon + ( max_epsilon - min_epsilon ) * np . exp ( - decay_rate * episode )","title":"Parametri u\u010denja in eksploracija"},{"location":"cliff3/#eksploracija","text":"","title":"Eksploracija"},{"location":"cliff3/#vizualizacija-funkcije-za-epsilon","text":"epsilon.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 import matplotlib.pyplot as plt from matplotlib.widgets import Slider , Button import numpy as np # Exploration parameters epsilon = 1.0 # Exploration rate max_epsilon = 1.0 # Exploration probability at start min_epsilon = 0.01 # Minimum exploration probability decay_rate = 0.001 # Exponential decay rate for exploration prob total_episodes = 10000 def plot_fcn ( min_epsilon_fcn , max_epsilon_fcn , decay_rate_fcn , total_episodes_fcn ): epsilon_values = [( min_epsilon_fcn + ( max_epsilon_fcn - min_epsilon_fcn ) * np . exp ( - decay_rate_fcn * episode )) for episode in range ( total_episodes_fcn )] ep_num = range ( total_episodes_fcn ) return epsilon_values #define inital values for sliders init_min_epsilon = min_epsilon init_max_epsilon = max_epsilon init_decay_rate = decay_rate init_total_episodes = total_episodes fig1 , ax1 = plt . subplots () line , = plt . plot ( plot_fcn ( init_min_epsilon , init_max_epsilon , init_decay_rate , init_total_episodes ), lw = 2 ) ax1 . set_xlabel ( \"Epizoda\" , fontsize = 15 ) ax1 . set_ylabel ( \"Epsilon vrednost\" , fontsize = 15 ) plt . subplots_adjust ( bottom = 0.3 ) plt . xticks ( fontsize = 14 ) plt . yticks ( fontsize = 14 ) plt . title ( \"Vrednost parametra epsilon v odvisnosti od \u0161tevila epizod (u\u010denje Q)\" , fontsize = 18 ) plt . gcf () . text ( 0.4 , 0.05 , r '$\\epsilon = \\epsilon_ {min} + (\\epsilon_ {max} - \\epsilon_ {min} )*e^{(\\lambda*N)}$' , fontsize = 18 ) #slider for decay rate ax_decay_rate = plt . axes ([ 0.25 , 0.1 , 0.65 , 0.03 ]) decay_rate_slider = Slider ( ax = ax_decay_rate , label = 'Lambda' , valmin = 0.0001 , valmax = 0.01 , valinit = init_decay_rate , ) #slider for max epsilon value ax_max_epsilon = plt . axes ([ 0.25 , 0.13 , 0.65 , 0.03 ]) max_epsilon_slider = Slider ( ax = ax_max_epsilon , label = 'Epsilon (max)' , valmin = 0.1 , valmax = 1 , valinit = init_max_epsilon , ) #slider for min epsilon value ax_min_epsilon = plt . axes ([ 0.25 , 0.16 , 0.65 , 0.03 ]) min_epsilon_slider = Slider ( ax = ax_min_epsilon , label = 'Epsilon (min)' , valmin = 0 , valmax = 0.5 , valinit = init_min_epsilon , ) #slider for max episode value ax_total_episodes = plt . axes ([ 0.25 , 0.19 , 0.65 , 0.03 ]) total_episodes_slider = Slider ( ax = ax_total_episodes , label = 'Skupno \u0161tevilo epizod - N' , valmin = 1 , valstep = 1 , valmax = 10000 , valinit = init_total_episodes , ) def update ( val ): line . set_data ( range ( total_episodes_slider . val ), plot_fcn ( min_epsilon_slider . val , max_epsilon_slider . val , decay_rate_slider . val , total_episodes_slider . val )) #ax3.set_xlim(0,total_episodes_slider.val) ax3 . autoscale_view ( True , True , True ) ax3 . relim () fig . canvas . draw_idle () decay_rate_slider . on_changed ( update ) max_epsilon_slider . on_changed ( update ) min_epsilon_slider . on_changed ( update ) total_episodes_slider . on_changed ( update ) decay_rate_slider . label . set_size ( 16 ) max_epsilon_slider . label . set_size ( 16 ) min_epsilon_slider . label . set_size ( 16 ) total_episodes_slider . label . set_size ( 16 ) resetax = plt . axes ([ 0.8 , 0.025 , 0.1 , 0.04 ]) button = Button ( resetax , 'Reset' , hovercolor = '0.975' ) button . label . set_size ( 16 ) def reset ( event ): decay_rate_slider . reset () max_epsilon_slider . reset () min_epsilon_slider . reset () total_episodes_slider . reset () ax3 . autoscale_view ( True , True , True ) ax3 . relim () button . on_clicked ( reset ) plt . show ()","title":"Vizualizacija funkcije za epsilon"},{"location":"cliff3/#sprememba-parametrov-za-izbolsanje-ucenja","text":"cliff.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import gym import numpy as np import random from time import sleep import matplotlib.pyplot as plt env = gym . make ( 'CliffWalking-v0' ) env . reset () env . render () print ( \"Observation space = \" , env . observation_space . n ) print ( \"Actions = \" , env . action_space . n ) q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) #q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n]) print ( \"Q table size = \" , q_table . shape ) learning_rate = 0.5 discount_factor = 0.95 epochs = 45000 max_steps = 99 epsilon = 1 START_EPSILON_DECAYING = 1 END_EPSILON_DECAYING = epochs // 2 epsilon_decay_value = epsilon / ( END_EPSILON_DECAYING - START_EPSILON_DECAYING ) Zamenjamo z # Exploration parameters epsilon = 1.0 # Exploration rate max_epsilon = 1.0 # Exploration probability at start min_epsilon = 0.01 # Minimum exploration probability decay_rate = 0.001 # Exponential decay rate for exploration prob Zamenjamo krivuljo za epsilon spremenljivko if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING : epsilon -= epsilon_decay_value s kodo epsilon = min_epsilon + ( max_epsilon - min_epsilon ) * np . exp ( - decay_rate * episode )","title":"Sprememba parametrov za izbol\u0161anje u\u010denja"},{"location":"conda_env/","text":"Priprava python okolja Paket conda Paket conda je priljubljen upravitelj paketov za Python, ki se pogosto uporablja za razvoj in testiranje Python programov. Je tudi odli\u010dno orodje za upravljanje in vzdr\u017eevanje okolij Pythona, zlasti za usposabljanje in razvoj algoritmov spodbujevalnega u\u010denja (RL). S paketom conda lahko preprosto ustvarite in upravljate okolja Python, kar vam omogo\u010da delo na razli\u010dnih projektih z razli\u010dnimi knji\u017enicami in algoritmi, ne da bi vas skrbelo upravljanje ve\u010d kopij istega paketa. Tako lahko na primer ustvarite novo okolje za projekt RL in vanj namestite potrebne knji\u017enice in algoritme. To okolje lahko nato uporabite za u\u010denje in razvoj modela RL. Paket conda zagotavlja tudi veliko vgrajenih funkcij za upravljanje in organizacijo okolja, kot so spremenljivke okolja in upravitelji paketov. Paket conda je torej zmogljivo orodje za upravljanje in vzdr\u017eevanje okolij Pythona, zlasti za u\u010denje in razvoj algoritmov spodbujevalnega u\u010denja. Omogo\u010da enostavno ustvarjanje in upravljanje okolij, namestitev potrebnih knji\u017enic in algoritmov ter obdelavo velikih koli\u010din podatkov. S paketom conda lahko preprosto zgradite in namestite svoj model RL ter hitro za\u010dnete s svojim prvim projektom. Bistvene to\u010dke: Ustvarjanje razli\u010dnih okolij v katerih imate lahko in\u0161talirane programske pakete za rezli\u010dne pakete. In\u0161talacije so lo\u010dene med okolji in ne vplivajo drugo na drugo. Deluje na operacijskih sistemih Windowsi in Linux. Spletna stran: https://www.anaconda.com/ In\u0161talacija po navodilih dodatna nastavitev conda config --set auto_activate_base false Ustvarjanje python okolja s conda paketom conda create --name rl_test python=3.9 Aktivacija okolja conda activate rl_test Deaktivacija okolja conda deactivate Organizacija vaj \u010cetrtkova skupina conda okolje rl_cet Petkova skupina conda okolje rl_pet Navodila Odprite terminal z bli\u017enjico CTRL+ALT+T Pomaknite se v mapo RL cd RL Ustvarite va\u0161e mapo, npr. rl_cet : mkdir rl_cet cd rl_cet Ustvarite python okolje rl_cet s conda paketom conda create --name rl_cet python=3.9 Na vpra\u0161anje Proceed ([y]/n)? odgovorite z y . Aktivirajte okolje conda activate rl_cet In\u0161talacija python paketov za spodbujevalno u\u010denje pip install gym==0.21.0 pip install pyglet==1.5.27 pip install stable-baselines3 pip install sb3_contrib pip install tensorboard","title":"Priprava okolja"},{"location":"conda_env/#priprava-python-okolja","text":"","title":"Priprava python okolja"},{"location":"conda_env/#paket-conda","text":"Paket conda je priljubljen upravitelj paketov za Python, ki se pogosto uporablja za razvoj in testiranje Python programov. Je tudi odli\u010dno orodje za upravljanje in vzdr\u017eevanje okolij Pythona, zlasti za usposabljanje in razvoj algoritmov spodbujevalnega u\u010denja (RL). S paketom conda lahko preprosto ustvarite in upravljate okolja Python, kar vam omogo\u010da delo na razli\u010dnih projektih z razli\u010dnimi knji\u017enicami in algoritmi, ne da bi vas skrbelo upravljanje ve\u010d kopij istega paketa. Tako lahko na primer ustvarite novo okolje za projekt RL in vanj namestite potrebne knji\u017enice in algoritme. To okolje lahko nato uporabite za u\u010denje in razvoj modela RL. Paket conda zagotavlja tudi veliko vgrajenih funkcij za upravljanje in organizacijo okolja, kot so spremenljivke okolja in upravitelji paketov. Paket conda je torej zmogljivo orodje za upravljanje in vzdr\u017eevanje okolij Pythona, zlasti za u\u010denje in razvoj algoritmov spodbujevalnega u\u010denja. Omogo\u010da enostavno ustvarjanje in upravljanje okolij, namestitev potrebnih knji\u017enic in algoritmov ter obdelavo velikih koli\u010din podatkov. S paketom conda lahko preprosto zgradite in namestite svoj model RL ter hitro za\u010dnete s svojim prvim projektom. Bistvene to\u010dke: Ustvarjanje razli\u010dnih okolij v katerih imate lahko in\u0161talirane programske pakete za rezli\u010dne pakete. In\u0161talacije so lo\u010dene med okolji in ne vplivajo drugo na drugo. Deluje na operacijskih sistemih Windowsi in Linux. Spletna stran: https://www.anaconda.com/ In\u0161talacija po navodilih dodatna nastavitev conda config --set auto_activate_base false","title":"Paket conda"},{"location":"conda_env/#ustvarjanje-python-okolja-s-conda-paketom","text":"conda create --name rl_test python=3.9","title":"Ustvarjanje python okolja s conda paketom"},{"location":"conda_env/#aktivacija-okolja","text":"conda activate rl_test","title":"Aktivacija okolja"},{"location":"conda_env/#deaktivacija-okolja","text":"conda deactivate","title":"Deaktivacija okolja"},{"location":"conda_env/#organizacija-vaj","text":"","title":"Organizacija vaj"},{"location":"conda_env/#cetrtkova-skupina","text":"conda okolje rl_cet","title":"\u010cetrtkova skupina"},{"location":"conda_env/#petkova-skupina","text":"conda okolje rl_pet","title":"Petkova skupina"},{"location":"conda_env/#navodila","text":"Odprite terminal z bli\u017enjico CTRL+ALT+T Pomaknite se v mapo RL cd RL Ustvarite va\u0161e mapo, npr. rl_cet : mkdir rl_cet cd rl_cet Ustvarite python okolje rl_cet s conda paketom conda create --name rl_cet python=3.9 Na vpra\u0161anje Proceed ([y]/n)? odgovorite z y . Aktivirajte okolje conda activate rl_cet In\u0161talacija python paketov za spodbujevalno u\u010denje pip install gym==0.21.0 pip install pyglet==1.5.27 pip install stable-baselines3 pip install sb3_contrib pip install tensorboard","title":"Navodila"},{"location":"hockey_env/","text":"Hockey Environment Opis Koda definira okolje za simulacijo igre zra\u010dnega hokeja. Okolje se izvaja z uporabo fizikalnega motorja Pybox2D in API OpenAI Gym. Okolje je sestavljeno iz pravokotnega igralnega polja z osrednjo \u010drto, ki polje deli na dve polovici. Na igri\u0161\u010du sta dva igralca, enega upravlja agent umetne inteligence, drugega pa hevristi\u010dni algoritem. Cilj igre je izstreliti plo\u0161\u010dek v nasprotnikova vrata in hkrati braniti svoja vrata. Koda definira razred z imenom HockeyEnv , ki deduje iz razreda gym.Env . Konstruktor razreda inicializira razli\u010dne parametre okolja, kot so dimenzije igralnega polja, velikosti plo\u0161\u010dka in igralcev, najve\u010dje hitrosti igralcev in plo\u0161\u010dka ter \u0161tevilo \u010dasovnih korakov za vsako epizodo. Opredeljena sta tudi prostor stanj in akcij okolja. Prostor stanj je 12-razse\u017eni zvezni prostor, ki predstavlja polo\u017eaje in hitrosti agenta, nasprotnika in plo\u0161\u010dka. Prostor stanj je dvodimenzionalni zvezni prostor, ki predstavlja hitrosti x in y agenta. Razred HockeyEnv ustvari svet Pybox2D in inicializira entitete igre, kot so igralci, plo\u0161\u010dek in vratnice. Okolje ustvari tudi dva objekta umetne inteligence, top_ai in bottom_ai , ki nadzorujeta gibanje nasprotnika. Ta objekta uporabljata preprosto hevristi\u010dno strategijo sledenja plo\u0161\u010dku in premikanja proti njemu za obrambo gola. Koda opredeljuje tudi razred ContactListener , ki podeduje razred b2ContactListener , ki ga zagotavlja Pybox2D. Ta razred se uporablja za zaznavanje trkov med entitetami igre. Na koncu razred HockeyEnv definira ve\u010d metod za pridobivanje polo\u017eajev in hitrosti igralnih entitet, posodabljanje stanja igre in prikazovanja igre. Igra se lahko v realnem \u010dasu prika\u017ee v oknu Pygame, zaslon pa se lahko shrani v obliki videoposnetka.","title":"Hockey Environment"},{"location":"hockey_env/#hockey-environment","text":"","title":"Hockey Environment"},{"location":"hockey_env/#opis","text":"Koda definira okolje za simulacijo igre zra\u010dnega hokeja. Okolje se izvaja z uporabo fizikalnega motorja Pybox2D in API OpenAI Gym. Okolje je sestavljeno iz pravokotnega igralnega polja z osrednjo \u010drto, ki polje deli na dve polovici. Na igri\u0161\u010du sta dva igralca, enega upravlja agent umetne inteligence, drugega pa hevristi\u010dni algoritem. Cilj igre je izstreliti plo\u0161\u010dek v nasprotnikova vrata in hkrati braniti svoja vrata. Koda definira razred z imenom HockeyEnv , ki deduje iz razreda gym.Env . Konstruktor razreda inicializira razli\u010dne parametre okolja, kot so dimenzije igralnega polja, velikosti plo\u0161\u010dka in igralcev, najve\u010dje hitrosti igralcev in plo\u0161\u010dka ter \u0161tevilo \u010dasovnih korakov za vsako epizodo. Opredeljena sta tudi prostor stanj in akcij okolja. Prostor stanj je 12-razse\u017eni zvezni prostor, ki predstavlja polo\u017eaje in hitrosti agenta, nasprotnika in plo\u0161\u010dka. Prostor stanj je dvodimenzionalni zvezni prostor, ki predstavlja hitrosti x in y agenta. Razred HockeyEnv ustvari svet Pybox2D in inicializira entitete igre, kot so igralci, plo\u0161\u010dek in vratnice. Okolje ustvari tudi dva objekta umetne inteligence, top_ai in bottom_ai , ki nadzorujeta gibanje nasprotnika. Ta objekta uporabljata preprosto hevristi\u010dno strategijo sledenja plo\u0161\u010dku in premikanja proti njemu za obrambo gola. Koda opredeljuje tudi razred ContactListener , ki podeduje razred b2ContactListener , ki ga zagotavlja Pybox2D. Ta razred se uporablja za zaznavanje trkov med entitetami igre. Na koncu razred HockeyEnv definira ve\u010d metod za pridobivanje polo\u017eajev in hitrosti igralnih entitet, posodabljanje stanja igre in prikazovanja igre. Igra se lahko v realnem \u010dasu prika\u017ee v oknu Pygame, zaslon pa se lahko shrani v obliki videoposnetka.","title":"Opis"},{"location":"hockey_help_snippets/","text":"Ideje za kodo Primer za igranje dveh RL modelov med sabo modelBM = TQC . load ( \"models/TQC3/5680000\" ) modelTM = TQC . load ( \"models/TQC4/17360000\" ) for episode in range ( EPISODES ): obs = env . reset () done = False while not done : actionBM , _states = modelBM . predict ( obs ) actionTM , _states = modelTM . predict ( ConvertObsTopMallet ( obs , env . observation_space . high [ 0 ], env . observation_space . high [ 1 ])) obs , rewards , done , info = env . step ( bottom_mallet_action = actionBM , top_mallet_action = ConvertActionTopMallet ( actionTM )) env . render () env . close () Potrebna je predelava klica step funkcije v circle_world.py python skripti: Stara verzija def step ( self , action ) Nova verzija: def step ( self , bottom_mallet_action = None , top_mallet_action = None ) Pretvorba opazovanj za zgornjega agenta (nasprotnika) def ConvertObsTopMallet ( obs , width , height ): puck_pos = - np . array ([ obs [ 0 ], obs [ 1 ]]) + np . array ([ width , height ]) puck_vel = - np . array ([ obs [ 2 ], obs [ 3 ]]) bottom_mallet_pos = - np . array ([ obs [ 4 ], obs [ 5 ]]) + np . array ([ width , height ]) bottom_mallet_vel = - np . array ([ obs [ 6 ], obs [ 7 ]]) #top_mallet_pos = -np.array([obs[6], obs[7]]) + np.array([width, height]) top_mallet_pos = - np . array ([ obs [ 8 ], obs [ 9 ]]) + np . array ([ width , height ]) top_mallet_vel = - np . array ([ obs [ 10 ], obs [ 11 ]]) #conv_obs = np.concatenate((puck_pos, puck_vel, top_mallet_pos, bottom_mallet_pos)) conv_obs = np . concatenate (( puck_pos , puck_vel , top_mallet_pos , top_mallet_vel , bottom_mallet_pos , bottom_mallet_vel )) return conv_obs Pretvorba akcij za zgornjega agenta (nasprotnika) def ConvertActionTopMallet ( action ): return - np . array ([ action [ 0 ], action [ 1 ]])","title":"Funkcije za pomo\u010d pri programiranju"},{"location":"hockey_help_snippets/#ideje-za-kodo","text":"","title":"Ideje za kodo"},{"location":"hockey_help_snippets/#primer-za-igranje-dveh-rl-modelov-med-sabo","text":"modelBM = TQC . load ( \"models/TQC3/5680000\" ) modelTM = TQC . load ( \"models/TQC4/17360000\" ) for episode in range ( EPISODES ): obs = env . reset () done = False while not done : actionBM , _states = modelBM . predict ( obs ) actionTM , _states = modelTM . predict ( ConvertObsTopMallet ( obs , env . observation_space . high [ 0 ], env . observation_space . high [ 1 ])) obs , rewards , done , info = env . step ( bottom_mallet_action = actionBM , top_mallet_action = ConvertActionTopMallet ( actionTM )) env . render () env . close () Potrebna je predelava klica step funkcije v circle_world.py python skripti: Stara verzija def step ( self , action ) Nova verzija: def step ( self , bottom_mallet_action = None , top_mallet_action = None ) Pretvorba opazovanj za zgornjega agenta (nasprotnika) def ConvertObsTopMallet ( obs , width , height ): puck_pos = - np . array ([ obs [ 0 ], obs [ 1 ]]) + np . array ([ width , height ]) puck_vel = - np . array ([ obs [ 2 ], obs [ 3 ]]) bottom_mallet_pos = - np . array ([ obs [ 4 ], obs [ 5 ]]) + np . array ([ width , height ]) bottom_mallet_vel = - np . array ([ obs [ 6 ], obs [ 7 ]]) #top_mallet_pos = -np.array([obs[6], obs[7]]) + np.array([width, height]) top_mallet_pos = - np . array ([ obs [ 8 ], obs [ 9 ]]) + np . array ([ width , height ]) top_mallet_vel = - np . array ([ obs [ 10 ], obs [ 11 ]]) #conv_obs = np.concatenate((puck_pos, puck_vel, top_mallet_pos, bottom_mallet_pos)) conv_obs = np . concatenate (( puck_pos , puck_vel , top_mallet_pos , top_mallet_vel , bottom_mallet_pos , bottom_mallet_vel )) return conv_obs Pretvorba akcij za zgornjega agenta (nasprotnika) def ConvertActionTopMallet ( action ): return - np . array ([ action [ 0 ], action [ 1 ]])","title":"Primer za igranje dveh RL modelov med sabo"},{"location":"instalCircEnv/","text":"In\u0161talacija okolja CircEnv Novo Python okolje Ustvarite novo conda okolje, recimo rl_test17 : conda create --name rl_test17 python=3.9 conda activate rl_test17 Ustvarite mapo v katero boste skopirali zip datoteko s CircEnv okoljem cd RL mkdir rl_test17 cd rl_test17 Vanjo odpakirate zip datoteko In\u0161talacija swig paketa Preverite, \u010de je in\u0161taliran swig z ukazom swig -version Ukaz bi moral vrniti nekaj podobnega spodnjemu izpisu SWIG Version 3.0.12 Compiled with g++ [x86_64-pc-linux-gnu] Configured options: +pcre Please see http://www.swig.org for reporting bugs and further information \u010ce niste dobili podobnega izpisa, oziroma ste dobili obvestilo Command 'swig' not found, but can be installed with: sudo apt install swig je potrebno swig in\u0161talirati. sudo apt update sudo apt -y install swig Geslo za sudo je 3krat4 . Instalacija okolja in paketov Ime mape za okolje mora biti CircEnv . Pomaknite se v mapo, ki vsebuje mapo CircEnv . Primer: ll total 24 drwxrwxr-x 4 student student 12288 May 3 09:51 ./ drwxrwxr-x 16 student student 4096 Apr 13 16:01 ../ drwxrwxr-x 4 student student 4096 May 3 09:59 CircEnv/ Nato naredite in\u0161talacijo okolja in paketov z ukazi: pip install -e CircEnv pip install box2d box2d-kengz Testni zagon okolja cd CircEnv python playCirc.py","title":"In\u0161talacija paketa CircEnv"},{"location":"instalCircEnv/#instalacija-okolja-circenv","text":"","title":"In\u0161talacija okolja CircEnv"},{"location":"instalCircEnv/#novo-python-okolje","text":"Ustvarite novo conda okolje, recimo rl_test17 : conda create --name rl_test17 python=3.9 conda activate rl_test17 Ustvarite mapo v katero boste skopirali zip datoteko s CircEnv okoljem cd RL mkdir rl_test17 cd rl_test17 Vanjo odpakirate zip datoteko","title":"Novo Python okolje"},{"location":"instalCircEnv/#instalacija-swig-paketa","text":"Preverite, \u010de je in\u0161taliran swig z ukazom swig -version Ukaz bi moral vrniti nekaj podobnega spodnjemu izpisu SWIG Version 3.0.12 Compiled with g++ [x86_64-pc-linux-gnu] Configured options: +pcre Please see http://www.swig.org for reporting bugs and further information \u010ce niste dobili podobnega izpisa, oziroma ste dobili obvestilo Command 'swig' not found, but can be installed with: sudo apt install swig je potrebno swig in\u0161talirati. sudo apt update sudo apt -y install swig Geslo za sudo je 3krat4 .","title":"In\u0161talacija swig paketa"},{"location":"instalCircEnv/#instalacija-okolja-in-paketov","text":"Ime mape za okolje mora biti CircEnv . Pomaknite se v mapo, ki vsebuje mapo CircEnv . Primer: ll total 24 drwxrwxr-x 4 student student 12288 May 3 09:51 ./ drwxrwxr-x 16 student student 4096 Apr 13 16:01 ../ drwxrwxr-x 4 student student 4096 May 3 09:59 CircEnv/ Nato naredite in\u0161talacijo okolja in paketov z ukazi: pip install -e CircEnv pip install box2d box2d-kengz","title":"Instalacija okolja in paketov"},{"location":"instalCircEnv/#testni-zagon-okolja","text":"cd CircEnv python playCirc.py","title":"Testni zagon okolja"},{"location":"load_car/","text":"Zagon agenta Skripta za izvajanje agenta na podlagi shranjene Q tabele Ustvarite novo skripto load_car.py . Inicializacija okolja import gym import numpy as np from time import sleep env = gym . make ( \"MountainCar-v0\" ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) Inicializacija agenta: nalo\u017eimo shranjeno Q tabelo q_table = np . load ( f \"cart_e14900-qtable.npy\" ) print ( \"Q table size = \" , q_table . shape ) 4. Za\u017eenemo okolje in agenta state = env . reset () discrete_state = get_discrete_state ( state ) env . render () done = False while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) env . render () #sleep(0.5) env . close () Celotna koda load_car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import gym import numpy as np from time import sleep env = gym . make ( \"MountainCar-v0\" ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) q_table = np . load ( f \"cart_e14900-qtable.npy\" ) print ( \"Q table size = \" , q_table . shape ) state = env . reset () discrete_state = get_discrete_state ( state ) env . render () done = False while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) env . render () #sleep(0.5) env . close ()","title":"Test agenta"},{"location":"load_car/#zagon-agenta","text":"Skripta za izvajanje agenta na podlagi shranjene Q tabele Ustvarite novo skripto load_car.py . Inicializacija okolja import gym import numpy as np from time import sleep env = gym . make ( \"MountainCar-v0\" ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) Inicializacija agenta: nalo\u017eimo shranjeno Q tabelo q_table = np . load ( f \"cart_e14900-qtable.npy\" ) print ( \"Q table size = \" , q_table . shape ) 4. Za\u017eenemo okolje in agenta state = env . reset () discrete_state = get_discrete_state ( state ) env . render () done = False while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) env . render () #sleep(0.5) env . close () Celotna koda load_car.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import gym import numpy as np from time import sleep env = gym . make ( \"MountainCar-v0\" ) #DISCRETE_OS_SIZE = [20, 20] GRID_SIZE = 20 ; DISCRETE_OS_SIZE = [ GRID_SIZE ] * len ( env . observation_space . high ) obs_high = env . observation_space . high obs_low = env . observation_space . low discrete_os_win_size = ( obs_high - obs_low ) / DISCRETE_OS_SIZE print ( discrete_os_win_size ) def get_discrete_state ( state ): discrete_state = ( state - obs_low ) / discrete_os_win_size discrete_state = np . clip ( discrete_state . astype ( int ), 0 , GRID_SIZE - 1 ) return tuple ( discrete_state ) q_table = np . load ( f \"cart_e14900-qtable.npy\" ) print ( \"Q table size = \" , q_table . shape ) state = env . reset () discrete_state = get_discrete_state ( state ) env . render () done = False while not done : action = np . argmax ( q_table [ discrete_state ]) state , reward , done , info = env . step ( action ) discrete_state = get_discrete_state ( state ) env . render () #sleep(0.5) env . close ()","title":"Zagon agenta"},{"location":"move_agent_mouse/","text":"step() ############ TUKAJ SPREMINJATE action = self . move_agent_mouse ()","title":"Premik agenta z mi\u0161ko"},{"location":"playCirc_mouse/","text":"playCirc.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import gym import numpy as np import pygame import circ_env env = gym . make ( 'circ_env/Circle-v0' , render_mode = \"human\" ) # Reset the environment obs = env . reset () env . render () # Run the environment with random actions #for i in range(500): start_pos = None reset_time = 0 while True : #if any([event.type == pygame.QUIT for event in pygame.event.get()]): break #player controls keys = pygame . key . get_pressed () if keys [ pygame . K_LEFT ]: x = - 1 elif keys [ pygame . K_RIGHT ]: x = 1 else : x = 0 if keys [ pygame . K_UP ]: y = - 1 elif keys [ pygame . K_DOWN ]: y = 1 else : y = 0 for event in pygame . event . get (): if event . type == pygame . MOUSEBUTTONDOWN : # Set the starting position start_pos = pygame . mouse . get_pos () elif event . type == pygame . MOUSEBUTTONUP : # Reset the starting position start_pos = None if start_pos is not None : current_pos = pygame . mouse . get_pos () if pygame . time . get_ticks () - reset_time >= 100 : # Reset the starting position every second start_pos = current_pos reset_time = pygame . time . get_ticks () dx = current_pos [ 0 ] - start_pos [ 0 ] dy = current_pos [ 1 ] - start_pos [ 1 ] x = np . sign ( dx ) y = np . sign ( dy ) action = np . array ([ x , y ], dtype = np . float32 ) #action = env.action_space.sample() #print(action) obs , reward , done , _ = env . step ( action ) # Render the environment env . render () # Check if the episode is finished if done : obs = env . reset () start_pos = None reset_time = 0 # Close the environment env . close ()","title":"playCirc mouse"},{"location":"save_models/","text":"Logiranje in spremljanje u\u010denja Python skripta za testiranje agenta load_carDQN2.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import gym from stable_baselines3 import DQN from stable_baselines3.common.evaluation import evaluate_policy import matplotlib.pyplot as plt import numpy as np env = gym . make ( \"MountainCar-v0\" ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) model = DQN . load ( \"dqn_car\" , env = env ) mean_reward , std_reward = evaluate_policy ( model , model . get_env (), n_eval_episodes = 10 ) print ( f 'Mean reward: { mean_reward } , Std reward: { std_reward } ' ) reward_list = [] ### for episode in range ( 50 ): ### state = env . reset () done = False print ( \"EPISODE \" , episode ) tot_reward , reward = 0 , 0 ### while not done : action , _state = model . predict ( state , deterministic = True ) state , reward , done , info = env . step ( action ) tot_reward += reward ### reward_list . append ( tot_reward ) ### env . close () f = plt . figure () plt . plot ( reward_list ) plt . xticks ( range ( 0 , len ( reward_list ), 2 ), range ( 1 , len ( reward_list ) + 1 , 2 )) plt . axhline ( y = - 200 , color = 'k' , linestyle = '--' ) plt . axhline ( y = np . average ( reward_list ), color = 'blue' , linestyle = '--' , label = 'povpre\u010dna nagrada' ) plt . annotate ( str ( np . average ( reward_list )), xy = ( - 2 , np . average ( reward_list ) + 0.7 ), color = 'blue' , fontsize = 13 , weight = 'bold' ) ax = plt . gca () ax . tick_params ( axis = \"both\" , labelsize = 12 ) f . legend ( loc = 'right' , fontsize = 13 ) plt . xlabel ( 'Epizoda' , fontsize = 14 ) plt . ylabel ( 'Nagrada' , fontsize = 14 ) plt . title ( 'Vrednost nagrade v posamezni epizodi' , fontsize = 16 ) #f.savefig('reward3.jpg') ### plt . show () Logiranje in sprotno shranjevanje agentov Paket Tensorboard Prilagojena Python skripta Pripravimo mape za shranjevanje modelov 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import gym from stable_baselines3 import DQN from stable_baselines3.common.evaluation import evaluate_policy import os models_dir = \"models/DQN\" if not os . path . exists ( models_dir ): os . makedirs ( models_dir ) logdir = \"logs\" if not os . path . exists ( logdir ): os . makedirs ( logdir ) env = gym . make ( \"MountainCar-v0\" ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) Inicializiramo u\u010denje agenta Podatki za inicializacijo so na spletni strani 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 policy_kwargs = dict ( net_arch = [ 256 , 256 ]) model = DQN ( 'MlpPolicy' , env = env , learning_rate = 4e-3 , batch_size = 128 , buffer_size = 10000 , learning_starts = 1000 , gamma = 0.99 , target_update_interval = 600 , train_freq = 16 , gradient_steps = 8 , exploration_fraction = 0.2 , exploration_final_eps = 0.07 , policy_kwargs = policy_kwargs , seed = 2 , tensorboard_log = logdir , verbose = 1 ) U\u010denje in shranjevanje 39 40 41 42 43 44 45 TIMESTEPS = 2000 iters = 0 while True : iters += 1 model . learn ( total_timesteps = TIMESTEPS , reset_num_timesteps = False , tb_log_name = \"DQN\" ) model . save ( f \" { models_dir } / { TIMESTEPS * iters } \" ) Uporaba logiranja za spremljanje u\u010denja tensorboard --logdir=logs python -m tensorboard.main --logdir=logs","title":"Shranjevanje agentov in logiranje"},{"location":"save_models/#logiranje-in-spremljanje-ucenja","text":"","title":"Logiranje in spremljanje u\u010denja"},{"location":"save_models/#python-skripta-za-testiranje-agenta","text":"load_carDQN2.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import gym from stable_baselines3 import DQN from stable_baselines3.common.evaluation import evaluate_policy import matplotlib.pyplot as plt import numpy as np env = gym . make ( \"MountainCar-v0\" ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) model = DQN . load ( \"dqn_car\" , env = env ) mean_reward , std_reward = evaluate_policy ( model , model . get_env (), n_eval_episodes = 10 ) print ( f 'Mean reward: { mean_reward } , Std reward: { std_reward } ' ) reward_list = [] ### for episode in range ( 50 ): ### state = env . reset () done = False print ( \"EPISODE \" , episode ) tot_reward , reward = 0 , 0 ### while not done : action , _state = model . predict ( state , deterministic = True ) state , reward , done , info = env . step ( action ) tot_reward += reward ### reward_list . append ( tot_reward ) ### env . close () f = plt . figure () plt . plot ( reward_list ) plt . xticks ( range ( 0 , len ( reward_list ), 2 ), range ( 1 , len ( reward_list ) + 1 , 2 )) plt . axhline ( y = - 200 , color = 'k' , linestyle = '--' ) plt . axhline ( y = np . average ( reward_list ), color = 'blue' , linestyle = '--' , label = 'povpre\u010dna nagrada' ) plt . annotate ( str ( np . average ( reward_list )), xy = ( - 2 , np . average ( reward_list ) + 0.7 ), color = 'blue' , fontsize = 13 , weight = 'bold' ) ax = plt . gca () ax . tick_params ( axis = \"both\" , labelsize = 12 ) f . legend ( loc = 'right' , fontsize = 13 ) plt . xlabel ( 'Epizoda' , fontsize = 14 ) plt . ylabel ( 'Nagrada' , fontsize = 14 ) plt . title ( 'Vrednost nagrade v posamezni epizodi' , fontsize = 16 ) #f.savefig('reward3.jpg') ### plt . show ()","title":"Python skripta za testiranje agenta"},{"location":"save_models/#logiranje-in-sprotno-shranjevanje-agentov","text":"Paket Tensorboard","title":"Logiranje in sprotno shranjevanje agentov"},{"location":"save_models/#prilagojena-python-skripta","text":"Pripravimo mape za shranjevanje modelov 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import gym from stable_baselines3 import DQN from stable_baselines3.common.evaluation import evaluate_policy import os models_dir = \"models/DQN\" if not os . path . exists ( models_dir ): os . makedirs ( models_dir ) logdir = \"logs\" if not os . path . exists ( logdir ): os . makedirs ( logdir ) env = gym . make ( \"MountainCar-v0\" ) print ( \"Actions = \" , env . action_space . n ) print ( \"Obs space high = \" , env . observation_space . high ) print ( \"Obs space low\" , env . observation_space . low ) Inicializiramo u\u010denje agenta Podatki za inicializacijo so na spletni strani 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 policy_kwargs = dict ( net_arch = [ 256 , 256 ]) model = DQN ( 'MlpPolicy' , env = env , learning_rate = 4e-3 , batch_size = 128 , buffer_size = 10000 , learning_starts = 1000 , gamma = 0.99 , target_update_interval = 600 , train_freq = 16 , gradient_steps = 8 , exploration_fraction = 0.2 , exploration_final_eps = 0.07 , policy_kwargs = policy_kwargs , seed = 2 , tensorboard_log = logdir , verbose = 1 ) U\u010denje in shranjevanje 39 40 41 42 43 44 45 TIMESTEPS = 2000 iters = 0 while True : iters += 1 model . learn ( total_timesteps = TIMESTEPS , reset_num_timesteps = False , tb_log_name = \"DQN\" ) model . save ( f \" { models_dir } / { TIMESTEPS * iters } \" )","title":"Prilagojena Python skripta"},{"location":"save_models/#uporaba-logiranja-za-spremljanje-ucenja","text":"tensorboard --logdir=logs python -m tensorboard.main --logdir=logs","title":"Uporaba logiranja za spremljanje u\u010denja"}]}