{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spodbujevalno u\u010denje","text":""},{"location":"#strojno-ucenje","title":"Strojno u\u010denje","text":""},{"location":"#spodbujevalno-ucenje_1","title":"Spodbujevalno u\u010denje","text":"<ul> <li>U\u010denje robotskih strategij v simuliranem okolju<ul> <li>OpenAI Gym + Stable Baselines3 </li> <li>Preprosta okolja</li> <li>AirHockey</li> </ul> </li> <li>Prenos na dejanskega robota</li> <li>Vodene + samostojne vaje</li> </ul> <ul> <li>Okolje<ul> <li>Stanje okolja</li> <li>Nagrada za agenta, za dolo\u010deno stanje</li> <li>Seznam mo\u017enih akcij</li> </ul> </li> <li>Agent<ul> <li>Strategija za izbiro najbolj\u0161ih akciji</li> <li>Izvaja in se odlo\u010da o najbolj\u0161i akciji</li> </ul> </li> </ul>"},{"location":"#okolje","title":"Okolje","text":"<ul> <li>Spodbujevalno u\u010denje zahteva veliko ponovitev<ul> <li>Zelo neprakti\u010dno za razvoj na robotu</li> <li>U\u010denje v simulaciji okolja -&gt; Env</li> <li>Zbirka okolij -&gt; Gym</li> </ul> </li> </ul> <p>OpenAI Gym okolja za izvajanje spodbujevalnega u\u010denja (https://gym.openai.com/)</p>"},{"location":"#programsko-okolje","title":"Programsko okolje","text":""},{"location":"#openai-gym","title":"OpenAI Gym","text":"<ul> <li>OpenAI Gym<ul> <li>Simulacij razli\u010dnih okolij</li> <li>Pred pripravljena okolja</li> <li>Struktura za razvoj lastnih okolij</li> </ul> </li> </ul> <p>https://gymnasium.farama.org/environments/toy_text/cliff_walking/</p> <p>https://gymnasium.farama.org/environments/mujoco/</p>"},{"location":"#stable-baselines3","title":"Stable Baselines3","text":"<ul> <li>Knji\u017enica za algoritme za spodbujevalno u\u010denje<ul> <li>Globoko spodbujevalno u\u010denje</li> </ul> </li> <li>Stable Baselines3 (SB3) algoritmi za spodbujevalno u\u010denje<ul> <li>https://stable-baselines3.readthedocs.io/en/master/#</li> <li>vsebuje \u0161e druga okolja v skladu s strukturo OpenAI Gym</li> </ul> </li> </ul>"},{"location":"#q-ucenje","title":"Q u\u010denje","text":""},{"location":"car/","title":"MountainCar primer","text":"<ul> <li>Cilj je priti iz doline do zastavice s prenihavanjem na klan\u010dinah,</li> <li>github povezava na py skripto za okolje</li> <li>Mountain Car</li> <li>Diskretne akcije<ul> <li>Potisk levo</li> <li>Potisk desno</li> <li>Brez potiska</li> </ul> </li> </ul> <ul> <li>Zvezna opazovanja<ul> <li>pozicija</li> <li>hitrost</li> <li>Za Q tabelo potrebujemo diskretna opazovanja!</li> </ul> </li> <li>Diskretizacija opazovanj na obmo\u010dja</li> <li>Nagrada: -1 za vsak korak</li> <li>Optimiziramo, da dose\u017ee gol v najkraj\u0161em \u010dasu</li> </ul>"},{"location":"car/#python-skripta","title":"Python skripta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 15000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n</code></pre> </li> <li> <p>Preverimo prostor akcij in opazovanja</p> <pre><code>print( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Diskretizacija opazovanj</p> <pre><code>#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n</code></pre> </li> <li> <p>Inicializacija Q tabele</p> <pre><code>#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\n</code></pre> </li> <li> <p>Funkcija, ki vrne indeks diskretnega stanja glede na vrednost opazovanj</p> <pre><code>def get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\n</code></pre> </li> <li> <p>Za\u010dnemo izvajati epizode u\u010denja</p> <pre><code>for episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\n</code></pre> </li> <li> <p>Akcija, korak, stanje, Q tabela</p> <ul> <li>Izberemo akcijo</li> <li>Izvedemo korak</li> <li>Dolo\u010dimo novo diskretno stanje</li> </ul> <pre><code>\u00a0 \u00a0 while not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\n</code></pre> <ul> <li>Posodobimo Q tabelo</li> <li>Posodobimo stanje</li> </ul> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 if not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state\n</code></pre> <ul> <li>Posodobimo epsilon</li> </ul> <pre><code>\u00a0 \u00a0 if END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> <ul> <li>Shranimo trenutno vrednost Q tabele</li> </ul> <pre><code>\u00a0 \u00a0 if episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\n</code></pre> <ul> <li>Po kon\u010danih epizodah shranimo kon\u010dno Q tabelo </li> </ul> <pre><code>np.save(f\"cart_e{episode}-qtable.npy\", q_table) \n</code></pre> </li> <li> <p>Celotna koda</p> </li> </ol> car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 15000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nfor episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\nwhile not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\nif not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state     \nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nif episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table) \n</code></pre>"},{"location":"carSB3Contrib/","title":"Uporaba drugih algoritmov za u\u010denje","text":"<ul> <li>Stable-Baselines3 vsebuje veliko implementacij algoritmov<ul> <li>RL Algorithms</li> </ul> </li> <li>Dodatni algoritmi so v SB3 Contrib<ul> <li>SB3 Contrib</li> </ul> </li> <li>Hiperparametri za posamezna Gym okolja in RL algoritme so v RL Baselines3 Zoo<ul> <li>RL Baselines3 Zoo hiperparametri</li> </ul> </li> </ul>"},{"location":"carSB3Contrib/#python-skripta-za-ucenje-agenta-z-qrdqn-algoritmom","title":"Python skripta za u\u010denje agenta z QRDQN algoritmom","text":"carQRDQN.py<pre><code>import gym\nfrom sb3_contrib import QRDQN\nimport os\nmodels_dir = \"models/QRDQN\"\nif not os.path.exists(models_dir):\nos.makedirs(models_dir)\nlogdir = \"logs\"\nif not os.path.exists(logdir):\nos.makedirs(logdir) \nenv = gym.make(\"MountainCar-v0\")\npolicy_kwargs = dict(net_arch=[256, 256], n_quantiles=25)\nmodel = QRDQN('MlpPolicy', \nenv=env, \ntensorboard_log=logdir,\nverbose=1,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.98,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs)\nTIMESTEPS = 2000\niters = 0\nwhile True:\niters += 1\nmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"QRDQN\")\nmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n</code></pre>"},{"location":"carSB3Contrib/#ppo-algoritem","title":"PPO algoritem","text":"<p>Primer 1 Primer 2</p>"},{"location":"car_DQN/","title":"U\u010denje z nevronskimi mre\u017eami z metodo DQN","text":"<ul> <li>U\u010denje z nevronskimi mre\u017eami z metodo DQN</li> <li>Uporabimo python paket Stable Baselines3 (SB3)</li> <li>Stable-Baselines3 Docs</li> <li>Stable Baselines3 omogo\u010da celo vrsto drugih algoritmov<ul> <li>A2C</li> <li>PPO</li> </ul> </li> <li>Raz\u0161iritev SB3 Contrib<ul> <li>dodatni sodobnej\u0161i algoritmi</li> <li>SB3 Contrib dokumentacije</li> <li>Github repozitorij</li> </ul> </li> </ul>"},{"location":"car_DQN/#python-skripta-za-ucenje-agenta","title":"Python skripta za u\u010denje agenta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <p><pre><code>import gym\nfrom stable_baselines3 import DQN \nenv = gym.make(\"MountainCar-v0\")\n</code></pre> 3. Preverimo prostor akcij in opazovanja</p> <pre><code>print( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Inicializiramo u\u010denje agenta</p> </li> <li> <p>Podatki za inicializacijo so na spletni strani ter na rl-baselines3-zoo</p> <pre><code>policy_kwargs = dict(net_arch=[256, 256])\nmodel = DQN('MlpPolicy', \nenv=env,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.99,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs,\nseed=2,\nverbose=1\n)\n</code></pre> </li> <li> <p>U\u010denje agenta</p> <pre><code>model.learn(total_timesteps=1.2e5)\n</code></pre> </li> <li> <p>Shranimo model</p> <pre><code>model.save(\"dqn_car\")\n</code></pre> </li> </ol>"},{"location":"car_DQN/#python-skripta-za-testiranje-agenta","title":"Python skripta za testiranje agenta","text":"<ol> <li>Nova python skripta</li> <li> <p>Incializacija okolja in u\u010denja</p> <pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Nalo\u017eimo in incializiramo agenta</p> <pre><code>model = DQN.load(\"dqn_car\", env=env)\n</code></pre> </li> <li> <p>Za\u017eenemo in testiramo agenta</p> <pre><code>mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\nprint(f'Mean reward: {mean_reward}, Std reward: {std_reward}')\nobs = env.reset()\nwhile True:\naction, _state = model.predict(obs, deterministic=True)\nobs, reward, done, info = env.step(action)\nenv.render()\nif done:\nobs = env.reset()\n</code></pre> </li> </ol>"},{"location":"car_action/","title":"Prikaz akcij glede na Q tabelo","text":"load_car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nimport sys ###\nimport matplotlib.pyplot as plt ###\nimport matplotlib.patches as mpatches ###\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\ndef get_actions(dataset):\nstolpec = 0\n#print(type(data))\nactions = np.ndarray([GRID_SIZE, GRID_SIZE])\nfor stolpec in range(GRID_SIZE):\nvrstica = 0\nfor vrstica in range(GRID_SIZE):\nif dataset[stolpec, vrstica, 0] == dataset[stolpec, vrstica, 1] == dataset[stolpec, vrstica, 2] == 0:\nactions[stolpec, vrstica] = -1\nelse:\nactions[stolpec, vrstica] = np.argmax(dataset[stolpec, vrstica])\nreturn actions\ndef plot_graphs(ep_list):\nep = 0\nfig,axs = plt.subplots(2,2,figsize = (15,15))\nfig.suptitle(\"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\", fontsize = 16)\nprint(enumerate(axs.flat))\nfor i, ax in enumerate(axs.flat):\ndata = np.load('cart_e'+str(ep_list[ep])+'-qtable.npy')\nnp.set_printoptions(threshold=sys.maxsize)\npositions = np.arange(-1.2,0.6+discrete_os_win_size[0],discrete_os_win_size[0])\nvelocities = np.arange(-0.07,0.07+discrete_os_win_size[1],discrete_os_win_size[1])\n#ax = fig.add_subplot(2,2)\nlabels = [\"Neobiskana stanja\",\"Premik levo\", \"Ne naredimo ni\u010desar\", \"Premik desno\"]\ncmap = plt.colormaps.get_cmap('Blues') #matplotlib.colormaps\nax = plt.subplot(2,2,i+1)\nactions = get_actions(data)\nax.pcolor(velocities,positions, actions, cmap = cmap)\nax.set_ylabel(\"Pozicija\", fontsize = 14)\nax.set_xlabel(\"Hitrost\", fontsize = 14)        \n#ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r')\n#ax.plot(velocities_0[0],positions_0[0],'ro')\nax.set_title('Epizoda '+str(ep_list[ep]+1), fontsize = 13)\nep += 1\nbound = np.linspace(0, 1, 5)\nprint(bound)\nfig.legend([mpatches.Patch(color=cmap(b)) for b in bound[:-1]],\n[labels[i] for i in range(4)], loc = 'upper right')\nplt.subplots_adjust(wspace=0.4,hspace=0.4)\nfig.savefig('Qtable.jpg') ###\n#plt.show()\nplot_graphs([0,5000,10000,14999])\n</code></pre>"},{"location":"car_reward/","title":"Izpis nagrade agenta med u\u010denjem","text":"car.py<pre><code>import gym\nimport numpy as np\nimport matplotlib.pyplot as plt ###\nenv = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\n#env = gym.make(\"CartPole-v1\")\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 25000\nSHOW_EVERY = 100\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nreward_list = [] ###\nave_reward_list = [] ###\nfor episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\ntot_reward, reward = 0, 0 ###\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\nwhile not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\nif not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state\ntot_reward += reward ###\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nif episode % SHOW_EVERY == 0:\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table)\nreward_list.append(tot_reward) ###\nif episode % SHOW_EVERY == 0: ###\nave_reward = np.mean(reward_list) ###\nave_reward_list.append(ave_reward) ###\nreward_list = [] ###\nprint('Episode {} Average Reward: {}'.format(episode, ave_reward)) ###\nnp.save(f\"cart_e{episode}-qtable.npy\", q_table) \n# Plot Rewards\nfig, ax = plt.subplots()\nax.plot(100 * (np.arange(len(ave_reward_list)) + 1), ave_reward_list) ###\nax.set_xlabel('Episodes') ###\nax.set_ylabel('Average Reward') ###\nax.set_title('Average Reward vs Episodes') ###\nfig.savefig('rewards.jpg') ###\nplt.show() ###\n</code></pre>"},{"location":"car_reward/#izpis-nagrade-za-vec-ponovitev","title":"Izpis nagrade za ve\u010d ponovitev","text":"load_car.py<pre><code>import gym\nimport numpy as np\nimport matplotlib.pyplot as plt ###\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e24999-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nreward_list = [] ###\nfor episode in range(50): ###\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\ndone = False\nprint(\"EPISODE \", episode)\ntot_reward, reward = 0, 0 ###\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\ntot_reward += reward ###\nreward_list.append(tot_reward) ###\nenv.close()\nf = plt.figure()\nplt.plot(reward_list)\nplt.xticks(range(0,len(reward_list),2),range(1,len(reward_list)+1,2))\nplt.axhline(y = -200, color = 'k', linestyle = '--')\nplt.axhline(y = np.average(reward_list), color = 'blue', linestyle = '--', label = 'povpre\u010dna nagrada')\nplt.annotate(str(np.average(reward_list)), xy= (-2,np.average(reward_list)+0.7), color = 'blue',fontsize = 13, weight = 'bold')\nax=plt.gca()\nax.tick_params(axis=\"both\", labelsize=12)\nf.legend(loc = 'right', fontsize = 13)\nplt.xlabel('Epizoda', fontsize=14)\nplt.ylabel('Nagrada', fontsize=14)\nplt.title('Vrednost nagrade v posamezni epizodi', fontsize=16)\nplt.show()\n</code></pre>"},{"location":"cart/","title":"Cartpole primer","text":"<ul> <li>github povezava na py skripto za okolje</li> <li> <p>Cart Pole</p> </li> <li> <p>Diskretne akcije (2 akciji)</p> </li> <li> <p>Zvezna opazovanja (4 spremenljivke)</p> <ul> <li>Nujna diskretizacija za uporabo Q u\u010denja</li> <li>Nekatera opazovanje imajo meje od -\u221e do +\u221e -&gt; dolo\u010ditev mej na roke</li> </ul> </li> <li> <p>Nagrada: +1 za vsak korak     -Optimizacija, da vodenje \u010dim dlje obdr\u017ei nihalo in vozi\u010dek pokonci in znotraj okolja</p> </li> </ul>"},{"location":"cart/#q-ucenje","title":"Q u\u010denje","text":""},{"location":"cart/#namigi","title":"Namigi","text":"<pre><code>obs_high = np.array([2.4, 3, 0.21, 3])\nobs_low = -obs_high \n</code></pre>"},{"location":"cart/#predvideni-rezultati","title":"Predvideni rezultati","text":""},{"location":"cart/#ucenje","title":"U\u010denje","text":""},{"location":"cart/#qtabela","title":"Qtabela","text":"load_cartpole.py<pre><code>import gym\nimport numpy as np\nimport sys ###\nimport matplotlib.pyplot as plt ###\nimport matplotlib.patches as mpatches ###\nenv = gym.make(\"CartPole-v1\") ####\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = np.array([2.4, 3, 0.21, 3]) ####\nobs_low = -obs_high\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\ndef get_actions(dataset):\nstolpec = 0\n#print(type(data))\nactions = np.ndarray([GRID_SIZE, GRID_SIZE])\nfor stolpec in range(GRID_SIZE):\nvrstica = 0\nfor vrstica in range(GRID_SIZE):\nif dataset[stolpec, vrstica, GRID_SIZE//2, GRID_SIZE//2, 0] == dataset[stolpec, vrstica, GRID_SIZE//2, GRID_SIZE//2, 1] ==  0: ###\nactions[stolpec, vrstica] = -1\nelse:\nactions[stolpec, vrstica] = np.argmax(dataset[stolpec, vrstica, GRID_SIZE//2, GRID_SIZE//2]) ###\nreturn actions\ndef plot_graphs(ep_list):\nep = 0\nfig,axs = plt.subplots(2,2,figsize = (15,15))\nfig.suptitle(\"Izbira akcije glede na Q tabelo za razli\u010dno \u0161tevilo epizod\", fontsize = 16)\nprint(enumerate(axs.flat))\nfor i, ax in enumerate(axs.flat):\ndata = np.load('cartpole_e'+str(ep_list[ep])+'-qtable.npy')\nnp.set_printoptions(threshold=sys.maxsize)\npositions = np.arange(-2.4,2.4+discrete_os_win_size[0],discrete_os_win_size[0]) ###\nvelocities = np.arange(-3,3+discrete_os_win_size[1],discrete_os_win_size[1]) ###\n#ax = fig.add_subplot(2,2)\nlabels = [\"Neobiskana stanja\", \"Premik levo\", \"Premik desno\"]\ncmap = plt.colormaps.get_cmap('Blues') #matplotlib.colormaps\nax = plt.subplot(2,2,i+1)\nactions = get_actions(data)\nax.pcolor(velocities, positions, actions, cmap = cmap)\nax.set_ylabel(\"Pozicija\", fontsize = 14)\nax.set_xlabel(\"Hitrost\", fontsize = 14)        \n#ax.hlines(y=0, xmin=-0.6, xmax=-0.4, linewidth=2, color='r')\n#ax.plot(velocities_0[0],positions_0[0],'ro')\nax.set_title('Epizoda '+str(ep_list[ep]+1), fontsize = 13)\nep += 1\nbound = np.linspace(0, 1, 4)\nprint(bound)\nfig.legend([mpatches.Patch(color=cmap(b)) for b in bound[:-1]],\n[labels[i] for i in range(3)], loc = 'upper right')\nplt.subplots_adjust(wspace=0.4,hspace=0.4)\nfig.savefig('Qtable.jpg') ###\nplt.show()\nplot_graphs([0,10000,20000,50000])\n</code></pre>"},{"location":"cartpole1/","title":"Cartpole1","text":"cartpole.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\n#env = gym.make(\"MountainCar-v0\")\n#env = gym.make(\"Acrobot-v1\")\nenv = gym.make(\"CartPole-v1\") ##########\nLEARNING_RATE = 0.1\nDISCOUNT = 0.95\nEPISODES = 15000\nSHOW_EVERY = 1000\nepsilon = 1.0 \nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = EPISODES//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = np.array([2.4, 3, 0.21, 3]) ####\nobs_low = -obs_high ####\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\n#q_table = np.random.uniform(low=-1, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\nq_table = np.zeros(DISCRETE_OS_SIZE + [env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nfor episode in range(EPISODES):\ndiscrete_state = get_discrete_state(env.reset())\ndone = False\nif episode % SHOW_EVERY == 0:\nrender = True\nprint(episode)\nelse:\nrender = False\nwhile not done:\nif np.random.random() &gt; epsilon:\naction = np.argmax(q_table[discrete_state])\nelse:\naction = np.random.randint(0, env.action_space.n)\nnew_state, reward, done, _ = env.step(action)\nnew_discrete_state = get_discrete_state(new_state)\nif episode % SHOW_EVERY == 0:\nenv.render()\nif not done:\nmax_future_q = np.max(q_table[new_discrete_state])\ncurrent_q = q_table[discrete_state + (action,)]\nnew_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\nq_table[discrete_state + (action,)] = new_q\ndiscrete_state = new_discrete_state     \nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nif episode % SHOW_EVERY == 0:\nnp.save(f\"cartpole_e{episode}-qtable.npy\", q_table) ###\nnp.save(f\"cartpole_e{episode}-qtable.npy\", q_table) ###\n</code></pre> load_cartpole.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"CartPole-v1\") ####\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = np.array([2.4, 3, 0.21, 3]) ####\nobs_low = -obs_high\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cartpole_e14999-qtable.npy\") #### \nprint( \"Q table size = \", q_table.shape)\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\nenv.close()\n</code></pre>"},{"location":"circ_env/","title":"Okolje Circle Envrionment","text":"<p>This Python code defines a custom environment for the OpenAI Gym toolkit. The environment is a rectangular, and an agent in the form of a circle can move within it. The agent is required to avoid colliding with the boundary of the world. The agent's movement and collision with the environment are simulated using the Pybox2D and Pygame libraries.</p> <p>The environment is defined in a Python class named <code>CircleEnvironment</code> that inherits from the <code>gym.Env</code> class. The class defines several functions, including the <code>__init__()</code>, <code>reset()</code>, <code>step()</code>, <code>render()</code>, and <code>close()</code> functions, that are part of the OpenAI Gym environment API.</p>"},{"location":"circ_env/#libraries-used","title":"Libraries Used","text":"<p>The code imports the following libraries:</p> <ul> <li><code>gym</code>: an open-source toolkit for developing and comparing reinforcement learning algorithms. OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It provides a set of environments for testing and training reinforcement learning agents, as well as standardized interfaces for interacting with those environments.</li> <li><code>numpy</code>: a library for numerical computing with Python.</li> <li><code>math</code>: a library for mathematical functions.</li> <li><code>pygame</code>: a library for game development with Python. Pygame is a cross-platform set of Python modules designed for writing video games. It includes functionality to handle graphics, sound, input devices, and networking. Pygame can be used to create simple 2D games or more complex games with physics and AI.</li> <li><code>Box2D</code>: a 2D physics engine for game development. Box2D is a 2D physics engine for simulations of physical systems. It can be used for simulations in games, robotics, computer vision, and other fields. It provides realistic simulation of collisions, forces, friction, and other physical interactions between objects in a 2D space.</li> </ul>"},{"location":"circ_env/#variables-used","title":"Variables Used","text":"<p>The environment uses the following variables:</p> <ul> <li><code>PPM</code>: the number of pixels per meter used for rendering the environment.</li> <li><code>TARGET_FPS</code>: the target frame rate used for rendering the environment.</li> <li><code>TIME_STEP</code>: the length of each simulation time step.</li> <li><code>WORLD_WIDTH</code>: the width of the circular world in simulation units [m].</li> <li><code>WORLD_HEIGHT</code>: the height of the circular world in simulation units [m].</li> <li><code>metadata</code>: a dictionary that contains metadata about the environment, including the available render modes and the target render frame rate.</li> <li><code>contact_listener</code>: an instance of a custom contact listener class that is used to detect collisions between the agent and the environment.</li> </ul>"},{"location":"circ_env/#standard-gym-functions-used","title":"Standard <code>gym</code> functions Used","text":"<p>The environment uses the following functions:</p> <ul> <li><code>__init__()</code>: the initialization function for the environment. It sets up the environment parameters, initializes the Pybox2D world, and sets the observation and action spaces.</li> <li><code>_get_obs()</code>: a function that returns the current observation of the agent, which is its position within the world.</li> <li><code>reset()</code>: a function that resets the environment to its initial state and returns the initial observation.</li> <li><code>step()</code>: a function that takes an action as input, simulates the environment for one time step, and returns the new observation, reward, done flag, and optional information about the simulation.</li> <li><code>render()</code>: a function that renders the environment using the specified render mode.</li> <li><code>_render_frame()</code>: a function that renders a single frame of the environment.</li> <li><code>close()</code>: a function that destroys the Pybox2D world and the agent, and closes the Pygame window.</li> </ul>"},{"location":"circ_env/#custom-functions","title":"Custom functions","text":"<ul> <li> <p><code>_is_collision(object, goal)</code>: Checks if there is a collision between the body object and the body goal. It takes in two b2Body objects, object and goal, and returns a boolean indicating whether there is a collision. It can take any two <code>Box2D</code> b2Body objects to check collison between them and not just body object and the body goal.</p> </li> <li> <p><code>create_agent(radius_px, friction)</code>: Creates a new b2Body object representing the agent. It takes in the radius of the agent in pixels, radius_px, and a friction value, friction, for the agent's fixtures.</p> </li> <li> <p><code>create_puck(radius_px, friction, type)</code>: Creates a new b2Body object representing the puck. It takes in the radius of the puck in pixels, radius_px, a friction value, friction, for the puck's fixtures, and a string type that indicates whether the puck should be a dynamic object ('d') or a kinematic object ('k').</p> </li> <li> <p><code>create_goal(dim_px, position)</code>: Creates a new b2Body object representing the goal. It takes in a tuple dim_px representing the dimensions of the goal in pixels, and a tuple position representing the position of the goal in the simulation.</p> </li> <li> <p><code>create_circ_target(radius_px)</code>: Creates a new circular target. It takes in the radius of the target in pixels, radius_px.</p> </li> <li> <p><code>create_random_object(radius_px)</code>: Creates a new random object with a random shape. It takes in the radius of the object in pixels, radius_px..</p> </li> <li> <p><code>create_border()</code>: Creates the boundary of the simulation.</p> </li> <li> <p><code>get_agent_position()</code>: Returns the current position of the agent as a numpy array.</p> </li> <li> <p><code>get_puck_position()</code>: Returns the current position of the puck as a numpy array.</p> </li> <li> <p><code>get_puck_velocity()</code>: Returns the current velocity of the puck as a numpy array.</p> </li> <li> <p><code>get_target_position()</code>: Returns the current position of the target as a numpy array.</p> </li> <li> <p><code>reset_agent(position)</code>: Resets the position and velocity of the agent to a specified position.</p> </li> <li> <p><code>reset_puck(position)</code>: Resets the position and velocity of the puck to a specified position.</p> </li> <li> <p><code>reset_target(position)</code>: Resets the position of the target to a specified position.</p> </li> <li> <p><code>reset_random_object(position)</code>: Resets the position and velocity of the random object to a specified position.</p> </li> <li> <p><code>set_agent_velocity(vel)</code>: This function sets the linear and angular velocity of the agent in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the agent.</p> </li> <li> <p><code>set_puck_velocity(vel)</code>: This function sets the linear and angular velocity of the puck in the Box2D world. The input parameter vel is a tuple containing the x and y components of the velocity. The velocity is converted to a b2Vec2 object before being assigned to the puck.</p> </li> <li> <p><code>draw_agent(color)</code>: This function draws the agent as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the agent is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_puck(color)</code>: This function draws the puck as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the puck is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_border(color)</code>: This function draws the border of the game as a rectangle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color.</p> </li> <li> <p><code>draw_target(color)</code>: This function draws the target as a circle on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position of the target is converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>draw_goal(color)</code>: This function draws the goal as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color.</p> </li> <li> <p><code>draw_random_object(color)</code>: This function draws a random object as a polygon on the Pygame screen. The input parameter color is a tuple containing the RGB values of the color. The position, angle and vertices of the object are converted to Pygame coordinates before drawing.</p> </li> <li> <p><code>is_agent_outside_screen()</code>: This function checks if the agent is outside the screen or not. It returns True if the agent is outside the screen, otherwise it returns False.</p> </li> <li> <p><code>calc_distance(pos1, pos2)</code>: This function calculates the Euclidean distance between two points. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points.</p> </li> <li> <p><code>unit_vector(pos1, pos2)</code>: This function calculates the unit vector pointing from pos1 to pos2. The input parameters pos1 and pos2 are tuples containing the x and y coordinates of the points. The function returns the unit vector as a numpy array.</p> </li> <li> <p><code>calculate_scaled_component(pos_agent, pos_target, vel)</code>: This function calculates the scaled component of the velocity vector in the direction of the unit vector pointing from pos_agent to pos_target. The input parameters pos_agent and pos_target are tuples containing the x and y coordinates of the points. The input parameter vel is a tuple containing the x and y components of the velocity vector. The function returns the scaled component as a float value.</p> </li> </ul>"},{"location":"circ_env/#koda","title":"Koda","text":""},{"location":"circ_env/#premikanje-agenta-s-tipkovnico","title":"Premikanje agenta s tipkovnico","text":""},{"location":"circ_env/#ustvarimo-agenta-dolocimo-observation_space-in-action_space","title":"Ustvarimo agenta, dolo\u010dimo <code>observation_space</code> in <code>action_space</code>.","text":"<ul> <li><code>__init__()</code> <pre><code>        self.observation_space = spaces.Box(low=np.array([0.0, 0.0]), high=np.array([self.width, self.height]), dtype=np.float32)\nself.action_space = spaces.Box(low=-2.0, high=2.0, shape=(2,), dtype=np.float32)\nself.create_agent(agent_radius_px, 0.1)        \n</code></pre></li> </ul>"},{"location":"circ_env/#potrebujemo-vsaj-en-podatek-za-observacijo","title":"Potrebujemo vsaj en podatek za observacijo.","text":"<ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        agent_pos = self.get_agent_position()\nreturn agent_pos\n</code></pre>"},{"location":"circ_env/#ponastavimo-agenta-v-reset-funkciji","title":"Ponastavimo agenta v <code>reset()</code> funkciji.","text":"<ul> <li><code>reset()</code></li> </ul> <pre><code>        self.reset_agent((np.random.uniform(self.agent_radius*1.5, self.width - self.agent_radius*1.5), np.random.uniform(self.agent_radius*1.5, self.height - self.agent_radius*1.5)))\n</code></pre>"},{"location":"circ_env/#naredimo-korak-v-step-funkciji","title":"Naredimo korak v <code>step()</code> funkciji.","text":"<ul> <li><code>step()</code></li> </ul> <p>Premik agenta za <code>action</code></p> <pre><code>        self.set_agent_velocity(action)\n</code></pre> <p>Dolo\u010dimo <code>obs</code> spremenljivko.</p> <pre><code>        obs = self._get_obs()\n</code></pre> <p>Dolo\u010dimo, <code>reward</code> in <code>done</code> spremenljivki. Za za\u010detek inicializacija:</p> <pre><code>        reward = 0.0\ndone = False \n</code></pre> <p>Nato lahko dolo\u010dimo osnovne pogoje:</p> <pre><code>        if self.current_step &gt;= self.time_steps:\n#reward = -1.0\ndone = True\n</code></pre>"},{"location":"circ_env/#izrisemo-agenta","title":"izri\u0161emo agenta","text":"<ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        self.draw_agent(green)\n</code></pre>"},{"location":"circ_env/#na-koncu-izbrisemo-agenta-v-close-funkciji","title":"Na koncu izbri\u0161emo agenta v <code>close()</code> funkciji","text":"<ul> <li><code>close()</code></li> </ul> <pre><code>        self.world.DestroyBody(self.agent)\n</code></pre>"},{"location":"circ_env/#zazenemo-okolje","title":"Za\u017eenemo okolje","text":"<p>Za\u017eeni skripto <code>playCirc.py</code></p> <pre><code>python3 playCirc.py\n</code></pre> <p>V skripti pove\u010dajte hitrost na <code>5</code>.</p> playCirc.py<pre><code>while True:\nif any([event.type == pygame.QUIT for event in pygame.event.get()]): break\n#player controls\nkeys = pygame.key.get_pressed() \nif keys[pygame.K_LEFT]: x = -1\nelif keys[pygame.K_RIGHT]: x = 1\nelse: x = 0\nif keys[pygame.K_UP]: y = -1\nelif keys[pygame.K_DOWN]: y = 1\nelse: y = 0   \naction = np.array([x,y],dtype=np.float32)\n#action = env.action_space.sample()\n#print(action)\nobs, reward, done, _ = env.step(action)\n# Render the environment\nenv.render()\n# Check if the episode is finished\nif done:\nobs = env.reset()\n# Close the environment\nenv.close()\n</code></pre>"},{"location":"circ_env/#dodajmo-osnovno-mejo","title":"Dodajmo osnovno mejo","text":"<ul> <li><code>__init__()</code></li> </ul> <p><pre><code>        self.create_border()    \n</code></pre> - <code>_render_frame()</code></p> <pre><code>        self.draw_border(black)\n</code></pre> <p>Zaklju\u010dimo episodo, ko se agent dotakne meje.</p> <ul> <li><code>step()</code></li> </ul> <pre><code>        if self._is_collision(self.agent, self.border):\nreward = -1.0\ndone = True  \n</code></pre>"},{"location":"circ_env_obj_gol/","title":"Pomik naklju\u010dnega objekta v gol","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\nagent_radius_px = 30\nobject_radius_px = 30\nself.max_puck_vel = 5.0\nself.max_agent_vel = 2.0\nself.time_steps = 500\nself.observation_space = spaces.Box(low=np.array([0.0, 0.0, 0.0, 0.0]), high=np.array([self.width, self.height, self.width, self.height]), dtype=np.float32)\nself.action_space = spaces.Box(low=-self.max_agent_vel, high=self.max_agent_vel, shape=(2,), dtype=np.float32)\nself.create_agent(agent_radius_px, 0.1) \nself.create_border()\n#self.create_circ_target(25)\n#self.create_puck(object_radius_px, 0.5, 'k')\nself.create_random_object(object_radius_px)\nself.create_goal((400, 10), (self.width/2, 10/self.PPM))\n############ DO TUKAJ SPREMINJATE\n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        agent_pos = self.get_agent_position()\n#target_pos = self.get_target_position()\npak_pos = self.get_puck_position()\n#pak_vel = self.get_puck_velocity()\nreturn np.concatenate((agent_pos, pak_pos))\n</code></pre> <ul> <li><code>reset()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\nself.reset_agent((np.random.uniform(self.agent_radius*1.5, self.width - self.agent_radius*1.5), np.random.uniform(self.agent_radius*1.5, self.height - self.agent_radius*1.5)))\n#self.reset_target((np.random.uniform(self.agent_radius*2, self.width - self.agent_radius*2), np.random.uniform(self.agent_radius*2, self.height - self.agent_radius*2)))\n#self.reset_puck((np.random.uniform(self.object_radius+self.agent_radius*3, self.width - self.object_radius-self.agent_radius*3), np.random.uniform(self.object_radius+self.agent_radius*3, self.height - self.object_radius-self.agent_radius*3)))\nself.reset_random_object((np.random.uniform(self.object_radius+self.agent_radius*3, self.width - self.object_radius-self.agent_radius*3), np.random.uniform(self.object_radius+self.agent_radius*3, self.height - self.object_radius-self.agent_radius*3)))\n############ DO TUKAJ SPREMINJATE\n</code></pre> <ul> <li><code>step()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\nself.set_agent_velocity(action)  \nobs = self._get_obs() \nself.object.ApplyAngularImpulse(-0.25*self.object.inertia*self.object.angularVelocity, True)\nself.object.ApplyForce(-1*self.object.linearVelocity, self.object.worldCenter, True)        \nreward = 0.0\ndone = False \n#if self.calc_distance(self.get_puck_position(), self.get_target_position()) &lt; self.target_radius:\n#    reward = 1.0\n#    done = True\nif self._is_collision(self.object, self.goal):\nreward = 1.0\ndone = True        \nif self._is_collision(self.agent, self.object):\nreward = 0.01        \nif self._is_collision(self.object, self.border):\nreward = -0.01               \nif self.current_step &gt;= self.time_steps:\nreward = -1.0\ndone = True\nif self._is_collision(self.agent, self.border):\nreward = -1.0\ndone = True  \n############ DO TUKAJ SPREMINJATE\n</code></pre> <ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\nself.draw_agent(green)\nself.draw_border(black)\n#self.draw_target(blue)\nself.draw_goal(blue)\n#self.draw_puck(yellow)\nself.draw_random_object(yellow)\n############ DO TUKAJ SPREMINJATE  \n</code></pre>"},{"location":"circ_env_pak/","title":"Dotik s pakom","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        self.create_puck(object_radius_px, 0.5, 'k')\n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        pak_pos = self.get_puck_position()\nreturn np.concatenate((agent_pos, pak_pos))\n</code></pre> <ul> <li><code>reset()</code></li> </ul> <pre><code>        self.reset_puck((np.random.uniform(self.object_radius+self.agent_radius*3, self.width - self.object_radius-self.agent_radius*3), np.random.uniform(self.object_radius+self.agent_radius*3, self.height - self.object_radius-self.agent_radius*3)))\n</code></pre> <ul> <li><code>step()</code></li> </ul> <pre><code>        if self._is_collision(self.agent, self.object):\nreward = 1.0\ndone = True \n</code></pre> <ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        self.draw_puck(yellow)\n</code></pre>"},{"location":"circ_env_pak_gol/","title":"Pomik paka v gol","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        self.max_puck_vel = 5.0\nself.observation_space = spaces.Box(low=np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -self.max_puck_vel, -self.max_puck_vel]), high=np.array([self.width, self.height, self.width, self.height, self.width, self.height, self.max_puck_vel, self.max_puck_vel]), dtype=np.float32)\n#self.create_circ_target(25)\nself.create_goal((400, 10), (self.width/2, 10/self.PPM))\n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        pak_vel = self.get_puck_velocity()\nreturn np.concatenate((agent_pos, pak_pos, pak_vel))\n</code></pre> <ul> <li><code>reset()</code></li> </ul> <pre><code>        #self.reset_target((np.random.uniform(self.agent_radius*2, self.width - self.agent_radius*2), np.random.uniform(self.agent_radius*2, self.height - self.agent_radius*2)))\n</code></pre> <ul> <li><code>step()</code></li> </ul> <pre><code>        self.limit_puck_velocity(self.max_puck_vel)\n#if self.calc_distance(self.get_puck_position(), self.get_target_position()) &lt; self.target_radius:\n#    reward = 1.0\n#    done = True\nif self._is_collision(self.object, self.goal):\nreward = 1.0\ndone = True    \n</code></pre> <ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        #self.draw_target(blue)\nself.draw_goal(blue)  \n</code></pre>"},{"location":"circ_env_pak_tocka/","title":"Pomik paka v to\u010dko","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        self.observation_space = spaces.Box(low=np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), high=np.array([self.width, self.height, self.width, self.height, self.width, self.height]), dtype=np.float32)\n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        return np.concatenate((agent_pos, target_pos, pak_pos))\n</code></pre> <ul> <li><code>step()</code></li> </ul> <p>Po spodnjem algoritmu napi\u0161ite kodo za izra\u010dun hitrosti paka.</p> <p></p> <p>Razlaga algoritma 1. Nastavite K na 2000 in B na 0.5 2. Nastavite m na 1 3. Pridobite polo\u017eaj agenta in ga shranite v pos_a 4. Pridobite polo\u017eaj paka in ga shranite v pos_p 5. Izra\u010dunajte razdaljo med agentom in pakom kot (agent_radius+object_radius) - calc_distance(pos_a, pos_p) in jo shranite v dist 6. Ustvari spremenljivko FK z ni\u010dlami z enakimi dimenzijami kot pos_p 7. \u010ce je dist &gt; 0, potem nastavite  8. smer kot enotski vektor razlike med pos_a in pos_p in  9. nastavite FK kot K krat dist krat smer 10. Pridobite hitrost plo\u0161\u010dka in jo shranite v vel_p 11. Nastavite FB kot -B krat vel_p 12. Ustvari spremenljivko F z ni\u010del z enakimi dimenzijami kot pos_p 13. Ustvari spremenljivko acc z ni\u010dlami, ki ima enake dimenzije kot pos_p 14. Nastavite F kot vsoto FK in FB 15. Nastavite acc kot F, deljeno z m 16. Posodobite hitrost plo\u0161\u010dka tako, da jo nastavite na vel_p plus acc krat TIME_STEP</p> <p></p> <p></p> <p>Koda <pre><code>        K, B = 2000, 0.5\nm = 1\npos_a = self.get_agent_position()\npos_p = self.get_puck_position()\ndist = (self.agent_radius+self.object_radius)-self.calc_distance(pos_a, pos_p)\nFK = np.zeros_like(pos_p)\nif (dist &gt; 0):\ndirection = self.unit_vector(pos_a, pos_p)\nFK = K*dist*direction\nvel_p = self.get_puck_velocity()\nFB = -B*vel_p\nF = np.zeros_like(pos_p)\nacc = np.zeros_like(pos_p)\nF = FK+FB\nacc = F/m\nself.set_puck_velocity(vel_p+acc*TIME_STEP)\n</code></pre></p> <pre><code>        if self.calc_distance(self.get_puck_position(), self.get_target_position()) &lt; self.target_radius:\nreward = 1.0\ndone = True\n#if self._is_collision(self.agent, self.object):\n#    reward = 1.0\n#    done = True\n</code></pre>"},{"location":"circ_env_tocka/","title":"Premik v to\u010dko","text":"<ul> <li><code>__init__()</code></li> </ul> <pre><code>        self.observation_space = spaces.Box(low=np.array([0.0, 0.0, 0.0, 0.0]), high=np.array([self.width, self.height, self.width, self.height]), dtype=np.float32)\nself.create_circ_target(5)    \n</code></pre> <ul> <li><code>_get_obs()</code></li> </ul> <pre><code>        agent_pos = self.get_agent_position()\ntarget_pos = self.get_target_position()\nreturn np.concatenate((agent_pos, target_pos))\n</code></pre> <ul> <li><code>reset()</code></li> </ul> <pre><code>        self.reset_target((np.random.uniform(self.agent_radius*2, self.width - self.agent_radius*2), np.random.uniform(self.agent_radius*2, self.height - self.agent_radius*2)))\n</code></pre> <ul> <li><code>step()</code></li> </ul> <pre><code>        if self.calc_distance(self.get_agent_position(), self.get_target_position()) &lt; self.target_radius:\nreward = 1.0\ndone = True\nif self.current_step &gt;= self.time_steps:\nreward = -1.0\ndone = True\n</code></pre> <ul> <li><code>_render_frame()</code></li> </ul> <pre><code>        self.draw_target(blue)\n</code></pre>"},{"location":"circ_env_train/","title":"U\u010denje s spodbujevalnim u\u010denjem","text":""},{"location":"circ_env_train/#skripta-za-ucenje-train_circpy","title":"Skripta za u\u010denje <code>train_circ.py</code>","text":"train_circ.py<pre><code>import gym\nimport numpy as np\nfrom sb3_contrib import TQC\n#from stable_baselines3 import PPO\nimport os\nimport circ_env\nenv = gym.make('circ_env/Circle-v0')\n# tensorboard logiranje\nmodels_dir = \"models/TQC01\"\nif not os.path.exists(models_dir):\nos.makedirs(models_dir)\nlogdir = \"logs\"\nif not os.path.exists(logdir):  \nos.makedirs(logdir)\n# deklaracija modela\npolicy_kwargs = dict(n_critics=2, n_quantiles=25)\nmodel = TQC('MlpPolicy', env=env, tensorboard_log=logdir, verbose=1,policy_kwargs=policy_kwargs)\n# Reset the environment\nobs = env.reset()\n# iteracija skozi u\u010denje in shranjevanje modela\nTIMESTEPS = 10000\niters = 0\nwhile True:\niters += 1\nmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False,tb_log_name=\"TQC_01\")\nmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n</code></pre>"},{"location":"circ_env_train/#zagon-ucenja-in-logiranja","title":"Zagon u\u010denja in logiranja","text":"<ul> <li> <p>Terminal 1 <pre><code>python3 train_circ.py\n</code></pre></p> </li> <li> <p>Terminal 2 <pre><code>tensorboard --logdir=\"logs\"\n</code></pre></p> </li> </ul>"},{"location":"circ_env_train/#zagon-naucenega-agenta-load_circ_modelpy","title":"Zagon nau\u010denega agenta <code>load_circ_model.py</code>","text":"load_circ_model.py<pre><code>import gym\nimport numpy as np\nfrom sb3_contrib import TQC\n#from stable_baselines3 import PPO\nimport os\nimport circ_env\nenv = gym.make('circ_env/Circle-v0', render_mode=\"human\")\nmodel = TQC.load(\"./models/TQC07/2260000\", env=env)\n# Reset the environment\nobs = env.reset()\nEPISODES = 1000\nfor episode in range(EPISODES):\nobs = env.reset()\ndone = False\nwhile not done:\naction, _state = model.predict(obs, deterministic=True)\nobs, reward, done, _ = env.step(action)\nenv.render()\n# Close the environment\nenv.close()\n</code></pre>"},{"location":"cliff/","title":"Primer okolja Cliff Walking","text":"<p>Open AI Cliff Walking</p> <ol> <li> <p>Za\u010dnemo z novo python skripto</p> </li> <li> <p>Uvoz potrebnih paketov</p> <pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\n</code></pre> </li> <li> <p>Deklaracija okolja</p> <ul> <li>your_env = gym.make(\"YourEnv \", some_kwarg=your_vars)</li> <li> <p>your_env = gym.make(\"YourEnv\")</p> </li> <li> <p>Seznam okolij     https://gymnasium.farama.org/environments/toy_text/</p> </li> </ul> <pre><code>env = gym.make('CliffWalking-v0')\n</code></pre> </li> <li> <p>Resetiramo in prika\u017eemo okolje</p> <pre><code>env.reset()\nenv.render()\n</code></pre> </li> <li> <p>Prika\u017eemo nekaj parametrov</p> <pre><code>print( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\n</code></pre> </li> <li> <p>Deklariramo Q tabelo</p> <pre><code>q_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\n</code></pre> </li> <li> <p>Deklariramo parametre</p> <pre><code>learning_rate = 0.1\ndiscount_factor = 0.95\nepochs = 60000\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\n</code></pre> </li> <li> <p>Za\u010dnemo z u\u010denjem z izbranim \u0161tevilom epoh</p> <pre><code>for episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\n</code></pre> </li> <li> <p>Izvedemo posamezni \u201esprehod \u010dez okolje\u201c z izbiro akcij</p> <ul> <li>raziskovanje: naklju\u010dna akcija</li> <li>uporabo zbranega znanja: akcija z maximalno q vrednostjo</li> </ul> <pre><code>\u00a0 \u00a0 while not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Use the action with the highest q-value\naction = np.argmax(q_table[state]) \n</code></pre> </li> <li> <p>Izvedemo akcijo v okolju in preberemo vrednosti novega stanja in nagrade</p> <pre><code>        next_state, reward, done, info = env.step(action)\n</code></pre> </li> <li> <p>Posodobimo stanje Q tabele</p> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 curr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\n</code></pre> </li> <li> <p>Posodobimo stanje</p> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 state = next_state\n</code></pre> </li> <li> <p>Shranimo dol\u017eino trenutnega \u201esprehoda\u201c</p> <pre><code>        if episode % SHOW_EVERY == 0:\ntrial_length += 1\n</code></pre> </li> <li> <p>Celotna koda u\u010denja do sedaj</p> <pre><code>for episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> </li> <li> <p>Testiramo nau\u010deno Q tabelo oziroma agenta</p> <ul> <li>Resetiramo okolje in ga izri\u0161emo</li> </ul> <pre><code>print(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\n</code></pre> <ul> <li>Izvedemo sprehod</li> </ul> <pre><code>while not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre> </li> <li> <p>Ve\u010d testov, da vidimo uspe\u0161nost</p> <pre><code>lengths=[]\nfor trialnum in range(1, 11):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done and trial_length &lt; 25:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\nprint(\"Trial number \" + str(trialnum) + \" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\ntrial_length += 1\nlengths.append(trial_length)\nsleep(.2)\navg_len=sum(lengths)/10\nprint(avg_len)\n</code></pre> </li> <li> <p>Celotna koda</p> cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\nfor episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\nwhile not done:\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nprint(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\nwhile not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre> </li> <li> <p>Zagon skripte</p> </li> </ol> <p>Ob zagonu skripte s <code>python cliff.py</code> se bo pojavila napaka</p> <pre><code>/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py:44: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n  self._cliff = np.zeros(self.shape, dtype=np.bool)\nTraceback (most recent call last):\n  File \"/home/student/RL/rl_pet/cliff.py\", line 7, in &lt;module&gt;\n    env = gym.make('CliffWalking-v0')\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 235, in make\n    return registry.make(id, **kwargs)\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 129, in make\n    env = spec.make(**kwargs)\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/registration.py\", line 90, in make\n    env = cls(**_kwargs)\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py\", line 44, in __init__\n    self._cliff = np.zeros(self.shape, dtype=np.bool)\n  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/numpy/__init__.py\", line 305, in __getattr__\n    raise AttributeError(__former_attrs__[attr])\nAttributeError: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n</code></pre> <p>Najbolj informativen del so vrstice</p> <pre><code>  File \"/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py\", line 44, in __init__\n    self._cliff = np.zeros(self.shape, dtype=np.bool)\n</code></pre> <p>in</p> <pre><code>AttributeError: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n</code></pre> <p>Napaka nastane, ker je v <code>cliffwalking.py</code> napaka v vrstici 44, ker se uporablja star zapis za spremenljivko <code>bool</code> in sicer zapis <code>dtype=np.bool</code>. Tega je potrebno spremeniti v samo <code>bool</code>. Potrebno je odpreti datoteko, ki se nahaja npr. v <code>/home/student/anaconda3/envs/rl_pet/lib/python3.9/site-packages/gym/envs/toy_text/cliffwalking.py</code>. Polna pot je odvisna od ime va\u0161ega python okolja, npr. <code>/rl_pet</code>. Odprite python datoteko in spremenite kodo.</p> <p>Stara koda cliffwalking.py<pre><code>        # Cliff Location\nself._cliff = np.zeros(self.shape, dtype=np.bool)\nself._cliff[3, 1:-1] = True\n</code></pre></p> <p>Nova koda cliffwalking.py<pre><code>        # Cliff Location\nself._cliff = np.zeros(self.shape, bool)\nself._cliff[3, 1:-1] = True\n</code></pre></p> <ol> <li>Celotna koda verzija 2</li> </ol> <p>V tej verziji kode je dodana pred\u010dasna zaustevative u\u010denja za posamezno epoho, \u010de u\u010denje prese\u017ee 99 korakov u\u010denja v posamezni epohi.</p> cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nmax_steps = 99\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\nSHOW_EVERY = 1000\nfor episode in range(epochs):\nstate = env.reset()\ndone = False\ntrial_length = 0\n#while not done:\nfor step in range(max_steps):\nif (random.uniform(0, 1) &lt; epsilon): # Exploration with random action\naction = env.action_space.sample()\nelse: # Use the action with the highest q-value\naction = np.argmax(q_table[state]) \nnext_state, reward, done, info = env.step(action)\ncurr_q = q_table[state, action]\nnext_max_q = np.max(q_table[next_state])\nnew_q = (1 - learning_rate) * curr_q + learning_rate * (reward + discount_factor * next_max_q)\nq_table[state, action] = new_q\nstate = next_state\nif episode % SHOW_EVERY == 0:\ntrial_length += 1\nif done:\nbreak\nif episode % SHOW_EVERY == 0:\nprint(f'Episode: {episode:&gt;5d}, episode length: {int(trial_length):&gt;5d}')\nif END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\nprint(q_table)\nstate = env.reset()\nenv.render()\ndone = False\ntrial_length = 0\nwhile not done:\naction = np.argmax(q_table[state])\nstate, reward, done, info = env.step(action)\ntrial_length += 1\nprint(\" Step \" + str(trial_length))\nenv.render()\nsleep(.2)\n</code></pre>"},{"location":"cliff2/","title":"Prikaz Q tabele","text":""},{"location":"cliff2/#prikaz-okolja","title":"Prikaz okolja","text":""},{"location":"cliff2/#tekstovni-izpis-okolja","title":"Tekstovni izpis okolja","text":"<pre><code>env.render()\n</code></pre>"},{"location":"cliff2/#graficni-prikaz-okolja","title":"Grafi\u010dni prikaz okolja","text":"<p>Okolje bomo prikazali grafi\u010dno z matplotlib knji\u017enico.</p> <p><pre><code>import matplotlib.pyplot as plt\n</code></pre> Nato na konec datoteke dodamo kodo:</p> <pre><code>fig1, ax1 = plt.subplots()\nax1.axis('off')\nax1.axis('tight')\nokolje = [[\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\"], [\"S\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"G\"]]\nokolje_colours = np.asarray(okolje,dtype='U25')\nokolje_rows = len(okolje[0][:])\nokolje_columns = len(okolje[:][0])\nokolje_colours[okolje_colours == \"x\"] = \"firebrick\"\nokolje_colours[okolje_colours == \"G\"] = \"gold\"\nokolje_colours[okolje_colours == \"S\"] = \"limegreen\"\nokolje_colours[okolje_colours == \"o\"] = \"cornflowerblue\"\nokolje = [[\"o,1\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,12\"], [\"o,13\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,24\"], [\"o,25\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o\",\"o,36\"], [\"S,37\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"x\",\"G,48\"]]\ntable_okolje = ax1.table(cellText = okolje,cellColours=okolje_colours, loc = 'center', cellLoc = 'center', rowLoc = 'center',colLoc = 'center')\ntable_okolje.scale(1,1.5)\ntable_okolje.auto_set_font_size(True)\ntable_okolje.set_fontsize(10)\nplt.show()\n</code></pre>"},{"location":"cliff2/#tekstovni-izpis-q-tabele","title":"Tekstovni izpis Q tabele","text":"<pre><code>print(q_table)\n</code></pre>"},{"location":"cliff2/#graficni-izpis-q-tabele","title":"Grafi\u010dni izpis Q tabele","text":"<pre><code>fig2, ax2 = plt.subplots()\nfig2.patch.set_visible(False)\nax2.axis('off')\ncolumns = [\"GOR\",\"DESNO\",\"DOL\",\"LEVO\"]\nrows = [\"STANJE %d\" %(i+1) for i in range(env.observation_space.n)]\nqtable = np.around(qtable,3)\nqtable_s = qtable[:][0:22]\nrows_s = rows[0:22]\nnorm = plt.Normalize(qtable_s.min(), qtable_s.max()+0.1)\ncolours = plt.cm.YlGn(norm(qtable_s))\ntable = ax2.table(cellText=qtable_s, rowLabels=rows_s, colLabels=columns, loc = 'center', cellColours=colours,cellLoc ='center',rowLoc='center', colLoc ='center',colWidths=[0.1,0.1,0.1,0.1,0.1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(8)\nfig3, ax3 = plt.subplots()\nfig3.patch.set_visible(False)\nax3.axis('off')\nqtable_s = qtable[:][23:48]\nrows_s = rows[23:48]\ncolours = plt.cm.YlGn(norm(qtable_s))\ntable = ax3.table(cellText=qtable_s, rowLabels=rows_s, colLabels=columns, loc = 'center', cellColours=colours,cellLoc ='center',rowLoc='center', colLoc ='center',colWidths=[0.1,0.1,0.1,0.1,0.1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(8)\nplt.show()\n</code></pre>"},{"location":"cliff3/","title":"Eksploracija","text":""},{"location":"cliff3/#vizualizacija-funkcije-za-epsilon","title":"Vizualizacija funkcije za epsilon","text":"epsilon.py<pre><code>import matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider, Button\nimport numpy as np\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.01            # Minimum exploration probability\ndecay_rate = 0.001            # Exponential decay rate for exploration prob\ntotal_episodes = 10000\ndef plot_fcn(min_epsilon_fcn, max_epsilon_fcn, decay_rate_fcn, total_episodes_fcn):\nepsilon_values = [(min_epsilon_fcn + (max_epsilon_fcn - min_epsilon_fcn) * np.exp(-decay_rate_fcn * episode)) for episode in range(total_episodes_fcn)]\nep_num = range(total_episodes_fcn)\nreturn epsilon_values\n#define inital values for sliders\ninit_min_epsilon = min_epsilon\ninit_max_epsilon = max_epsilon\ninit_decay_rate = decay_rate\ninit_total_episodes = total_episodes\nfig1,ax1 = plt.subplots()\nline, = plt.plot(plot_fcn(init_min_epsilon,init_max_epsilon,init_decay_rate,init_total_episodes), lw=2)\nax1.set_xlabel(\"Epizoda\", fontsize = 15)\nax1.set_ylabel(\"Epsilon vrednost\",fontsize = 15)\nplt.subplots_adjust( bottom=0.3)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"Vrednost parametra epsilon v odvisnosti od \u0161tevila epizod (u\u010denje Q)\",fontsize = 18)\nplt.gcf().text(0.4,0.05,r'$\\epsilon = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min})*e^{(\\lambda*N)}$',fontsize=18)\n#slider for decay rate\nax_decay_rate = plt.axes([0.25, 0.1, 0.65, 0.03])\ndecay_rate_slider = Slider(\nax=ax_decay_rate,\nlabel='Lambda',\nvalmin=0.0001,\nvalmax=0.01,\nvalinit=init_decay_rate,\n)\n#slider for max epsilon value\nax_max_epsilon= plt.axes([0.25, 0.13, 0.65, 0.03])\nmax_epsilon_slider = Slider(\nax=ax_max_epsilon,\nlabel='Epsilon (max)',\nvalmin=0.1,\nvalmax=1,\nvalinit=init_max_epsilon,\n)\n#slider for min epsilon value\nax_min_epsilon= plt.axes([0.25, 0.16, 0.65, 0.03])\nmin_epsilon_slider = Slider(\nax=ax_min_epsilon,\nlabel='Epsilon (min)',\nvalmin=0,\nvalmax=0.5,\nvalinit=init_min_epsilon,\n)\n#slider for max episode value\nax_total_episodes= plt.axes([0.25, 0.19, 0.65, 0.03])\ntotal_episodes_slider = Slider(\nax=ax_total_episodes,\nlabel='Skupno \u0161tevilo epizod - N',\nvalmin=1,\nvalstep=1,\nvalmax=10000,\nvalinit=init_total_episodes,\n)\ndef update(val):\nline.set_data(range(total_episodes_slider.val),plot_fcn(min_epsilon_slider.val,max_epsilon_slider.val, decay_rate_slider.val,total_episodes_slider.val))\n#ax3.set_xlim(0,total_episodes_slider.val)\nax3.autoscale_view(True,True,True)\nax3.relim()\nfig.canvas.draw_idle()\ndecay_rate_slider.on_changed(update)\nmax_epsilon_slider.on_changed(update)\nmin_epsilon_slider.on_changed(update)\ntotal_episodes_slider.on_changed(update)\ndecay_rate_slider.label.set_size(16)\nmax_epsilon_slider.label.set_size(16)\nmin_epsilon_slider.label.set_size(16)\ntotal_episodes_slider.label.set_size(16)\nresetax = plt.axes([0.8, 0.025, 0.1, 0.04])\nbutton = Button(resetax, 'Reset', hovercolor='0.975')\nbutton.label.set_size(16)\ndef reset(event):\ndecay_rate_slider.reset()\nmax_epsilon_slider.reset()\nmin_epsilon_slider.reset()\ntotal_episodes_slider.reset()\nax3.autoscale_view(True, True, True)\nax3.relim()\nbutton.on_clicked(reset)\nplt.show()\n</code></pre>"},{"location":"cliff3/#sprememba-parametrov-za-izbolsanje-ucenja","title":"Sprememba parametrov za izbol\u0161anje u\u010denja","text":"cliff.py<pre><code>import gym\nimport numpy as np\nimport random\nfrom time import sleep\nimport matplotlib.pyplot as plt\nenv = gym.make('CliffWalking-v0')\nenv.reset()\nenv.render()\nprint( \"Observation space = \", env.observation_space.n)\nprint( \"Actions = \", env.action_space.n)\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n#q_table = np.random.uniform(low=0, high=1, size=[env.observation_space.n, env.action_space.n])\nprint( \"Q table size = \", q_table.shape)\nlearning_rate = 0.5\ndiscount_factor = 0.95\nepochs = 45000\nmax_steps = 99\nepsilon = 1\nSTART_EPSILON_DECAYING = 1\nEND_EPSILON_DECAYING = epochs//2\nepsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n</code></pre> <p>Zamenjamo z </p> <pre><code># Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.01            # Minimum exploration probability\ndecay_rate = 0.001            # Exponential decay rate for exploration prob\n</code></pre> <p>Zamenjamo krivuljo za <code>epsilon</code> spremenljivko </p> <pre><code>    if END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:\nepsilon -= epsilon_decay_value\n</code></pre> <p>s kodo</p> <pre><code>    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n</code></pre>"},{"location":"conda_env/","title":"Priprava python okolja","text":""},{"location":"conda_env/#paket-conda","title":"Paket conda","text":"<p>Paket conda je priljubljen upravitelj paketov za Python, ki se pogosto uporablja za razvoj in testiranje Python programov. Je tudi odli\u010dno orodje za upravljanje in vzdr\u017eevanje okolij Pythona, zlasti za usposabljanje in razvoj algoritmov spodbujevalnega u\u010denja (RL).</p> <p>S paketom conda lahko preprosto ustvarite in upravljate okolja Python, kar vam omogo\u010da delo na razli\u010dnih projektih z razli\u010dnimi knji\u017enicami in algoritmi, ne da bi vas skrbelo upravljanje ve\u010d kopij istega paketa. Tako lahko na primer ustvarite novo okolje za projekt RL in vanj namestite potrebne knji\u017enice in algoritme. To okolje lahko nato uporabite za u\u010denje in razvoj modela RL. Paket conda zagotavlja tudi veliko vgrajenih funkcij za upravljanje in organizacijo okolja, kot so spremenljivke okolja in upravitelji paketov. Paket conda je torej zmogljivo orodje za upravljanje in vzdr\u017eevanje okolij Pythona, zlasti za u\u010denje in razvoj algoritmov spodbujevalnega u\u010denja. Omogo\u010da enostavno ustvarjanje in upravljanje okolij, namestitev potrebnih knji\u017enic in algoritmov ter obdelavo velikih koli\u010din podatkov. S paketom conda lahko preprosto zgradite in namestite svoj model RL ter hitro za\u010dnete s svojim prvim projektom.</p> <p>Bistvene to\u010dke:</p> <ul> <li>Ustvarjanje razli\u010dnih okolij v katerih imate lahko in\u0161talirane programske pakete za rezli\u010dne pakete.</li> <li>In\u0161talacije so lo\u010dene med okolji in ne vplivajo drugo na drugo.</li> <li>Deluje na operacijskih sistemih Windowsi in Linux.</li> <li>Spletna stran: https://www.anaconda.com/</li> <li>In\u0161talacija po navodilih<ul> <li>dodatna nastavitev <code>conda config --set auto_activate_base false</code></li> </ul> </li> </ul>"},{"location":"conda_env/#ustvarjanje-python-okolja-s-conda-paketom","title":"Ustvarjanje python okolja s conda paketom","text":"<p><code>conda create --name rl_test python=3.9</code></p>"},{"location":"conda_env/#aktivacija-okolja","title":"Aktivacija okolja","text":"<p><code>conda activate rl_test</code></p>"},{"location":"conda_env/#deaktivacija-okolja","title":"Deaktivacija okolja","text":"<p><code>conda deactivate</code></p>"},{"location":"conda_env/#organizacija-vaj","title":"Organizacija vaj","text":""},{"location":"conda_env/#cetrtkova-skupina","title":"\u010cetrtkova skupina","text":"<ul> <li>conda okolje <code>rl_cet</code></li> </ul>"},{"location":"conda_env/#petkova-skupina","title":"Petkova skupina","text":"<ul> <li>conda okolje <code>rl_pet</code></li> </ul>"},{"location":"conda_env/#navodila","title":"Navodila","text":"<ol> <li>Odprite terminal z bli\u017enjico CTRL+ALT+T</li> <li>Pomaknite se v mapo <code>RL</code> <pre><code>cd RL\n</code></pre></li> <li>Ustvarite va\u0161e mapo, npr. <code>rl_cet</code>:     <code>console    mkdir rl_cet    cd rl_cet</code></li> <li>Ustvarite python okolje <code>rl_cet</code> s conda paketom    <pre><code>conda create --name rl_cet python=3.9\n</code></pre></li> <li>Na vpra\u0161anje <code>Proceed ([y]/n)?</code> odgovorite z <code>y</code>.</li> <li>Aktivirajte okolje     <pre><code>conda activate rl_cet\n</code></pre></li> <li>In\u0161talacija python paketov za spodbujevalno u\u010denje    <pre><code>pip install gym==0.21.0\npip install pyglet==1.5.27\npip install stable-baselines3\npip install sb3_contrib\npip install tensorboard\n</code></pre></li> </ol>"},{"location":"load_car/","title":"Zagon agenta","text":"<ol> <li>Skripta za izvajanje agenta na podlagi shranjene Q tabele    Ustvarite novo skripto <code>load_car.py</code>.</li> <li> <p>Inicializacija okolja</p> <pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\n</code></pre> </li> <li> <p>Inicializacija agenta: nalo\u017eimo shranjeno Q tabelo</p> <p><pre><code>q_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\n</code></pre> 4. Za\u017eenemo okolje in agenta <pre><code>state = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\n</code></pre></p> </li> <li> <p>Celotna koda</p> </li> </ol> load_car.py<pre><code>import gym\nimport numpy as np\nfrom time import sleep\nenv = gym.make(\"MountainCar-v0\")\n#DISCRETE_OS_SIZE = [20, 20]\nGRID_SIZE = 20;\nDISCRETE_OS_SIZE = [GRID_SIZE]*len(env.observation_space.high)\nobs_high = env.observation_space.high\nobs_low = env.observation_space.low\ndiscrete_os_win_size = (obs_high - obs_low)/DISCRETE_OS_SIZE\nprint(discrete_os_win_size)\ndef get_discrete_state(state):\ndiscrete_state = (state - obs_low)/discrete_os_win_size\ndiscrete_state = np.clip(discrete_state.astype(int),0,GRID_SIZE-1)\nreturn tuple(discrete_state)\nq_table = np.load(f\"cart_e14900-qtable.npy\") \nprint( \"Q table size = \", q_table.shape)\nstate = env.reset()\ndiscrete_state = get_discrete_state(state)\nenv.render()\ndone = False\nwhile not done:\naction = np.argmax(q_table[discrete_state])\nstate, reward, done, info = env.step(action)\ndiscrete_state = get_discrete_state(state)\nenv.render()\n#sleep(0.5)\nenv.close()\n</code></pre>"},{"location":"move_agent_mouse/","title":"Premik agenta z mi\u0161ko","text":"<ul> <li><code>step()</code></li> </ul> <pre><code>        ############ TUKAJ SPREMINJATE\naction = self.move_agent_mouse()\n</code></pre>"},{"location":"playCirc_mouse/","title":"Skripta playCirc.py za premike z mi\u0161ko","text":"playCirc.py<pre><code>import gym\nimport numpy as np\nimport pygame\nimport circ_env\nenv = gym.make('circ_env/Circle-v0', render_mode=\"human\")\n# Reset the environment\nobs = env.reset()\nenv.render()\n# Run the environment with random actions\n#for i in range(500):\nstart_pos = None\nreset_time = 0\nwhile True:\n#if any([event.type == pygame.QUIT for event in pygame.event.get()]): break\n#player controls\nkeys = pygame.key.get_pressed() \nif keys[pygame.K_LEFT]: x = -1\nelif keys[pygame.K_RIGHT]: x = 1\nelse: x = 0\nif keys[pygame.K_UP]: y = -1\nelif keys[pygame.K_DOWN]: y = 1\nelse: y = 0   \nfor event in pygame.event.get():\nif event.type == pygame.MOUSEBUTTONDOWN:\n# Set the starting position\nstart_pos = pygame.mouse.get_pos()\nelif event.type == pygame.MOUSEBUTTONUP:\n# Reset the starting position\nstart_pos = None\nif start_pos is not None:\ncurrent_pos = pygame.mouse.get_pos()\nif pygame.time.get_ticks() - reset_time &gt;= 100:\n# Reset the starting position every second\nstart_pos = current_pos\nreset_time = pygame.time.get_ticks()        \ndx = current_pos[0] - start_pos[0]\ndy = current_pos[1] - start_pos[1]\nx = np.sign(dx)\ny = np.sign(dy)\naction = np.array([x,y],dtype=np.float32)\n#action = env.action_space.sample()\n#print(action)\nobs, reward, done, _ = env.step(action)\n# Render the environment\nenv.render()\n# Check if the episode is finished\nif done:\nobs = env.reset()\nstart_pos = None\nreset_time = 0\n# Close the environment\nenv.close()\n</code></pre>"},{"location":"save_models/","title":"Logiranje in spremljanje u\u010denja","text":""},{"location":"save_models/#python-skripta-za-testiranje-agenta","title":"Python skripta za testiranje agenta","text":"load_carDQN2.py<pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport matplotlib.pyplot as plt\nimport numpy as np\nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\nmodel = DQN.load(\"dqn_car\", env=env)\nmean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\nprint(f'Mean reward: {mean_reward}, Std reward: {std_reward}')\nreward_list = [] ###\nfor episode in range(50): ###\nstate = env.reset()\ndone = False\nprint(\"EPISODE \", episode)\ntot_reward, reward = 0, 0 ###\nwhile not done:\naction, _state = model.predict(state, deterministic=True)\nstate, reward, done, info = env.step(action)\ntot_reward += reward ###\nreward_list.append(tot_reward) ###\nenv.close()\nf = plt.figure()\nplt.plot(reward_list)\nplt.xticks(range(0,len(reward_list),2),range(1,len(reward_list)+1,2))\nplt.axhline(y = -200, color = 'k', linestyle = '--')\nplt.axhline(y = np.average(reward_list), color = 'blue', linestyle = '--', label = 'povpre\u010dna nagrada')\nplt.annotate(str(np.average(reward_list)), xy= (-2,np.average(reward_list)+0.7), color = 'blue',fontsize = 13, weight = 'bold')\nax=plt.gca()\nax.tick_params(axis=\"both\", labelsize=12)\nf.legend(loc = 'right', fontsize = 13)\nplt.xlabel('Epizoda', fontsize=14)\nplt.ylabel('Nagrada', fontsize=14)\nplt.title('Vrednost nagrade v posamezni epizodi', fontsize=16)\n#f.savefig('reward3.jpg') ###\nplt.show()\n</code></pre>"},{"location":"save_models/#logiranje-in-sprotno-shranjevanje-agentov","title":"Logiranje in sprotno shranjevanje agentov","text":"<ul> <li>Paket Tensorboard</li> </ul>"},{"location":"save_models/#prilagojena-python-skripta","title":"Prilagojena Python skripta","text":"<ol> <li> <p>Pripravimo mape za shranjevanje modelov</p> <pre><code>import gym\nfrom stable_baselines3 import DQN \nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport os\nmodels_dir = \"models/DQN\"\nif not os.path.exists(models_dir):\nos.makedirs(models_dir)\nlogdir = \"logs\"\nif not os.path.exists(logdir):\nos.makedirs(logdir) \nenv = gym.make(\"MountainCar-v0\")\nprint( \"Actions = \", env.action_space.n)\nprint( \"Obs space high = \", env.observation_space.high)\nprint( \"Obs space low\", env.observation_space.low)\n</code></pre> </li> <li> <p>Inicializiramo u\u010denje agenta</p> </li> <li> <p>Podatki za inicializacijo so na spletni strani</p> <pre><code>policy_kwargs = dict(net_arch=[256, 256])\nmodel = DQN('MlpPolicy', \nenv=env,\nlearning_rate=4e-3,\nbatch_size=128,\nbuffer_size=10000,\nlearning_starts=1000,\ngamma=0.99,\ntarget_update_interval=600,\ntrain_freq=16,\ngradient_steps=8,\nexploration_fraction=0.2,\nexploration_final_eps=0.07,\npolicy_kwargs=policy_kwargs,\nseed=2,\ntensorboard_log=logdir,\nverbose=1\n)\n</code></pre> </li> <li> <p>U\u010denje in shranjevanje</p> <pre><code>TIMESTEPS = 2000\niters = 0\nwhile True:\niters += 1\nmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"DQN\")\nmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n</code></pre> </li> </ol>"},{"location":"save_models/#uporaba-logiranja-za-spremljanje-ucenja","title":"Uporaba logiranja za spremljanje u\u010denja","text":"<p>tensorboard --logdir=logs</p> <p>python3 -m tensorboard.main --logdir=logs</p> <p>python3 -m tensorboard.main --logdir=logs</p> <p></p> <p></p> <p></p>"}]}